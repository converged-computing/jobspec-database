# llama inference

## speed up inference of 7b and 70b llama models with
- vllm
- vllm + async
- mii
- fms
from __future__ import absolute_import, division, print_function, unicode_literals

# -*- coding: utf-8 -*-
"""finetuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/google-research/simclr/blob/master/colabs/finetuning.ipynb

# Copyright 2020 Google LLC.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import os, sys
if os.path.abspath(".") not in sys.path:
    sys.path.append(os.path.abspath("."))

import dataloaders.chest_xray as chest_xray
from validation_supervised import evaluation
import utils.model_ckpt as model_ckpt
import tensorflow.compat.v1 as tf
from absl import app
import warnings
warnings.filterwarnings("ignore")
import argparse
import yaml
from tensorflow.python.client import device_lib
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from datetime import datetime
import tensorflow_hub as hub
import re
import numpy as np
import tensorflow as tf
import tensorflow.compat.v1 as tf1
import mlflow
import pickle
from pathlib import Path
import time
import scipy
from Finetuning.package import *
print(tf.__version__)
tf1.disable_eager_execution()
from shutil import rmtree
from functools import partial
import simclr_master.resnet as resnet
from utils.create_folder import create_folder

from absl import flags


FLAGS = flags.FLAGS
BATCH_NORM_EPSILON = 1e-5

flags.DEFINE_float(
    'sk_ratio', 0.,
    'If it is bigger than 0, it will enable SK. Recommendation: 0.0625.')

flags.DEFINE_boolean(
    'global_bn', True,
    'Whether to aggregate BN statistics across distributed cores.')

flags.DEFINE_float(
    'batch_norm_decay', 0.9,
    'Batch norm decay parameter.')

flags.DEFINE_float(
    'se_ratio', 0.,
    'If it is bigger than 0, it will enable SE.')

flags.DEFINE_enum(
    'train_mode', 'pretrain', ['pretrain', 'finetune'],
    'The train mode controls different objectives and trainable components.')

flags.DEFINE_integer(
    'fine_tune_after_block', -1,
    'The layers after which block that we will fine-tune. -1 means fine-tuning '
    'everything. 0 means fine-tuning after stem block. 4 means fine-tuning '
    'just the linera head.')

flags.DEFINE_string(
    'output_dir', "supervised",
    'Main Folder to save checkpoints')

flags.DEFINE_string(
    'mlflow_dir', "supervised",
    'Main Folder to save mlflow')

flags.DEFINE_string(
    'config', "config.yml",
    'Config File')

flags.DEFINE_string(
    'xray_path', "",
    'Path to dataset')

flags.DEFINE_integer(
    'width_multiplier', 1,
    'Multiplier to change width of network.')

flags.DEFINE_integer(
    'resnet_depth', 50,
    'Depth of ResNet.')


flags.DEFINE_integer(
    'image_size', 224,
    'Input image size.')

def test_weighted_cel():
    with tf1.Session():
        b = 64
        c = 14
        logits = tf.random.uniform([b, c], minval=-1, maxval=1)
        labels = tf.cast(tf.random.uniform([b, c], minval=-1, maxval=1) > 0, tf.float32)
        loss = weighted_cel(labels=labels, logits=logits)
        pass

def show_one_image(im):
    plt.imshow(im)
    plt.title("Test")
    plt.axis("off")
    plt.show()

def train(yml_config):
    with strategy.scope():

        # @title Load tensorflow datasets: we use tensorflow flower dataset as an examplegit
        batch_size = yml_config['finetuning']['batch']
        buffer_size = yml_config['finetuning']['buffer_size']

        # @title Load tensorflow datasets: we use tensorflow flower dataset as an example
        dataset_name = yml_config['data_src']

        data_path = FLAGS.xray_path
        train_dataset, tfds_info = chest_xray.XRayDataSet(data_path, config=None, train=True)
        num_images = np.floor(yml_config['finetuning']['train_data_ratio'] *tfds_info['num_examples'])
        num_classes = tfds_info['num_classes']
        print(f"Training: {num_images} images...")


        def _preprocess(x):
            x['image'] = preprocess_image(
                x['image'], 224, 224, is_training=False, color_distort=False)
            return x
            
        x_ds = train_dataset \
            .take(num_images) \
            .map(_preprocess, deterministic=False) \
            .shuffle(buffer_size)\
            .batch(yml_config['finetuning']['batch'])


        x_iter = tf1.data.make_one_shot_iterator(x_ds)
        x_init = x_iter.make_initializer(x_ds)
        x = x_iter.get_next()

        print(f"{type(x)} {type(x['image'])} {x['image']} {x['label']}")
        # @title Load module and construct the computation graph
        learning_rate = yml_config['finetuning']['learning_rate']
        momentum = yml_config['finetuning']['momentum']
        weight_decay = yml_config['finetuning']['weight_decay']
        epoch_save_step = yml_config['finetuning']['epoch_save_step']
        load_saver = yml_config['finetuning'].get('load_ckpt')

        resnet.BATCH_NORM_DECAY = FLAGS.batch_norm_decay
        module = resnet.resnet_v1(
            resnet_depth=FLAGS.resnet_depth,
            width_multiplier=FLAGS.width_multiplier,
            cifar_stem=FLAGS.image_size <= 32)

        
        key = module(inputs=x['image'], is_training=True)
        
        # Attach a trainable linear layer to adapt for the new task.
        if dataset_name == 'tf_flowers':
            with tf1.variable_scope('head_supervised_new', reuse=tf1.AUTO_REUSE):
                logits_t = tf1.layers.dense(inputs=key['default'], units=num_classes, name='proj_head')
            loss_t = tf1.reduce_mean(input_tensor=tf1.nn.softmax_cross_entropy_with_logits(
                labels=tf1.one_hot(x['label'], num_classes), logits=logits_t))
        elif dataset_name == 'chest_xray':
            with tf1.variable_scope('head_supervised_new', reuse=tf1.AUTO_REUSE):
                logits_t = tf1.layers.dense(inputs=key, units=num_classes)
                cross_entropy = weighted_cel(labels=x['label'], logits=logits_t, bound = 3.0)
                loss_t = tf1.reduce_mean(tf1.reduce_sum(cross_entropy, axis=1))


        # Setup optimizer and training op.
        if yml_config['finetuning']['optimizer'] == 'adam':
            optimizer = tf1.train.AdamOptimizer(learning_rate)
        elif yml_config['finetuning']['optimizer'] == 'lars':
            optimizer = LARSOptimizer(
                learning_rate,
                momentum=momentum,
                weight_decay=weight_decay,
                exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])
        else:
            raise RuntimeError("Optimizer not supported")


        variables_to_train = tf1.trainable_variables()
        train_op = optimizer.minimize(
            loss_t, global_step=tf1.train.get_or_create_global_step(),
            var_list=variables_to_train)

        print('Variables to train:', variables_to_train)

        # Add ops to save and restore all the variables.
        sess = tf1.Session()
        Saver = tf1.train.Saver() # Default saves all variables
        current_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        directory = Path(FLAGS.output_dir)

        is_time_to_save_session = partial(model_ckpt.save_session, epoch_save_step, Saver, output=directory)
        if load_saver is not None:
            Saver.restore(sess, load_saver)
        else:
            sess.run(tf1.global_variables_initializer())

        # @title We fine-tune the new *linear layer* for just a few iterations.
        epochs = yml_config['finetuning']['epochs']


        # ===============Tensor board section ===============
        # with tf.name_scope('performance'):
        # tf_labels = tf1.placeholder(tf.int32, shape=[batch_size,num_classes], name='accuracy')
        tf_tot_acc_all_ph = tf1.placeholder(tf.float32, shape=None, name='accuracy_all_labels_ph')
        tf_tot_acc_all_summary = tf1.summary.scalar('accuracy_all_labels', tf_tot_acc_all_ph)
        tf_tot_acc_per_class_ph = tf1.placeholder(tf.float32, shape=None, name='accuracy_per_class_ph')
        tf_tot_acc_per_class_summary = tf1.summary.scalar('accuracy_per_class', tf_tot_acc_per_class_ph)
        tf_tot_acc_class_avg_ph = tf1.placeholder(tf.float32, shape=None, name='accuracy_per_class_averaged_ph')
        tf_tot_acc_class_avg_summary = tf1.summary.scalar('accuracy_per_class_averaged', tf_tot_acc_class_avg_ph)
        tf_train_tot_loss_ph = tf1.placeholder(tf.float32, shape=None, name='train_tot_loss')
        tf_train_tot_loss_summary = tf1.summary.scalar('train_tot_loss', tf_train_tot_loss_ph)
        tf_tot_auc_ph = tf1.placeholder(tf.float32, shape=None, name='auc_ph')
        tf_tot_auc_ph_summary = tf1.summary.scalar('auc', tf_tot_auc_ph)

        performance_summaries = tf1.summary.merge(
            [tf_tot_acc_all_summary, tf_tot_acc_class_avg_summary, tf_train_tot_loss_summary, tf_tot_auc_ph_summary])

        hyper_param = []
        for item in yml_config['finetuning']:
            hyper_param.append(tf1.summary.text(str(item), tf.constant(str(yml_config['finetuning'][item])),'HyperParam'))

        summ_writer = tf1.summary.FileWriter(directory / 'tb', sess.graph)
        tf.summary.record_if(yml_config['tensorboard'])
        # Limit the precision of floats...
        np.set_printoptions(formatter={'float': '{: 0.3f}'.format})
        with sess.as_default() as scope:
            if yml_config['mlflow']:
                mlflow.set_tracking_uri(FLAGS.mlflow_dir)
                mlflow.set_experiment('results')
                mlflow.start_run()
                mlflow.log_param('Directory', str(directory))
                finetuned_params =  {'F-' + str(key).replace('/', ''): val for key, val in yml_config['finetuning'].items()}
                mlflow.log_param('TB_Timestamp', current_time)
                mlflow.log_params(finetuned_params) 
            
            fname = str(directory / f'finetuning_hyper_params.txt')
            with open(fname, 'w') as f: 
                for key, value in yml_config['finetuning'].items(): 
                    f.write('%s:%s\n' % (key, value)) 
            
            writer = tf1.summary.FileWriter('./log', sess.graph)
            for index,summary_op in enumerate(hyper_param):
                text = sess.run(summary_op)
                summ_writer.add_summary(text, index)


            n_iter = int(num_images / batch_size)
            print(f"Batch:{batch_size}, n_iter:{n_iter} ")

            # =============== Main Loop (epoch) - START ===============
            for it in range(epochs):
                start_time_epoch = time.time()
                # Init dataset iterator
                sess.run(x_init)
                tot_acc_all = 0.0
                tot_acc_per_class = 0.0
                tot_acc_class_avg = 0.0
                train_tot_loss = 0.0
                epoch_acc_all = 0.0
                epoch_acc_per_class = 0.0
                epoch_acc_class_avg = 0.0
                #show_one_image(x['image'][0].eval())

                # =============== Main Loop (iteration) - START ===============
                all_labels = []
                all_logits = []
                for step in range(n_iter):

                    start_time_iter = time.time()
                    _, loss, image, logits, labels = sess.run(fetches=(train_op, loss_t, x['image'], logits_t, x['label']))
                    train_tot_loss += loss
                    all_labels.extend(labels)
                    if dataset_name == 'tf_flowers':
                        pred = logits.argmax(-1)
                        correct = np.sum(pred == labels)
                        acc_per_class = np.array([correct / float(batch_size)])
                    elif dataset_name == 'chest_xray':
                        # # New compute
                        logits_sig = scipy.special.expit(logits)
                        all_logits.extend(logits_sig)
                        pred = (logits_sig > 0.5).astype(np.float32)
                        acc_all = np.mean(np.min(np.equal(pred, labels).astype(np.float32), axis=1))
                        acc_per_class = np.mean(np.equal(pred, labels).astype(np.float32), axis=0)
                        acc_class_avg = np.mean(acc_per_class)
                        tot_acc_all += acc_all
                        tot_acc_per_class += acc_per_class
                        tot_acc_class_avg += acc_class_avg

                    #The function roc_auc_score can result in a error (ValueError: Only one class present in y_true.
                    # ROC AUC score is not defined in that) . The error occurred when each label has only one class
                    # in the batch. For example, if all the samples in the batch has hernia +1, the error will occurred.I
                    try:
                        auc_cum = roc_auc_score(np.array(all_labels),np.array(all_logits))
                    except:
                        auc_cum = None

                    current_time_iter = time.time()
                    elapsed_time_iter = current_time_iter - start_time_iter

                    if yml_config['finetuning']['verbose_train_loop']:
                        print(f"[Epoch {it + 1}/{epochs} Iter: {step}/{n_iter}] Model: {yml_config['finetuning']['pretrained_model']}, Total Loss: {train_tot_loss} Loss: {np.float32(loss)} Batch Acc: {np.float32(acc_all)} "
                              f"Acc Avg(class): {np.float32(acc_class_avg)}, AUC Cumulative: {auc_cum}")
                        print(f"Finished iteration:{step} in: " + str(int(elapsed_time_iter)) + " sec")
                    
                    # break if logits explose
                    if np.isnan(np.sum(logits)):
                        print(f"Loss has exploded: Nan")
                        break

                
                epoch_acc_all = (tot_acc_all/n_iter)
                epoch_acc_per_class = (tot_acc_per_class / n_iter)
                epoch_acc_class_avg = (tot_acc_class_avg / n_iter)


                try:
                    epoch_auc = roc_auc_score(np.array(all_labels),np.array(all_logits), average=None)
                    epoch_auc_mean = epoch_auc.mean()
                    aucs = dict(zip(chest_xray.XR_LABELS.keys(),epoch_auc ))
                    auc_scores = {'AUC ' + str(key): val for key, val in aucs.items()}

                except:
                    epoch_auc= None
                    epoch_auc_mean= None

                print(f"[Epoch {it + 1}/{epochs} Model: {yml_config['finetuning']['pretrained_model']}, Loss: {train_tot_loss} #Train Acc: {np.float32(epoch_acc_all)}, Train Acc Avg(class) {np.float32(epoch_acc_class_avg)}"
                      f" Train AUC: {epoch_auc_mean} AOC/Class {epoch_auc},")
                
                # Is it time to save the session?
                is_time_to_save_session(it, sess)

                current_time_epoch = time.time()
                elapsed_time_iter = current_time_epoch - start_time_epoch
                print(f"Finished EPOCH:{it + 1} in: " + str(int(elapsed_time_iter)) + " sec")
                # print(psutil.virtual_memory())

                # ===================== Write Tensorboard summary ===============================
                # Execute the summaries defined above

                summ = sess.run(performance_summaries, feed_dict={tf_tot_acc_all_ph: epoch_acc_all,
                                                                  tf_tot_acc_class_avg_ph: epoch_acc_class_avg,
                                                                  tf_train_tot_loss_ph: train_tot_loss,
                                                                  tf_tot_auc_ph: epoch_auc_mean})


                # Write the obtained summaries to the file, so it can be displayed in the TensorBoard
                summ_writer.add_summary(summ, it)

                # =============== Main Loop (epoch) - END ===============

            print(f"Training Done")


            # This MLFLOW code is now saving training metrics. When the validation accuracy will be completed,
            # we should save instead the validation/test metrics.
            # The saving will occured only at the end of the finetuning
            if yml_config['mlflow']:
                mlflow.log_metric('Total Train Accuracy',epoch_acc_all)
                mlflow.log_metric('Total Train Accuracy per class', np.mean(epoch_acc_per_class))
                mlflow.log_metric('Total Train Loss',train_tot_loss)

                if epoch_auc is not None:
                    mlflow.log_metrics(auc_scores)
                    mlflow.log_metric('Avg train AUC', epoch_auc_mean)

            fname_final = str(directory / f'final.ckpt')
            ckpt_pt = Saver.save(sess=sess,save_path=fname_final)
            print(f"Final Chekpoint Saved in {fname_final}")
            return directory


def start(yml_config):
    print("Start of Finetuning model")
    directory = train(yml_config)
    print("Finetuning Completed")
    print("Evaluation Started")
    fname_final = str(directory / f'final.ckpt')
    evaluation(yml_config, module_path=fname_final, FLAGS = FLAGS)
    print("Evaluation Completed with Success")
    print('All steps completed')


def main(argv):
    try:
        with open(FLAGS.config) as f:
            yml_config = yaml.load(f, Loader=yaml.FullLoader)
    except Exception:
        raise RuntimeError(f"Configuration file {FLAGS.config} do not exist")

    start(yml_config)
    sys.exit(0)

if __name__ == '__main__':
    #tf.disable_v2_behavior()  # Disable eager mode when running with TF2.
    if tf.config.list_physical_devices('gpu'):
        strategy = tf.distribute.CentralStorageStrategy()
    else:  # use default strategy
        strategy = tf.distribute.get_strategy()
    app.run(main)

    
# Config file for train_ray.py for CIFAR-10.
# =========================================================================== #
#                                   Meta Params                               #
# =========================================================================== #
meta:
  exp_id: 34
  seed: 2020
  model_name: null        # If set to null, model name is auto-generated
  save_path: '~/rand-smooth/save/'
  data_path: '~/data/imagenette2-320/'
  network: 'resnet34'
  # pretrained: True
  pretrained: '~/rand-smooth/save/imagenette/resnet34/none/1/model.pt'
  dataset: 'imagenette'
  classes: null
  augment: True           # Whether to augment training data
  normalize: null
  shuffle: True           # Should always be True for consistency
  val_size: 0.1           # Validation set split
  method: 'pgd'
  test:   # Testing params
    batch_size: 400
    num_samples: 1000
    save_adv: False          # Save generated adversarial examples
    save_adv_out: False      # Save logits output for adversarial examples\
    save_clean_out: False    # Save logits output for clean samples
    clean_only: False        # Evaluate model on clean data only
  train:  # Training params
    batch_size: 128
    epochs: 50
    l2_reg: 0.0005
    learning_rate: 0.01
    lr_scheduler: 'cos'
    step_len: 10
    optimizer: 'sgd'
    save_best_only: True
    save_epochs: 1
    eval_with_atk: False
    metric: 
      metric: 'weight_acc'    # 'adv_acc', 'clean_acc', 'weight_acc', 'sqrt_acc'
      adv_acc_weight: 2.
      clip_clean_acc: 100
  valid:  # Validation params
    batch_size: 200

# =========================================================================== #
#                          Adversarial Training Params                        #
# =========================================================================== #
at:
  method: 'pgd'
  random_start: True    # if True, use random start
  loss_func: 'mat'       # loss function for generating adversarial examples (options: 'ce', 'hinge', 'clipped_ce', 'trades')
  clip: [0., 1.]            # if True, clip adversarial input to [0, 1]
  beta: 0.55               # TRADES parameters

  # parameters for AT with l-inf norm
  p: 'inf'              # specify lp-norm to use
  # epsilon: 0.031372     # 8/255
  # step_size: 0.007843   # 2/255
  epsilon: 0.06274509803     # 16/255
  step_size: 0.0156862745    # 4/255
  # epsilon: 0.06274509803     # 10/255
  # step_size: 0.0156862745    # 2.5/255
  num_steps: 10
  
# =========================================================================== #
#                                  Attack Params                              #
# =========================================================================== #
attack:
  method: 'auto'   # 'pgd', 'opt', 'auto'
  p: 'inf'

# Parameters for pgd attack
pgd:
  random_start: True
  loss_func: 'ce'
  gap: 1.0e+9    # Gap parameter of hinge loss
  targeted: False
  clip: [0., 1.]    # clipping values on the perturbed sample (use null for no clipping)
  init_mode: 1
  epsilon: 0.06274509803
  num_restarts: 5
  num_steps: 1000
  step_size: 0.005
  # maximin: 10
  # momentum:
  #   mu: 1.
  #   decay: False
  #   normalize: True
  #   vr: True
  sgm_gamma: 0.5
  # linbp_layer: [4, 1]

# Parameters for Opt attack
opt:
  optimizer: 'adam'    # 'sgd', 'adam', 'rmsprop'
  random_start: True
  loss_func: 'ce'
  gap: 1.0e+9
  targeted: False
  clip: [0., 1.]
  init_mode: 1
  num_restarts: 1
  epsilon: 0.06274509803
  num_steps: 500
  learning_rate: 0.2
  lr_schedule: null    # 'cyclic', null
  # maximin: 10
  # momentum: 1.
  # sgm_gamma: 0.5
  # linbp_layer: [4, 1]

# Parameters for auto-attack
auto:
  version: 'standard'
  epsilon: 0.06274509803

# TODO: Parameters for saving gradient information
save_grad:
  save_grad: False    # Set to True to save gradients
  num_samples: 20
  num_steps: 10

diversity:
  method: null

rand:
  dummy: null
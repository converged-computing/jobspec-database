# -*- coding: utf-8 -*-
"""595 Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10gpsVITo05xBPVVyx-KFaCuuUFoY2UiC
"""

# COCO Dataset 640x480 px
IMG_WIDTH = 640
IMG_HEIGHT = 480

import os
import pickle
import random
from enum import Enum
import progressbar, time, string, json, sys
import matplotlib.pyplot as plt
from PIL import ImageFile
import numpy as np
import tensorflow as tf
import keras
from keras import layers
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.models import Model
from keras.applications.resnet import ResNet50
from keras.applications.resnet import preprocess_input
from squeezenet import SqueezeNet
import nltk
from nltk.translate.bleu_score import sentence_bleu
# from nltk.translate import meteor
# from ntlk import word_tokenize
from nltk.translate.meteor_score import single_meteor_score

nltk.download('wordnet')

class CNN_Model(str, Enum):
    VGG16 = "vgg"
    RESNET = "resnet"
    SQUEEZENET = "squeezenet"


def get_cnn_model(cnn_model_name):
    # VGG16
    if cnn_model_name == CNN_Model.VGG16:
        model = VGG16()
        model.layers.pop()
        model = Model(inputs=model.inputs, outputs=model.layers[-1].output)

    # ResNet50
    elif cnn_model_name == CNN_Model.RESNET:
        model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')
        # output = model.layers[-1].output
        # output = keras.layers.Flatten()(output)
        # model = Model(resnet.input, output=output)
        for layer in model.layers:
            layer.trainable = False

    # SqueezeNet
    elif cnn_model_name == CNN_Model.SQUEEZENET:
        model = SqueezeNet()

    else:
        raise ValueError('Invalid CNN model')

    return model



def extract_features(cnn_model, directory):
    """Extract features from each photo in the directory. """
    ImageFile.LOAD_TRUNCATED_IMAGES = True

    features = dict()
    with progressbar.ProgressBar(max_value=len(os.listdir(directory))) as bar:
        for i, name in enumerate(os.listdir(directory)):
            filename = directory + '/' + name
            image = load_img(filename, target_size=(224, 224))
            image = img_to_array(image)
            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
            image = preprocess_input(image)
            feature = cnn_model.predict(image, verbose=0)

            image_id = name.split('.')[0].split('_')[-1]
            features[int(image_id)] = feature[0]
            bar.update(i)

    return features



"""## Preprocess Captions"""

def preprocess_captions(fname):
    table = str.maketrans('', '', string.punctuation)
    captions_dic = {}
    with open(fname, "r") as f:
        data = json.loads(f.read())
        for annotation in data['annotations']:
            caption_text = annotation["caption"]
            caption_text = caption_text.split()
            # convert it into lower case
            caption_text = [token.lower() for token in caption_text]
            # remove punctuation from each token
            caption_text = [token.translate(table) for token in caption_text]
            # remove all the single letter tokens like 'a', 's'
            caption_text = [token for token in caption_text if len(token) > 1]
            captions_dic[int(annotation["image_id"])] = 'startseq ' + ' '.join(caption_text) + ' endseq'

    return captions_dic



"""## Preprocessing"""

def generate_indexed_captions(tokenizer, captions_dict):
    indexed_captions = {}
    for id, caption in list(captions_dict.items()):
        indexed_captions[id] = tokenizer.texts_to_sequences([caption])[0]

    return indexed_captions


def preprocessing(indexed_captions, features, max_len, vocab_size):
    N = len(indexed_captions)
    print("# captions/images = {}".format(N))

    # assert(N==len(features))
    Xcaption, Xfeature, ycaption = [], [], []
    for id in features:
        text, feature = indexed_captions[id], features[id]
        # print("text: {}".format(text))
        for i in range(1, len(text)):
            in_text, out_text = text[:i], text[i]
            in_text = pad_sequences([in_text], maxlen=max_len).flatten()
            out_text = to_categorical(out_text, num_classes=vocab_size)
            # print("in_text: {}\t out_text: {}".format(in_text, out_text))

            Xcaption.append(in_text)
            Xfeature.append(feature)
            ycaption.append(out_text)

    Xcaption = np.array(Xcaption)
    Xfeature = np.array(Xfeature)
    ycaption = np.array(ycaption)
    print(" {} {} {}".format(Xcaption.shape, Xfeature.shape, ycaption.shape))
    return Xcaption, Xfeature, ycaption


def generate_train_val_set(cnn_model, train_feature_path=None, val_feature_path=None):
    train_captions = preprocess_captions("data/annotations/captions_train2014.json")
    val_captions = preprocess_captions("data/annotations/captions_val2014.json")

    # Save features for future use
    if train_feature_path:
        with open(train_feature_path, 'rb') as f:
            train_features = pickle.load(f)
    else:
        train_features = extract_features(cnn_model, "data/train2014")
        with open("features/train_features.pickle", 'wb') as f:
            pickle.dump(train_features, f)
    
    # Save features for future use
    if val_feature_path:
        with open(val_feature_path, 'rb') as f:
            val_features = pickle.load(f)
    else:
        val_features = extract_features(cnn_model, "data/val2014")
        with open("features/val_features.pickle", 'wb') as f:
            pickle.dump(val_features, f)


    # initialise tokenizer
    tokenizer = Tokenizer()

    # create word count dictionary on the captions list (both train + val)
    tokenizer.fit_on_texts(list(train_captions.values()))
    tokenizer.fit_on_texts(list(val_captions.values()))

    # how many words are there in the vocabulary? store the total length in vocab_len and add 1 because word_index starts with 1 not 0
    vocab_size = len(tokenizer.word_index) + 1

    # store the length of the maximum sentence
    max_len = max(max(len(caption.split()) for caption in train_captions.values()),
                  max(len(caption.split()) for caption in val_captions.values()))

    # print("vocab size: {}\t max_len: {}".format(vocab_size, max_len))

    Xcaption_train, Xfeature_train, ycaption_train = preprocessing(generate_indexed_captions(tokenizer, train_captions),
                                                                   train_features, max_len, vocab_size)
    Xcaption_val, Xfeature_val, ycaption_val = preprocessing(generate_indexed_captions(tokenizer, val_captions),
                                                             val_features, max_len, vocab_size)

    return [(Xcaption_train, Xfeature_train, ycaption_train),
            (Xcaption_val, Xfeature_val, ycaption_val),
            (tokenizer, max_len, vocab_size)]


"""
RNN Model Architecture
"""

class RNN_Model(str, Enum):
    LSTM = "lstm"
    GRU = "gru"

def get_rnn_model(rnn_model, feature_shape, vocab_size, max_len):
    # LSTM
    if rnn_model == RNN_Model.LSTM:
        dim_embedding = 64

        input_image = layers.Input(shape=feature_shape)
        fimage = layers.Dense(256, activation='relu', name="ImageFeature")(input_image)
        ## sequence model
        input_txt = layers.Input(shape=(max_len,))
        ftxt = layers.Embedding(vocab_size, dim_embedding, mask_zero=True)(input_txt)
        ftxt = layers.LSTM(256, name="CaptionFeature")(ftxt)
        ## combined model for decoder
        decoder = layers.add([ftxt, fimage])
        decoder = layers.Dense(256, activation='relu')(decoder)
        output = layers.Dense(vocab_size, activation='softmax')(decoder)
        model = Model(inputs=[input_image, input_txt], outputs=output)

        model.compile(loss='categorical_crossentropy', optimizer='adam')

        print(model.summary())

    # GRU
    elif rnn_model == RNN_Model.GRU:
        dim_embedding = 64

        input_image = layers.Input(shape=feature_shape)
        fimage = layers.Dense(256, activation='relu', name="ImageFeature")(input_image)
        ## sequence model
        input_txt = layers.Input(shape=(max_len,))
        ftxt = layers.Embedding(vocab_size, dim_embedding, mask_zero=True)(input_txt)
        ftxt = layers.GRU(256, name="CaptionFeature")(ftxt)
        ## combined model for decoder
        decoder = layers.add([ftxt, fimage])
        decoder = layers.Dense(256, activation='relu')(decoder)
        output = layers.Dense(vocab_size, activation='softmax')(decoder)
        model = Model(inputs=[input_image, input_txt], outputs=output)

        model.compile(loss='categorical_crossentropy', optimizer='adam')

        print(model.summary())

    else:
        print(rnn_model)
        raise ValueError('Invalid RNN model')

    return model



""" Train """
def train(cnn_model, rnn_model, train_feature_path=None, val_feature_path=None):
    print("Preprocessing...")
    start = time.time()
    cnn_model = get_cnn_model(cnn_model)
    train_val = generate_train_val_set(cnn_model, train_feature_path, val_feature_path)
    Xcaption_train, Xfeature_train, ycaption_train = train_val[0]
    Xcaption_val, Xfeature_val, ycaption_val = train_val[1]
    tokenizer, max_len, vocab_size = train_val[2]

    end = time.time()
    print("TIME TOOK TO PREPROCESS IMAGES {:3.2f}MIN\n".format((end - start) / 60))

    print("Training...")
    start = time.time()
    # checkpoint_path = "training_1/cp.ckpt"
    # checkpoint_dir = os.path.dirname(checkpoint_path)

    # Create checkpoint callback
    # cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
    #                                                save_weights_only=True,
    #                                               verbose=2)

    feature_shape = (Xfeature_train.shape[1],)
    rnn_model = get_rnn_model(rnn_model, feature_shape, vocab_size, max_len)
    hist = rnn_model.fit([Xfeature_train, Xcaption_train], ycaption_train,
                     epochs=10, verbose=2,
                     batch_size=320,
                     validation_data=([Xfeature_val, Xcaption_val], ycaption_val))
    # callbacks = [cp_callback])
    end = time.time()
    print("TIME TOOK TO TRAIN {:3.2f}MIN\n".format((end - start) / 60))

    for label in ["loss", "val_loss"]:
        plt.plot(hist.history[label], label=label)
    plt.legend()
    plt.xlabel("epochs")
    plt.ylabel("loss")
    plt.savefig("training_loss.png")

    # Save trained model and relevant info
    dict = {
        'model': rnn_model,
        'tokenizer': tokenizer,
        'max_len': max_len
    }


    with open("trained/trained_model_dict.pickle", 'wb') as f:
        pickle.dump(dict, f)


""" Eval """
def predict_caption(trained_model, tokenizer, image, index_word, max_len):
    '''
    image.shape = (1,4462)
    '''

    in_text = 'startseq'

    for iword in range(max_len):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], max_len)
        yhat = trained_model.predict([image, sequence], verbose=0)
        yhat = np.argmax(yhat)
        newword = index_word[yhat]
        in_text += " " + newword
        if newword == "endseq":
            break
    return in_text


def eval(trained_model, cnn_model_name, tokenizer, max_len, feature_path=None):
    index_word = dict([(index, word) for word, index in tokenizer.word_index.items()])

    cnn_model = get_cnn_model(cnn_model_name)

    if feature_path:
        with open(feature_path, 'rb') as f:
            test_features = pickle.load(f)

    else:
        test_features = extract_features(cnn_model, "data/test2014")
        with open("features/test_features.pickle", 'wb') as f:
            pickle.dump(test_features, f)


    npic = 10
    npix = 224
    target_size = (npix, npix, 3)
    count = 1
    fig = plt.figure(figsize=(10, 20))

    test_features = list(test_features.items())
    for img_id, feature in random.sample(test_features, npic):
        ## images
        img_fname = "data/test2014/COCO_test2014_" + f"{img_id:012}" + ".jpg"
        image_load = load_img(img_fname, target_size=target_size)
        ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])
        ax.imshow(image_load)
        count += 1

        ## captions
        caption = predict_caption(trained_model, tokenizer, feature.reshape(1, len(feature)), index_word, max_len)
        ax = fig.add_subplot(npic, 2, count)
        plt.axis('off')
        ax.plot()
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.text(0, 0.5, caption, fontsize=20)
        count += 1
        

    plt.savefig("predictions.png", bbox_inches="tight")



def getEvalScores(trained_model, tokenizer, max_len, features, captions, sample_size):
    index_word = dict([(index, word) for word, index in tokenizer.word_index.items()])
    
    bleu1 = bleu2 = bleu3 = bleu4 = meteor_score = 0
    for id in random.sample(list(features), sample_size):
        reference = captions[id].split()[1:-1]
        feature = features[id]
        candidate = predict_caption(trained_model, tokenizer, feature.reshape(1, len(feature)), index_word, max_len)
        candidate = candidate.split()[1:-1]
        
        
        bleu1 += sentence_bleu(reference, " ".join(candidate), weights=(1,0,0,0))
        bleu2 += sentence_bleu(reference, " ".join(candidate), weights=(0.5,0.5,0,0))
        bleu3 += sentence_bleu(reference, " ".join(candidate), weights=(0.33,0.33,0.33,0))
        bleu4 += sentence_bleu(reference, " ".join(candidate), weights=(0.25, 0.25, 0.25, 0.25))
        meteor_score += single_meteor_score(reference, candidate)
        # print("truth: {}\n predict: {}\n bleu1: {}\n meteor: {}".format(reference, candidate, bleu1, meteor_score))
        # if score > 0:
        #     print("truth: {}\n predict: {}\n score: {}".format(reference, candidate, score))
        
    print("BLEU1 score on {} samples: {}/{} = {:.2f}".format(sample_size, bleu1, sample_size, float(bleu1) / sample_size))
    print("BLEU2 score on {} samples: {}/{} = {:.2f}".format(sample_size, bleu2, sample_size, float(bleu2) / sample_size))
    print("BLEU3 score on {} samples: {}/{} = {:.2f}".format(sample_size, bleu3, sample_size, float(bleu3) / sample_size))
    print("BLEU4 score on {} samples: {}/{} = {:.2f}".format(sample_size, bleu4, sample_size, float(bleu4) / sample_size))
    print("METEOR score on {} samples: {}/{} = {:.2f}".format(sample_size, meteor_score, sample_size, float(meteor_score) / sample_size))
    


def printHelp():
    print(
        "Invalid input. Run the code using one of the following formats:\n"
        "1)\t train [cnn_model = {vgg, resnet, squeezenet}] [rnn_model = {gru, lstm}] [optional: path to training features] [optional: path to validation features]\n"
        "2)\t evaluate [cnn_model = {vgg, resnet, squeezenet}] [path to trained_dict] [optional: path to testing features]\n\n"
        "2)\t bleu [path to trained_dict] [path to features] [path to captions] [sample size]\n\n"
        "For example, to train on ResNet and LSTM, use\n"
        "train resnet lstm"
        )


def main(argv):

    if argv[1] == "train":
        train_feature_path, val_feature_path = None, None
        if len(argv) == 6:
            train_feature_path, val_feature_path = argv[4], argv[5]

        print(argv[3])
        train(argv[2], argv[3], train_feature_path, val_feature_path)

    elif argv[1] == "evaluate":
        path = argv[3]

        if not os.path.exists(path):
            print("Path does not exists")
            return

        with open(path, 'rb') as f:
            trained_dict = pickle.load(f)

        trained_model, tokenizer, max_len = trained_dict['model'], trained_dict['tokenizer'], trained_dict['max_len']
        
        testing_feature_path = None
        if len(argv) == 5:
            testing_feature_path = argv[4]
        eval(trained_model, argv[2], tokenizer, max_len, testing_feature_path)
        
    elif argv[1] == "bleu":
        with open(argv[2], 'rb') as f:
            trained_dict = pickle.load(f)
        with open(argv[3], 'rb') as f:
            features = pickle.load(f)
        
        trained_model, tokenizer, max_len = trained_dict['model'], trained_dict['tokenizer'], trained_dict['max_len']
        
        captions = preprocess_captions(argv[4])
        getEvalScores(trained_model, tokenizer, max_len, features, captions, int(argv[5]))
    
    else:
        printHelp()


if __name__ == "__main__":
    main(sys.argv)



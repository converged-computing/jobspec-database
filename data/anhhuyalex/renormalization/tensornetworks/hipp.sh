#!/bin/bash
#SBATCH --job-name=hipp_test
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH --time=24:00:00
#SBATCH --output imagenet-%J.log
#SBATCH --mail-type=end
#SBATCH -o slurms/random%j.out

#SBATCH --mail-user=alexn@uni.minerva.edu
# SBATCH --partition=della-gpu
# SBATCH --gres=gpu:1

# SBATCH --mail-type=ALL
# SBATCH --mail-user=alexn@minerva.kgi.edu
source activate renormalization
# export MODEL_SAVE_FOLDER='/home/an633/project/CuriousContrast/results_alex'
# srun --pty -p gpu -c 2 -t 4:00:00 --gres=gpu:1 --mem-per-cpu=10G bash
# sbatch hipp.sh 2 && sbatch hipp.sh 4 && sbatch hipp.sh 8 && sbatch hipp.sh 12 && sbatch hipp.sh 20 && sbatch hipp.sh 30 && sbatch hipp.sh 40 
# for num_inputs in {2..60..2}; do sbatch hipp.sh $num_inputs; done
# for num_inputs in {2..60..2}; do for first_layer_l1_regularize in 0.0 0.00001 0.00005 0.0001 0.0005 0.001 0.005 0.01; do sbatch hipp.sh $num_inputs $first_layer_l1_regularize; done; done

# for data_rescale in 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1;  do sbatch hipp.sh $data_rescale; done 

# python -u polynomial.py --save_dir /scratch/gpfs/qanguyen/poly_roots --num_inputs $1 --order 5 --num_examples 50000 --model_name mlp_small --random_coefs True --input_strategy random  --is_online False --output_strategy evaluate_at_0 --sample_strategy roots --noise 0.0 --weight_decay 0.0 --lr 5e-3 --first_layer_l1_regularize $2 --tags vary_l1_regularizer_vary_num_inputs_vary_order 
# python -u polynomial.py --save_dir /scratch/gpfs/qanguyen/poly_roots --num_inputs $1 --order 6 --num_examples 50000 --model_name mlp_small --random_coefs True --input_strategy random  --is_online False --output_strategy evaluate_at_0 --sample_strategy roots --noise 0.0 --weight_decay 0.0 --lr 5e-3 --first_layer_l1_regularize $2 --tags vary_l1_regularizer_vary_num_inputs_vary_order 
# python -u polynomial.py --save_dir /scratch/gpfs/qanguyen/poly_roots --num_inputs $1 --order 4 --num_examples 50000 --model_name mlp_small_repeat --random_coefs True --input_strategy repeat  --is_online False --output_strategy evaluate_at_0 --sample_strategy roots --noise 0.0 --weight_decay 0.0 --lr 5e-3 --first_layer_l1_regularize $2 --tags vary_l1_regularizer_vary_num_inputs_vary_order 
# python -u polynomial.py --save_dir /scratch/gpfs/qanguyen/poly_roots --num_inputs $1 --order 7 --num_examples 50000 --model_name mlp_small_repeat --random_coefs True --input_strategy repeat  --is_online False --output_strategy evaluate_at_0 --sample_strategy roots --noise 0.0 --weight_decay 0.0 --lr 5e-3 --tags vary_l1_regularizer_vary_num_inputs_vary_order 
# python -u imagenet.py --model_name resnet18 --num_train_epochs 90 --resume resnet18_rep_1673614260.528039.pkl
# python -u imagenet_devel.py --model_name resnet18 --num_train_epochs 90 --data_rescale $1 #--resume resnet18_rep_1673625654.48548.pkl
# python imagenet_devel.py -a resnet18 --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization --data_rescale $1 --epochs 36 --scheduler_step_size 12 --zero_out all_except_center
# python imagenet_devel.py -a resnet18 --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs 36 --scheduler_step_size 12  --image_transform_loader TileImagenet --tiling_imagenet  $1 --tiling_orientation_ablation True
# for tiling in "1,1" "1,2" "1,3" "2,1" "2,2" "2,3" "3,1" "3,2" "3,3"; do sbatch hipp.sh $tiling; done
# for tiling in "3,4" "3,5" "3,6" "4,3" "4,4" "4,4" "5,3" "5,4" "5,5"; do sbatch hipp.sh $tiling; done
# for tiling in 1 2 3 4 5 ; do sbatch hipp.sh $tiling; done
# python imagenet.py -a resnet18 --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs 24 --scheduler_step_size 8  --image_transform_loader TileImagenet --tiling_imagenet  $1 --tiling_orientation_ablation no_ablation --gaussian_blur True --max_sigma 4.0 --fileprefix AUGSTILINGmar23
# python imagenet.py -a resnet18 --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs 24 --scheduler_step_size 8  --image_transform_loader TileImagenet --tiling_imagenet  $1 --tiling_orientation_ablation orientation --gaussian_blur False --max_sigma 0.0 --fileprefix AUGSTILINGmar23 
# for i in 9 8 7 6 5 4 3 2 1; do sbatch hipp.sh $i; done
# python imagenet_ensemble.py -a resnet18 --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs 24 --scheduler_step_size 8  --image_transform_loader TileImagenet --num_models_ensemble $1
# python imagenet_vmap_ensemble.py -a resnet18 --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs 24 --scheduler_step_size 8  --image_transform_loader TileImagenet --num_independent_samples num_models_ensemble --num_models_ensemble $1 --fileprefix Ensemble24EpochsMar19TwoStageAug  --gaussian_blur False --max_sigma 0.0

# for growth_factor in 1.0 1.3 1.6 1.9 2.2 2.5 2.8 3.1;  do sbatch hipp.sh $growth_factor; done 
# python imagenet_devel.py -a resnet18 --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization --data_rescale 0.3 --epochs 36 --scheduler_step_size 12 --zero_out grow_from_center --growth_factor $1



# sbatch --array=0-239 hipp.sh 0.0
# i_vals=(10 20 50 75 100 150 200 250 500 750 1000 1500 2000 2500 5000 6000 7500 10000 12500 15000 17500 20000 60000) # 6
# j_vals=(1 3 5 7 9 10 20 50 75 100 200 250 500 1000 1250 1500 2000 2500 5000 7500 10000 12500 15000 17500 20000 60000) 
# j_vals=(1 3 5 7 9 10 20 50 75 100 200 250 500 1000 1250 1500 2000 2500 5000 7500 10000 12500 15000 17500 20000 60000)
# i_vals=(28 27 25 23 21 19 17 15 13 11 9 7 5 3 1) # 15
i_vals=(0.0 0.005 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.99) # len 30
j_vals=(500 1000 10000 20000 30000 40000 50000 60000) # 6
# j_vals=(10 100 500 1000 2000 5000 10000) # 7



i=${i_vals[$SLURM_ARRAY_TASK_ID / ${#j_vals[@]}]}
j=${j_vals[$SLURM_ARRAY_TASK_ID % ${#j_vals[@]}]}
echo "SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_ID, i = $i, j = $j"
# echo "i = $i"
# i=$1

# python imagenet.py -a coarsegrain_attention --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs 24 --scheduler_step_size 8 --lr 0.03  --image_transform_loader CoarseGrainImagenet --coarsegrain_blocksize  $i --fileprefix coarsegraining_attention_APR25_lr0.03
# i=3; j=100; python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs 5 --scheduler_step_size 5 --lr 0.01 --image_transform_loader SubsampleImagenet --coarsegrain_blocksize  $i --num_hidden_features $j  --fileprefix randomfeatures_MAY11
# j=50; k=0.001; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --scheduler_step_size 5 --lr 0.01 --image_transform_loader SubsampleImagenet --coarsegrain_blocksize  $i --num_hidden_features $j --train_fraction $k  --fileprefix randomfeatures_MAY24
# for i in {1..35}; do j=3001; k=1001; epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.01 --image_transform_loader SubsampleImagenet --nonlinearity line --coarsegrain_blocksize  $i --num_hidden_features $j --num_train_samples $k  --fileprefix randomfeatures_JUN2; done
# for i in {36..60..7}; do j=3001; k=1001; epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.01 --image_transform_loader SubsampleImagenet --nonlinearity line --coarsegrain_blocksize  $i --num_hidden_features $j --num_train_samples $k  --fileprefix randomfeatures_JUN2; done
# for k in {1..35}; do epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.001 --image_transform_loader SubsampleImagenet --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUN18; done
# for k in {36..60..7}; do epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.001 --image_transform_loader SubsampleImagenet --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUN18; done
# for k in {1..35}; do epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.00001 --image_transform_loader SubsampleImagenet --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUN18; done
# for k in {36..60..7}; do epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.00001 --image_transform_loader SubsampleImagenet --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUN18; done
# for k in {1..10}; do epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet_classification.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.001 --image_transform_loader RandomFeaturesClassification --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUL11; done
# for k in {10..36..2}; do epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet_classification.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.001 --image_transform_loader RandomFeaturesClassification --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUL11; done
# for k in {36..60..7}; do epochs=100; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet_classification.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.001 --image_transform_loader RandomFeaturesClassification --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUL11; done
# epochs=100; i=100; j=100; k=5; CUDA_VISIBLE_DEVICES=0 python randomfeatures_imagenet_classification.py -a randomfeatures --dist-url 'tcp://127.0.0.1' --dist-backend 'nccl' --world-size 1 --rank 0 /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --epochs $epochs --lr 0.01 --image_transform_loader RandomFeaturesClassification --nonlinearity line --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix randomfeatures_JUN18 
# for k in {1..10}; do python randomfeatures_imagenet_regression.py /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix lr1gradient_descent_noisyJUL25 --nonlinearity line --SNR 0.2 --train_method gradient_descent --epochs 1000 --lr 1.0 ; done
# for k in {10..36..2}; do python randomfeatures_imagenet_regression.py /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix lr1gradient_descent_noisyJUL25 --nonlinearity line --SNR 0.2 --train_method gradient_descent --epochs 1000 --lr 1.0 ; done
# for k in {36..60..7}; do python randomfeatures_imagenet_regression.py /scratch/gpfs/DATASETS/imagenet/ilsvrc_2012_classification_localization  --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix lr1gradient_descent_noisyJUL25 --nonlinearity line --SNR 0.2 --train_method gradient_descent --epochs 1000 --lr 1.0; done
# for k in 1 2 3 4 5 6 8 10 15; do python -u mnist_classification.py ./data  --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix shuffle_pixels_upsample_calibrated_relu_randomfeatures_lr0.001_wd1em5_mnistOCT5 --lr 0.001 --shuffle_pixels --nonlinearity relu --avg_kernel average --train_method gradient_descent --epochs 150  --upsample  ; done # --upsample   --wd 0.0000000000000000000000001
# for k in 1 2 3 4 5 6 8 10 15; do python -u mnist_classification_wholenetwork.py ./data  --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix upsample_calibrated_relu_wholenetwork_lr0.1_wd1em5_mnistAUG29 --lr 0.1 --nonlinearity relu --train_method gradient_descent --epochs 150  --upsample; done #    --wd 0.0000000000000000000000001
# for k in 1 2 3 4 5 6 7 8 9 11 17; do python -u cifar_classification_randomfeatures.py ./data  --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix tanhlr0.1_wd1em5_cifarAUG15 --nonlinearity tanh --train_method gradient_descent --epochs 150 --lr 0.1; done #    --wd 0.0000000000000000000000001
# for k in 1 2 3 4 5 6 7 9 11 32; do python -u cifar_classification_wholenetwork.py ./data  --coarsegrain_blocksize  $k --num_hidden_features $i --num_train_samples $j  --fileprefix relulr0.1_wd1em5_cifar_fullnetOCT30  --lr 0.1 --nonlinearity line --train_method gradient_descent  --avg_kernel average --epochs 150 --upsample; done #    --wd 0.0000000000000000000000001
# for k in 1 3 5 7 9 11 13 15 17 19 21 23 25 27 28; do python -u mnist_classification_wholenetwork.py ./data  --target_size  $k --num_hidden_features $i --num_train_samples $j  --fileprefix linelr0.1_mnist_fullnet_fractional_cg_DEC04  --lr 0.1 --nonlinearity line  --epochs 150 --upsample; done #    --wd 0.0000000000000000000000001
# for k in 28 27 25 23 21 19 17 15 13 11 9 7 5 3 1; do python -u mnist_classification_wholenetwork.py ./data  --target_size  $k --num_hidden_features $j --num_train_samples 60000  --fileprefix relulr0.1_mnist_fullnet_fractional_cg_DEC04  --lr 0.1 --nonlinearity relu  --epochs 150 --upsample; done #    --wd 0.0000000000000000000000001
# for i in {0..200}; do python -u mnist_classification_wholenetwork.py ./data --fileprefix linelr0.01_mnist_fullnet_fractional_cg_no_transform_DEC13 --grid_NDP --lr 0.01 --nonlinearity line --epochs 10 --upsample --no_transform; done # --wd 0.0000000000000000000000001 
# for k in 1 3 5 7 9 11 13 15 17 19 21 23 25 27 28; do python -u mnist_classification_lr.py ./data --fileprefix multiclasslr0.01_mnist_fractional_cg_no_transform_DEC21 --lr 0.01   --target_size  $k --num_hidden_features $i --num_train_samples $j --epochs 20 --upsample --no_transform --multiclass_lr; done # --wd 0.0000000000000000000000001 
# for k in 1 3 5 7 9 11 13 15 17 19 21 23 25 27 28; do python -u mnist_classification_lr.py ./data --fileprefix multiclasslr$1_mnist_fractional_cg_no_transform_DEC28 --lr $1   --target_size  $k --num_hidden_features $i --nonlinearity line --multiclass_lr --num_train_samples $j --epochs 20 --upsample --no_transform; done # --wd 0.0000000000000000000000001 
# for k in 1 3 5 7 9 11 13 15 17 19 21 23 25 27 28; do python -u mnist_classification_scale_epochs.py ./data --fileprefix full_batch_no_regularize_multiclasslr$1_mnist_fractional_cg_no_transform_scale_epochs_JAN23 --lr $1 --multiclass_lr  --target_size  $k -b $j --num_hidden_features $i --num_train_samples $j --epochs 500 --upsample --no_transform --wd 0.0; done # --wd 0.0000000000000000000000001 
# 
# for k in 28 27 25 23 21 19 17 15 13 11 9 7 5 3 1; do ; done # --wd 0.0000000000000000000000001 
# python -u mnist_classification_scale_epochs.py ./data --fileprefix Adamonecyclelrschedule_full_batch_no_regularize_multiclasslr$1_mnist_JAN29 --optimizer_type adam --lr $1 --lr_scheduler OneCycleLR --multiclass_lr  --target_size  $i -b 500  --num_train_samples $j --epochs 1000 --upsample --no_transform --wd 0.0
# for k in 28 27 25 23 21 19 17 15 13 11 9 7 5 3 1; do python -u mnist_classification_lbfgs.py --data mnist --penalty none --fileprefix noregularize_multiclasslbfgs_mnist_fractional_cg_no_transform_JAN22   --target_size  $k   --num_train_samples $j     ; done # --wd 0.0000000000000000000000001  
#0.005 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1.0
#1.0 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.005
# python -u mnist_classification_lbfgs.py --data mnist --penalty none --fileprefix highsignal_multiclasslbfgs_pca_mnist_MAR10  --is_high_signal_to_noise True  --target_size  28   --num_train_samples $j --n_pca_components_kept $i --save_dir /scratch/qanguyen/imagenet_info_shuffledpca  # --wd 0.0000000000000000000000001  
# python -u mnist_classification_scale_epochs.py ./data --fileprefix P_10000_Adamonecyclelrschedule_full_batch_no_regularize_relu$1_mnist_MAR4 --optimizer_type adam --lr $1 --lr_scheduler OneCycleLR  --target_size  $i -b 500 --nonlinearity relu --num_train_samples $j --num_hidden_features $2 --epochs 500 --upsample --no_transform --wd 0.0  --randomfeatures --save_dir /scratch/gpfs/qanguyen/imagenet_info_relurandom #
# for k in 1 3 5 7 9 11 13 15 17; do python -u mnist_classification_shuffled_svd.py --data fashionmnist --fileprefix Adamshuffledsvd_highsignal_no_regularize_multiclass_fashionmnist_setPCAtozero_MAR16 --optimizer_type adam --lr 0.001 --lr_scheduler OneCycleLR --multiclass_lr   -b 500  --num_train_samples $j --epochs 2000  --wd 0.0 --is_high_signal_to_noise True --is_shuffle_signal False --highsignal_pca_components_kept $i --save_dir /scratch/qanguyen/imagenet_info; done
# for k in 1 3 5 7 9 11 13 15 17; do python -u mnist_classification_shuffled_svd.py --data fashionmnist --randomfeatures_target_size 7 --fileprefix Adamshuffledsvd_highsignal_fashionmnist_doinversetransform_APR9_wd_$1 --optimizer_type adam --lr 0.001 --lr_scheduler OneCycleLR --multiclass_lr   -b 500  --num_train_samples $j --epochs 2000  --wd $1 --is_task_binary False --is_high_signal_to_noise True --is_shuffle_signal True --is_inverse_transform True --highsignal_pca_components_kept $i --save_dir /scratch/qanguyen/imagenet_info; done
# for k in 1 3 5 7 9 11 13 15 17; do python -u mnist_classification_shuffled_svd.py --data mnist --fileprefix lbfgs_shuffledsvd_highsignal_mnist_noinversetransform_APR23_wd_$1 --optimizer_type lbfgs --lr 0.001 --lr_scheduler OneCycleLR --multiclass_lr   -b 500  --num_train_samples $j --epochs 2000  --wd $1 --is_task_binary False --is_high_signal_to_noise True --is_shuffle_signal True --is_inverse_transform False --highsignal_pca_components_kept $i --save_dir /scratch/qanguyen/imagenet_info; done
# for k in 1 3 5 7 9 11 13 15 17; do python -u mnist_classification_shuffled_svd.py --data mnist --fileprefix Adam_gaussianshuffledsvd_highsignal_mnist_doinversetransform_APR30_wd_$1 --optimizer_type adam --lr 0.001 --lr_scheduler OneCycleLR --multiclass_lr   -b 500  --num_train_samples $j --epochs 2000  --wd $1 --is_task_binary False --is_high_signal_to_noise True --is_shuffle_signal Gaussian --is_inverse_transform True --highsignal_pca_components_kept $i --save_dir /scratch/qanguyen/imagenet_info; done
for k in 1 3 5 7 9 11 13 15 17; do python -u mnist_classification_shuffled_svd.py --data mnist --fileprefix sklearnlbfgs_gaussianshuffledsvd_highsignal_mnist_doinversetransform_randominit_MAY02_wd_$1 --optimizer_type sklearn_lbfgs --is_random_init True --lr 0.001 --lr_scheduler OneCycleLR --multiclass_lr   -b 500  --num_train_samples $j  --wd $1 --is_high_signal_to_noise True --is_shuffle_signal Gaussian --is_inverse_transform True --highsignal_pca_components_kept $i --save_dir /scratch/qanguyen/imagenet_info; done
import argparse
import yaml
import torch
import torch.nn
import numpy as np
import pickle

from utils import *
from models import *
from data import *


parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument(
    "-f",
    "--config_file",
    default="configs/config-1.yaml",
    help="Configuration file to load.",
)
ARGS = parser.parse_args(args=[])

with open(ARGS.config_file, "r") as f:
    config = yaml.safe_load(f)
print("Loaded configuration file", ARGS.config_file)
print(config)


if config["seed"] != None:
    torch.cuda.manual_seed_all(config["seed"])

p_val = config["p_val"]
p_test = config["p_test"]
p_train = 1 - p_val - p_test

scale = False
verbose = True

data_name = "AS"
SLURM_ARRAY_JOB_ID = "5044097"
# SLURM_ARRAY_JOB_ID = "5043513"
# SLURM_ARRAY_JOB_ID = "5043514"
# SLURM_ARRAY_JOB_ID = "5043515"
# SLURM_ARRAY_JOB_ID = "5043516"


# SLURM_ARRAY_JOB_ID = "aproova"
print(
    "Evaluating embeddings of "
    + str(data_name)
    + "dataset, generated by JOB ID "
    + str(SLURM_ARRAY_JOB_ID)
)

name = "Results/" + data_name + "_" + SLURM_ARRAY_JOB_ID + "/"
logger = init_logging_handler(name + "evaluation/")
logger.debug(str(config))

device = check_if_gpu()
logger.debug("The code will be running on {}.".format(device))

L = 64

if data_name == "SBM":
    data = Dataset_SBM("datasets/sbm_50t_1000n_adj.csv")
elif data_name == "UCI":
    data = Dataset_UCI(
        (
            "datasets/download.tsv.opsahl-ucsocial.tar.bz2",
            "opsahl-ucsocial/out.opsahl-ucsocial",
        )
    )
elif data_name == "AS":
    # data = Dataset_AS("datasets/as_data")
    with open("datasets/as_data/ASdata_class", "rb") as f:
        data = pickle.load(f)
elif data_name == "BitCoin":
    data = Dataset_BitCoin("datasets/soc-sign-bitcoinotc.csv")
elif data_name == "RM":
    data = Dataset_RM(("datasets/download.tsv.mit.tar.bz2", "mit/out.mit"))

print("Loading embeddings: ", end="")
with open(name + "/saved_embed/mu" + str(L), "rb") as f:
    mu_list = pickle.load(f)
with open(name + "/saved_embed/sigma" + str(L), "rb") as f:
    sigma_list = pickle.load(f)

# mu_list = mu_list[0][2:]
# sigma_list = sigma_list[0][2:]

assert L == len(mu_list[0][0])
print("Embedding size: %d" % (L))


def unison_shuffled_copies(a, b, seed):
    assert len(a) == len(b)
    np.random.seed(seed)
    p = np.random.permutation(len(a))
    return a[p], b[p]


def get_embed_labels(A, t, ones_num, zeroes_num):
    val_edges = np.row_stack((sample_ones(A, ones_num), sample_zeros(A, zeroes_num)))
    val_label = A[val_edges[:, 0], val_edges[:, 1]].A1
    val_edges, val_label = unison_shuffled_copies(val_edges, val_label, seed=t)
    val_edges_mu = np.array(mu_list[t])[val_edges.astype(int)]
    # val_edges_sigma = np.array(sigma_list[t])[val_edges.astype(int)]
    embd_input = []
    for idx in range(len(val_edges)):
        embd_input.append(
            np.concatenate((val_edges_mu[idx][0], val_edges_mu[idx][1]), axis=0)
        )
    embd_input = torch.tensor(np.asarray(embd_input)).to(device)
    labels = torch.tensor(np.asarray(val_label)).to(device)
    return val_edges, embd_input, labels


logger.debug("Training classifier")
classifier = Classifier(L)
classifier.to(device)
loss_fun = torch.nn.BCEWithLogitsLoss(reduce=None)
optim = torch.optim.Adam(classifier.parameters(), lr=1e-3)

MAP_record = {}
MRR_record = {}
loss_record = {}

num_epochs = 1
for t in range(len(data)):
    logger.debug("timestamp {}".format(t))
    A, X = data[t]
    N_now, D = X.shape
    if t > 0 and N_now > N_prev:
        A = A[:N_prev, :N_prev]
    patience = 10
    wait = 0
    best_MAP = 0
    best_MRR = 0
    if t > 0 and t < p_train * len(data):
        logger.debug("Training")
        ones_num = A.nnz
        if data_name == "SBM":
            zeroes_num = 100 * A.shape[0]
        else:
            zeroes_num = min((A.shape[0] - 1) ** 2 - A.nnz, 10 * A.shape[0])
        for epoch in range(1, num_epochs + 1):
            classifier.train()
            optim.zero_grad()

            val_edges, embd_input, labels = get_embed_labels(
                A, t - 1, ones_num, zeroes_num
            )
            probs_out = classifier(embd_input).squeeze()

            weight = torch.tensor([0.1, 0.9]).to(device)
            weight_ = weight[labels.data.long()]
            loss = torch.mean(weight_ * loss_fun(probs_out, labels))

            loss.backward()
            optim.step()

            MAP = get_MAP_e(probs_out.cpu(), labels.cpu())
            MRR = get_MRR(probs_out.cpu(), labels.cpu(), np.transpose(val_edges))

            logger.debug(
                "L: %d, Epoch: %d, Timestep: %d, Loss: %.4f, MAP: %.4f, MRR: %.4f"
                % (L, epoch, t, loss, MAP, MRR)
            )
            wait += 1
            if MAP > best_MAP:
                best_MAP = MAP
                wait = 0
            if MRR > best_MRR:
                best_MRR = MRR
                wait = 0
            if wait >= patience:
                logger.debug("L: {}, epoch: {:3d}: Early Stopping".format(L, epoch))
                break
    elif t >= (1 - p_test) * len(data):
        logger.debug("Testing")
        classifier.eval()

        ones_num = A.nnz
        if data_name == "SBM":
            zeroes_num = 100 * A.shape[0]
        else:
            zeroes_num = min((A.shape[0] - 1) ** 2 - A.nnz, 50 * A.shape[0])
        val_edges, embd_input, labels = get_embed_labels(A, t - 1, ones_num, zeroes_num)
        probs_out = classifier(embd_input).squeeze()

        weight = torch.tensor([0.1, 0.9]).to(device)
        weight_ = weight[labels.data.long()]
        loss = torch.mean(weight_ * loss_fun(probs_out, labels))

        MAP = get_MAP_e(probs_out.cpu(), labels.cpu())
        MRR = get_MRR(probs_out.cpu(), labels.cpu(), np.transpose(val_edges))

        logger.debug(
            "L:{}, Epoch: {}, Timestep: {}, Loss: {:.4}, MAP: {:.4}, MRR: {:.4}".format(
                L,
                epoch,
                t,
                loss,
                MAP,
                MRR,
            )
        )
        loss_record[t] = loss.cpu().detach().numpy()
        MAP_record[t] = MAP
        MRR_record[t] = MRR
    N_prev = data[t][0].shape[0]


# logger.debug("Training classifier")
# classifier = Classifier(L)
# classifier.to(device)
# loss_fun = torch.nn.BCEWithLogitsLoss(reduce=False)
# optim = torch.optim.Adam(classifier.parameters(), lr=1e-3)

# MAP_record = {}
# MRR_record = {}
# loss_record = {}

# num_epochs = 100
# for epoch in range(num_epochs):
#     for t in range(len(data)):
#         logger.debug("timestamp {}".format(t))
#         A, X = data[t]
#         N_now, D = X.shape
#         if t > 0 and N_now > N_prev:
#             A = A[:N_prev, :N_prev]
#         if t < 35:
#             logger.debug("Training")
#             classifier.train()
#             optim.zero_grad()
#             val_edges, embd_input, labels = get_embed_labels(A, t-1, A.nnz, A.shape[0] * 100)
#             probs_out = classifier(embd_input).squeeze()

#             weight = torch.tensor([0.1, 0.9]).to(device)
#             weight_ = weight[labels.data.long()]
#             loss = torch.mean(weight_ * loss_fun(probs_out, labels))

#             loss.backward()
#             optim.step()

#             MAP = get_MAP_e(probs_out.cpu(), labels.cpu())
#             MRR = get_MRR(probs_out.cpu(), labels.cpu(), np.transpose(val_edges))

#             logger.debug(
#                 "L:{}, Epoch: {}, Timestep: {}, Loss: {:.4}, MAP: {:.4}, MRR: {:.4}".format(
#                     L,
#                     epoch,
#                     t,
#                     loss,
#                     MAP,
#                     MRR,
#                 )
#             )
#         elif t >= 40:
#             logger.debug("Testing")
#             classifier.eval()
#             val_edges, embd_input, labels = get_embed_labels(A, t-1, A.nnz, A.shape[0] * 100)
#             probs_out = classifier(embd_input).squeeze()

#             weight = torch.tensor([0.1, 0.9]).to(device)
#             weight_ = weight[labels.data.long()]
#             loss = torch.mean(weight_ * loss_fun(probs_out, labels))

#             MAP = get_MAP_e(probs_out.cpu(), labels.cpu())
#             MRR = get_MRR(probs_out.cpu(), labels.cpu(), np.transpose(val_edges))

#             logger.debug(
#                 "L:{}, Epoch: {}, Timestep: {}, Loss: {:.4}, MAP: {:.4}, MRR: {:.4}".format(
#                     L,
#                     epoch,
#                     t,
#                     loss,
#                     MAP,
#                     MRR,
#                 )
#             )
#             loss_record[t] = loss
#             MAP_record[t] = MAP
#             MRR_record[t] = MRR
#         N_prev = A.shape[0]

print("mean test loss: " + str(sum(loss_record.values()) / len(loss_record)))
print("mean test MAP: " + str(sum(MAP_record.values()) / len(MAP_record)))
print("mean test MRR: " + str(sum(MRR_record.values()) / len(MRR_record)))

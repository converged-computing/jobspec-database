#!/bin/bash
#PBS -P q95
#PBS -q gpuvolta
#PBS -l walltime=04:00:00
#PBS -l mem=32GB
#PBS -l jobfs=16000MB
#PBS -l ngpus=4
#PBS -l ncpus=48
#PBS -l other=mpi:hyperthread
#PBS -l wd
#PBS -r y
#PBS -l storage=scratch/q95+gdata/q95

## General-purpose resubmit script for GROMACS jobs on gadi

## this runs jobs in four hour blocks with checkpinting and resubmission, 
## edit $GMXMDRUN to vary the runtime
## nsteps is set in the mdp
## Starting structure, .top, .ndx and .mdp should have the same name as the
## script, and all be in the same folder. Output will also have 
## the same name.
## Eg:  if script name is GlyT2_POPC_CHOL_r1, mdp is GlyT2_POPC_CHOL_r1.mdp
##
## IMPORTANT: SCRIPT NAME SHOULD NOT END WITH .sh

echo "CLUSTERID=GADI-gpuvolta" # print which cluster was used, so if a system is run on multiple systems I can separate the log files for benchmarking

module load singularity
module list

GMX='singularity run --nv /g/data/q95/SHARED/gromacs_2023.sif gmx'
GMXMDRUN=GMXMDRUN='singularity run --nv /g/data/q95/SHARED/gromacs_2023.sif gmx mdrun -maxh 3.95'


# Define error function so we can see the error code given when something
# important crashes
errexit ()
{
    errstat=$?
    if [ $errstat != 0 ]; then
        # A brief nap so PBS kills us in normal termination
        # Prefer to be killed by PBS if PBS detected some resource
        # excess
        sleep 5
        echo "Job returned error status $errstat - stopping job sequence $PBS_JOBNAME at job $PBS_JOBID"
        exit $errstat
    fi
}

# Guarantee GPUs are visible
export CUDA_VISIBLE_DEVICES=$(seq 0 $(( $PBS_NGPUS-1 )) | tr  '\r\n' ',')

# Change to working directory - not necessary with #PBS -l wd
cd $PBS_O_WORKDIR

# Terminate the job sequence if the file STOP_SEQUENCE is found in pwd
if [ -f STOP_SEQUENCE ]; then
    echo "STOP_SEQUENCE file found - terminating job sequence $PBS_JOBNAME at job $PBS_JOBID"
    exit 0
fi

####  GROMACS time!  ####

# Load the gromacs module

module load gromacs/2021.4-gpuvolta

export OMP_NUM_THREADS=12
 

# Notes:
# -cpi: Continue from checkpoint if available, otherwise start new simulation
# -maxh n: Write a checkpoint and terminate after 3.95 hours
# -nb gpu: Die if GPU not usable (unfortunately, won't die if GPU isn't found)

#####  START MD WITH A GROMPP  #####

## GROMPP if there's no TPR file (eg, this is the first submission)
if [ ! -f ${PBS_JOBNAME}.tpr ]; then
    $GMX grompp -f ${PBS_JOBNAME}.mdp -c ${PBS_JOBNAME}_start.gro -o ${PBS_JOBNAME}.tpr -p ${PBS_JOBNAME}.top  -n ${PBS_JOBNAME}.ndx -maxwarn 2 || errexit
fi

#####  RUN MD FOR 3.95 HOURS  #####

 $GMXMDRUN -v -deffnm ${PBS_JOBNAME} -cpi ${PBS_JOBNAME}.cpt || errexit

#####  CHECK IF JOB IS DONE; IF NOT DONE RESUBMIT THIS SCRIPT  #####


# Check the log file for the number of steps completed
steps_done=`perl -n -e'/Statistics over (\d+) steps using (\d+) frames/ && print $1' ${PBS_JOBNAME}.log`
# Check the mdp file for the number of steps we want
steps_wanted=`perl -n -e'/nsteps\s*=\s*(\d+)/ && print $1' ${PBS_JOBNAME}.mdp`
# Resubmit if we need to
if (( steps_done < steps_wanted )); then
    echo "Job ${PBS_JOBID} terminated with ${steps_done}/${steps_wanted} steps finished."
    echo "Submitting next job in sequence $PBS_JOBNAME."
    qsub $PBS_JOBNAME
fi


#####  END  #####

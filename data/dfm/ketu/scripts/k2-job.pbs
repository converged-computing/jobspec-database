#!/bin/bash
#PBS -l nodes=2:ppn=20
#PBS -l walltime=24:00:00
#PBS -l mem=60GB
#PBS -N ketu
#PBS -m ae
#PBS -M danfm@nyu.edu
#PBS -j oe
Â 
module purge
export PATH="$HOME/miniconda3/bin:$PATH"
module load mvapich2/intel/2.0rc1

# Campaign.
export K2C_NUM="c1"

# Locations.
export SRCDIR=$HOME/projects/ketu
export RUNDIR=$SCRATCH/ketu/results/$K2C_NUM
export PROFILEDIR=$RUNDIR/profile
mkdir -p $RUNDIR
cd $RUNDIR

# Note which version of MPI we're using.
echo `which mpiexec` > mpiexec.log

# Set up and start the IPython cluster. Note: this *is* started using MPI!!
# and it should use the mpiexec that I enabled above.
rm -rf $PROFILEDIR
cp -r $HOME/.ipython/profile_mpi $PROFILEDIR
export OMP_NUM_THREADS=1
ipcluster start -n $((PBS_NP-2)) --profile-dir=$PROFILEDIR &> ipcluster.log &

sleep 5
for (( try=0; try < 100; ++try )); do
    if cat ipcluster.log | grep -q "Engines appear to have started successfully"; then
        success=1
        break
    fi
    sleep 5
done

if (( success )); then
    # Run the analysis.
    python $SRCDIR/scripts/ketu-search $SRCDIR/campaigns/$K2C_NUM.mercer.json \
        --profile-dir $PROFILEDIR &> output.log
else
    echo "Server never started" &> output.log
fi

# Shut the cluster down.
ipcluster stop --profile-dir=$PROFILEDIR

exit $(( 1-success ));

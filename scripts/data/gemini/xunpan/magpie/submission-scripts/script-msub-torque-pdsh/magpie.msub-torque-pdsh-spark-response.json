{
    "candidates": [
        {
            "content": {
                "parts": [
                    {
                        "text": "Magpie\n\n```json\n{\n  \"application\": \"Magpie\",\n  \"description\": \"Magpie is a set of scripts designed to run Hadoop and related software on traditional HPC systems, making it easier to utilize the resources of such systems for big data processing.\",\n  \"software_requirements\": [\n    {\n      \"name\": \"Java\",\n      \"version\": \"1.8.0\",\n      \"path\": \"/usr/lib/jvm/jre-1.8.0/\"\n    },\n    {\n      \"name\": \"Python\",\n      \"version\": \"any\",\n      \"path\": \"/usr/bin/python\"\n    },\n    {\n      \"name\": \"Spark\",\n      \"version\": \"3.0.3-bin-hadoop3.2\",\n      \"path\": \"${HOME}/spark-${SPARK_VERSION}\"\n    },\n    {\n      \"name\": \"Hadoop\",\n      \"version\": \"any\",\n      \"path\": \"any\"\n    },\n    {\n      \"name\": \"PDSH\",\n      \"version\": \"any\",\n      \"path\": \"any\"\n    },\n    {\n      \"name\": \"PBS\",\n      \"version\": \"any\",\n      \"path\": \"any\"\n    },\n    {\n      \"name\": \"MOAB\",\n      \"version\": \"any\",\n      \"path\": \"any\"\n    }\n  ],\n  \"resource_requirements\": [\n    {\n      \"name\": \"nodes\",\n      \"description\": \"Number of nodes required for the job, including master node, Hadoop compute nodes, Zookeeper nodes, and other services.\"\n    },\n    {\n      \"name\": \"walltime\",\n      \"description\": \"Time limit for the job in seconds or HH:MM:SS format.\"\n    },\n    {\n      \"name\": \"partition\",\n      \"description\": \"Partition of the cluster to run the job on.\"\n    },\n    {\n      \"name\": \"queue\",\n      \"description\": \"Batch queue to submit the job to.\"\n    },\n    {\n      \"name\": \"memory\",\n      \"description\": \"Memory requirement for each node in the job, calculated based on the total memory available and number of CPUs on the system.\"\n    },\n    {\n      \"name\": \"cores\",\n      \"description\": \"Number of cores per node for the job, calculated based on the number of CPUs on the system.\"\n    },\n    {\n      \"name\": \"local_scratch_directory\",\n      \"description\": \"Directory for local scratch data on each node, typically a fast storage location like local SSDs or NVRAM.\"\n    },\n    {\n      \"name\": \"network_bandwidth\",\n      \"description\": \"Network bandwidth for communication between nodes.\"\n    }\n  ],\n  \"configuration_options\": [\n    {\n      \"name\": \"MAGPIE_SCRIPTS_HOME\",\n      \"description\": \"Directory where Magpie launching scripts and files are stored, typically an NFS mount.\"\n    },\n    {\n      \"name\": \"MAGPIE_LOCAL_DIR\",\n      \"description\": \"Directory for storing local configuration files and log files on each node, typically in /tmp.\"\n    },\n    {\n      \"name\": \"MAGPIE_JOB_TYPE\",\n      \"description\": \"Type of job to run, such as Spark, script, interactive, or testall.\"\n    },\n    {\n      \"name\": \"MAGPIE_JOB_SCRIPT\",\n      \"description\": \"Script to execute for the 'script' job type.\"\n    },\n    {\n      \"name\": \"MAGPIE_STARTUP_TIME\",\n      \"description\": \"Time allowed for job startup activities, defaults to 30 minutes.\"\n    },\n    {\n      \"name\": \"MAGPIE_SHUTDOWN_TIME\",\n      \"description\": \"Time allowed for job shutdown activities, defaults to 30 minutes.\"\n    },\n    {\n      \"name\": \"MAGPIE_ONE_TIME_RUN\",\n      \"description\": \"Whether to run the job as a one-time run, avoiding persistent data storage and complete teardown.\"\n    },\n    {\n      \"name\": \"MAGPIE_PRE_JOB_RUN\",\n      \"description\": \"Script to execute before the job starts, typically for setup or debugging.\"\n    },\n    {\n      \"name\": \"MAGPIE_POST_JOB_RUN\",\n      \"description\": \"Script to execute after the job finishes, typically for cleanup or logging.\"\n    },\n    {\n      \"name\": \"MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT\",\n      \"description\": \"File to write environment variables needed for interactive job access.\"\n    },\n    {\n      \"name\": \"MAGPIE_REMOTE_CMD\",\n      \"description\": \"Command for executing remote shell commands, defaults to ssh.\"\n    },\n    {\n      \"name\": \"SPARK_SETUP\",\n      \"description\": \"Whether to setup Spark, defaults to yes.\"\n    },\n    {\n      \"name\": \"SPARK_SETUP_TYPE\",\n      \"description\": \"Type of Spark setup, either STANDALONE or YARN, defaults to STANDALONE.\"\n    },\n    {\n      \"name\": \"SPARK_VERSION\",\n      \"description\": \"Version of Spark to use.\"\n    },\n    {\n      \"name\": \"SPARK_HOME\",\n      \"description\": \"Path to Spark binaries, typically an NFS mount.\"\n    },\n    {\n      \"name\": \"SPARK_LOCAL_DIR\",\n      \"description\": \"Directory for storing local Spark configuration files and log files.\"\n    },\n    {\n      \"name\": \"SPARK_CONF_FILES\",\n      \"description\": \"Directory for alternate Spark configuration templates.\"\n    },\n    {\n      \"name\": \"SPARK_WORKER_CORES_PER_NODE\",\n      \"description\": \"Number of cores per node to allocate to Spark workers.\"\n    },\n    {\n      \"name\": \"SPARK_WORKER_MEMORY_PER_NODE\",\n      \"description\": \"Amount of memory per node to allocate to Spark workers, specified in MB.\"\n    },\n    {\n      \"name\": \"SPARK_WORKER_DIRECTORY\",\n      \"description\": \"Directory for Spark worker applications, including logs and scratch space.\"\n    },\n    {\n      \"name\": \"SPARK_JOB_MEMORY\",\n      \"description\": \"Memory for Spark jobs, defaults to SPARK_WORKER_MEMORY_PER_NODE.\"\n    },\n    {\n      \"name\": \"SPARK_DRIVER_MEMORY\",\n      \"description\": \"Memory for the Spark driver, can be configured separately from executor memory.\"\n    },\n    {\n      \"name\": \"SPARK_DAEMON_HEAP_MAX\",\n      \"description\": \"Maximum heap size for Spark daemons, specified in MB, defaults to 1000.\"\n    },\n    {\n      \"name\": \"SPARK_ENVIRONMENT_EXTRA_PATH\",\n      \"description\": \"Path to a file containing extra environment variables for Spark.\"\n    },\n    {\n      \"name\": \"SPARK_JOB\",\n      \"description\": \"Specific Spark job to run, such as sparkpi or sparkwordcount.\"\n    },\n    {\n      \"name\": \"SPARK_DEFAULT_PARALLELISM\",\n      \"description\": \"Default number of tasks to use for distributed shuffle operations, defaults to number of compute nodes.\"\n    },\n    {\n      \"name\": \"SPARK_MEMORY_FRACTION\",\n      \"description\": \"Fraction of heap space to use for execution and storage, defaults to 0.6.\"\n    },\n    {\n      \"name\": \"SPARK_MEMORY_STORAGE_FRACTION\",\n      \"description\": \"Fraction of memory to use for storage, immune to eviction, defaults to 0.5.\"\n    },\n    {\n      \"name\": \"SPARK_STORAGE_MEMORY_FRACTION\",\n      \"description\": \"Fraction of heap space to use for Spark's memory cache, defaults to 0.6.\"\n    },\n    {\n      \"name\": \"SPARK_SHUFFLE_MEMORY_FRACTION\",\n      \"description\": \"Fraction of heap space to use for shuffle operations, defaults to 0.2.\"\n    },\n    {\n      \"name\": \"SPARK_RDD_COMPRESS\",\n      \"description\": \"Whether to compress RDDs by default, defaults to true.\"\n    },\n    {\n      \"name\": \"SPARK_IO_COMPRESSION_CODEC\",\n      \"description\": \"Compression codec to use for Spark I/O, defaults to lz4.\"\n    },\n    {\n      \"name\": \"SPARK_DEPLOY_SPREADOUT\",\n      \"description\": \"Whether to spread Spark applications across nodes, defaults to true.\"\n    },\n    {\n      \"name\": \"SPARK_JOB_CLASSPATH\",\n      \"description\": \"CLASSPATH for Spark jobs, may be necessary for running specific code.\"\n    },\n    {\n      \"name\": \"SPARK_JOB_LIBRARY_PATH\",\n      \"description\": \"LIBRARY_PATH for Spark jobs, may be necessary for running specific code.\"\n    },\n    {\n      \"name\": \"SPARK_JOB_JAVA_OPTS\",\n      \"description\": \"Java options for Spark jobs, may be necessary for setting specific Spark options.\"\n    },\n    {\n      \"name\": \"SPARK_LOCAL_SCRATCH_DIR\",\n      \"description\": \"Directory for Spark local scratch data, defaults to calculated directory based on Hadoop setup.\"\n    },\n    {\n      \"name\": \"SPARK_LOCAL_SCRATCH_DIR_CLEAR\",\n      \"description\": \"Whether to clear Spark local scratch directory after the job finishes, defaults to yes.\"\n    },\n    {\n      \"name\": \"SPARK_NETWORK_TIMEOUT\",\n      \"description\": \"Timeout for Spark network operations, specified in seconds, defaults to 120.\"\n    },\n    {\n      \"name\": \"SPARK_YARN_STAGING_DIR\",\n      \"description\": \"Directory for staging Spark files when using YARN, defaults to home directory.\"\n    },\n    {\n      \"name\": \"SPARK_SPARKPI_SLICES\",\n      \"description\": \"Number of slices to use for the sparkpi job, defaults to number of nodes.\"\n    },\n    {\n      \"name\": \"SPARK_SPARKWORDCOUNT_FILE\",\n      \"description\": \"File to use for the sparkwordcount job, specify scheme appropriately.\"\n    },\n    {\n      \"name\": \"SPARK_SPARKWORDCOUNT_COPY_IN_FILE\",\n      \"description\": \"File to copy into the location specified by SPARK_SPARKWORDCOUNT_FILE before running sparkwordcount.\"\n    }\n  ]\n}\n```"
                    }
                ],
                "role": "model"
            },
            "finish_reason": 1,
            "index": 0,
            "safety_ratings": [
                {
                    "category": 9,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 8,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 7,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 10,
                    "probability": 1,
                    "blocked": false
                }
            ],
            "token_count": 0,
            "grounding_attributions": []
        }
    ],
    "usage_metadata": {
        "prompt_token_count": 7173,
        "candidates_token_count": 2421,
        "total_token_count": 9594,
        "cached_content_token_count": 0
    }
}
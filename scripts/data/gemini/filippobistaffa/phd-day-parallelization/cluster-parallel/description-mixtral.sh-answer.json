{
    "application": "llama.cpp",
    "details": {
        "model": "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "format": "[INST] {} [/INST]",
        "dependencies": [
            "python3",
            "pandas"
        ],
        "resources": {
            "time": "10:00",
            "cores": 20,
            "memory": "16G"
        },
        "slurm": {
            "job_name": "llama-cpp-description-mixtral",
            "output_file": "description-mixtral-%j.out",
            "error_file": "description-mixtral-%j.err"
        }
    }
}
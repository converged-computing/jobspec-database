{
    "application": "SLURM",
    "details": {
        "resource_requirements": {
            "nodes": 2,
            "tasks_per_node": 4,
            "cpus_per_task": 12,
            "gpus": 4,
            "time": "00:20:00",
            "account": "trustllm-eu",
            "partition": "develbooster"
        },
        "software_requirements": {
            "container_run.sh": "runs a container",
            "training_script": "the main script to be run",
            "get_curr_file.sh": "finds the current file's directory",
            "configuration.sh": "sets configuration parameters",
            "NeMo": "NVIDIA's deep learning framework",
            "megatron_llama_config": "configuration file for the training"
        },
        "other_parameters": {
            "SRUN_CPUS_PER_TASK": "number of CPUs per task",
            "MASTER_ADDR": "address of the master node",
            "MASTER_PORT": "port for the master node",
            "GLOO_SOCKET_IFNAME": "interface for communication",
            "NCCL_IB_TIMEOUT": "timeout for communication",
            "CUDA_DEVICE_MAX_CONNECTIONS": "maximum number of connections per device",
            "DEVICES_PER_NODE": "number of devices per node",
            "NUM_NODES": "number of nodes",
            "PER_SPLIT_NUM_WORKERS": "number of workers per split",
            "TRAIN_CONFIG_YAML_DIR": "directory containing the configuration file",
            "TRAIN_CONFIG_YAML_NAME": "name of the configuration file",
            "TRAIN_DATA_PREFIX": "prefix for the training data",
            "EVAL_DATA_PREFIX": "prefix for the evaluation data",
            "TEST_DATA_PREFIX": "prefix for the test data",
            "TOKENIZER_VOCAB_FILE": "file containing the tokenizer vocabulary",
            "TOKENIZER_MERGE_FILE": "file containing the tokenizer merges",
            "TOKENIZER_MODEL_FILE": "file containing the tokenizer model",
            "MODEL_CHECKPOINT_DIR": "directory for saving the model checkpoint"
        }
    }
}
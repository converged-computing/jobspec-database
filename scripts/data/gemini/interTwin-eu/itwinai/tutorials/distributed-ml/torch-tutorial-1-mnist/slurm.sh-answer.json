{
    "application": "SLURM",
    "details": {
        "job_name": "distributed_training",
        "account": "intertwin",
        "time": "00:30:00",
        "partition": "batch",
        "nodes": 2,
        "gpus_per_node": 4,
        "cpus_per_gpu": 4,
        "exclusive": true,
        "modules": [
            "Stages/2024",
            "GCC",
            "OpenMPI",
            "CUDA/12",
            "MPI-settings/CUDA",
            "Python",
            "HDF5",
            "PnetCDF",
            "libaio",
            "mpi4py"
        ],
        "distributed_training_modes": [
            "horovod",
            "ddp",
            "deepspeed"
        ],
        "environment_variables": [
            "DIST_MODE",
            "RUN_NAME",
            "TRAINING_CMD",
            "PYTHON_VENV",
            "CUDA_VISIBLE_DEVICES",
            "OMP_NUM_THREADS",
            "MASTER_ADDR",
            "MASTER_PORT",
            "NCCL_IB_DISABLE",
            "NCCL_SOCKET_IFNAME",
            "NCCL_DEBUG"
        ],
        "required_resources": [
            "GPU",
            "CPU",
            "MPI",
            "Python",
            "CUDA",
            "NCCL"
        ],
        "additional_notes": [
            "The script utilizes SLURM for job scheduling and resource management.",
            "The script supports three distributed training modes: Horovod, DDP, and DeepSpeed.",
            "Environment variables are used to configure the training process and specify the training command, python environment, and distribution mode.",
            "The script retrieves information about available GPUs and launches the training process using the specified distributed training mode.",
            "The script includes checks for required environment variables and provides error messages if they are missing."
        ]
    }
}
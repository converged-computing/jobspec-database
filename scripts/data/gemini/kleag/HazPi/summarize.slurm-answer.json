{
    "application": "SLURM",
    "details": {
        "nodes": 1,
        "tasks": 1,
        "gpus": 1,
        "job_name": "hazpi_summarize",
        "partition": "gpu,gpuv100,gpup6000,lasti",
        "mail_type": "start,end,fail",
        "mail_user": "gael.de-chalendar@cea.fr",
        "time": "5:00:00",
        "memory": "50G",
        "environment": {
            "conda": "anaconda/3-5.3.1",
            "conda_env": "hazpi",
            "cuda": "10.1",
            "python": "3"
        },
        "script": "summarize.py",
        "script_arguments": "-checkpoint_path /home/data/jlopez/experiments_all_db/ckp_not_all_vocab/ckp_pre_transformer_4Layers_moreEnc_vocab100_100/epoch_350/fine-tuning/epoch_550/epoch_550_FT_1/ -path_summaries_encoded /home/data/jlopez/headlines/results_complete_db/test/dec_bs_fine-tuning_epochs_350_550_4L_4E_noFilters_encoder2000_FT1/encoded_ngram/ -path_summaries_decoded /home/data/jlopez/headlines/results_complete_db/test/dec_bs_fine-tuning_epochs_350_550_4L_4E_noFilters_encoder2000_FT1/decoded_ngram/ -path_summaries_error /home/data/jlopez/headlines/results_complete_db/test/dec_bs_fine-tuning_epochs_350_550_4L_4E_noFilters_encoder2000_FT1/error_ngram/ -batch_size 32 -num_heads 8 -dff 2048 -num_layers 4 -d_model 128 -encoder_max_vocab 100000 -decoder_max_vocab 100000 -num_layers 4 -vocab_load_dir /home/data/jlopez/experiments_all_db/vocab/ckp_pre_transformer_4Layers_moreEnc_vocab100_100/ -ngram_size 2 -k 6 -len_summary 216 file.txt",
        "libraries": [
            "rsync",
            "nvidia-smi"
        ]
    }
}
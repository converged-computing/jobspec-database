{
    "candidates": [
        {
            "content": {
                "parts": [
                    {
                        "text": "```json\n{\n\"application\": \"deepspeed\",\n\"details\": {\n\"slurm\": {\n\"job_name\": \"llama2-7b-2gpu\",\n\"nodes\": 1,\n\"ntasks_per_node\": 1,\n\"cpus_per_task\": 8,\n\"partition\": \"clara\",\n\"mem\": \"256G\",\n\"time\": \"2-00:00:00\",\n\"gres\": \"gpu:v100:2\",\n\"output\": \"logs/%x-%j.out\",\n\"mail_type\": \"ALL\"\n},\n\"modules\": [\n\"Python\",\n\"PyTorch\"\n],\n\"environment\": \".env/bin/activate\",\n\"pip_packages\": [\n\"transformers\",\n\"typing-extensions\",\n\"datasets\",\n\"torch\",\n\"accelerate\",\n\"pytest\",\n\"scikit-learn\",\n\"evaluate\",\n\"sentencepiece\",\n\"deepspeed\"\n],\n\"environment_variables\": {\n\"GPUS_PER_NODE\": 2,\n\"MASTER_ADDR\": \"scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1\",\n\"MASTER_PORT\": 9907,\n\"NCCL_DEBUG\": \"INFO\",\n\"NCCL_IGNORE_DISABLED_P2P\": 1\n},\n\"model_arguments\": {\n\"MODEL_NAME\": \"meta-llama/Llama-2-7b-hf\",\n\"CACHE_DIR\": \"./cache\",\n\"USE_FAST_TOKENIZER\": false,\n\"MODEL_REVISION\": \"main\",\n\"USE_AUTH_TOKEN\": true,\n\"HUGGING_TOKEN\": \"<BLANKED>\",\n\"TORCH_DTYPE\": \"auto\",\n\"LOW_CPU_MEM_USAGE\": false\n},\n\"data_training_arguments\": {\n\"TRAIN_FILE\": \"./input/health_information_systems_epub.md\",\n\"MAX_TRAIN_SAMPLES\": null,\n\"OVERWRITE_CACHE\": false,\n\"VALIDATION_SPLIT_PERCENTAGE\": 5,\n\"PREPROCESSING_NUM_WORKERS\": 1,\n\"KEEP_LINEBREAKS\": true\n},\n\"training_arguments\": {\n\"OUTPUT_DIR\": \"./trained/7B-8\",\n\"OVERWRITE_OUTPUT_DIR\": true,\n\"DO_TRAIN\": true,\n\"DO_EVAL\": true,\n\"PER_DEVICE_TRAIN_BATCH_SIZE\": 1,\n\"PER_DEVICE_EVAL_BATCH_SIZE\": 1,\n\"BLOCK_SIZE\": 1024,\n\"EVALUATION_STRATEGY\": \"steps\",\n\"EVAL_STEPS\": 50,\n\"LEARNING_RATE\": 3e-4,\n\"WEIGHT_DECAY\": 0.1,\n\"ADAM_BETA1\": 0.9,\n\"ADAM_BETA2\": 0.95,\n\"ADAM_EPSILON\": 1e-5,\n\"MAX_GRAD_NORM\": 1.0,\n\"NUM_TRAIN_EPOCHS\": 8,\n\"LR_SCHEDULER_TYPE\": \"cosine\",\n\"WARMUP_STEPS\": 0,\n\"LOG_LEVEL\": \"passive\",\n\"SAVE_STRATEGY\": \"steps\",\n\"SAVE_STEPS\": 100,\n\"SAVE_TOTAL_LIMIT\": 1,\n\"NO_CUDA\": false,\n\"SEED\": 42,\n\"FP16\": true,\n\"BF16\": false,\n\"HALF_PRECISION_BACKEND\": \"auto\",\n\"DDP_BACKEND\": \"nccl\",\n\"DEEPSPEED\": \"./ds_configs/stage2_offload.json\",\n\"OPTIM\": \"adamw_torch\"\n}\n}\n```\n"
                    }
                ],
                "role": "model"
            },
            "finish_reason": 1,
            "index": 0,
            "safety_ratings": [
                {
                    "category": 9,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 8,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 7,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 10,
                    "probability": 1,
                    "blocked": false
                }
            ],
            "citation_metadata": {
                "citation_sources": [
                    {
                        "start_index": 674,
                        "end_index": 891,
                        "uri": "https://discuss.huggingface.co/t/llama2-7b-uses-128-gb-of-gpu-ram-and-fails-with-oom-or-loss-scale-minimum/47762",
                        "license_": ""
                    },
                    {
                        "start_index": 1072,
                        "end_index": 1242,
                        "uri": "https://discuss.huggingface.co/t/llama2-7b-uses-128-gb-of-gpu-ram-and-fails-with-oom-or-loss-scale-minimum/47762",
                        "license_": ""
                    },
                    {
                        "start_index": 1449,
                        "end_index": 1595,
                        "uri": "https://discuss.huggingface.co/t/llama2-7b-uses-128-gb-of-gpu-ram-and-fails-with-oom-or-loss-scale-minimum/47762",
                        "license_": ""
                    }
                ]
            },
            "token_count": 0,
            "grounding_attributions": []
        }
    ],
    "usage_metadata": {
        "prompt_token_count": 2058,
        "candidates_token_count": 847,
        "total_token_count": 2905,
        "cached_content_token_count": 0
    }
}
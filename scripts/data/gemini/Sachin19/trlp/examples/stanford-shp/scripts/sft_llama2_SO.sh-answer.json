{
    "application": "TorchRun",
    "details": {
        "framework": "PyTorch",
        "script": "scripts/sft_llama2.py",
        "resources": {
            "nodes": 1,
            "processes_per_node": 1,
            "gpu": 1,
            "memory": "50 GB",
            "modules": [
                "cuda-11.1.1",
                "cudnn-11.1.1-v8.0.4.30",
                "gcc-7.4"
            ],
            "environment_variables": {
                "HF_DATASETS_CACHE": "/projects/tir6/general/sachink/huggingface"
            }
        },
        "model": "Llama 2 7B",
        "dataset": "SO",
        "data_dir": "/projects/tir6/general/sachink/personalized-LM/2023/steamshp/data",
        "data_prefix": "sft_meta-llama-Llama-2-7b-chat-hf_",
        "output_dir": "/projects/tir6/general/sachink/personalized-LM/2023/models/0923/sft/llama_SO",
        "learning_rate": "1e-5",
        "max_steps": 5000,
        "streaming": true,
        "gradient_checkpointing": false
    }
}
{
    "application": "SLURM",
    "details": {
        "software": [
            "Bash",
            "Conda",
            "Python",
            "SLURM",
            "CUDA",
            "OpenMind8"
        ],
        "resources": {
            "compute": {
                "cpus": "10",
                "gpus": "1 (a100)"
            },
            "memory": "Not specified in script",
            "time": "03:00:00"
        },
        "libraries": [
            "cruxeval"
        ],
        "datasets": [
            "/om2/user/gua/Documents/verify/verification_dataset_codellama_34b_0.6.jsonl"
        ],
        "models": [
            "codellama/CodeLlama-7b-hf",
            "codellama/CodeLlama-13b-hf",
            "codellama/CodeLlama-34b-hf",
            "codellama/CodeLlama-7b-Python-hf",
            "codellama/CodeLlama-13b-Python-hf",
            "codellama/CodeLlama-34b-Python-hf",
            "allenai/codetulu-2-34b",
            "deepseek-ai/deepseek-coder-1.3b-base",
            "deepseek-ai/deepseek-coder-6.7b-base",
            "deepseek-ai/deepseek-coder-33b-base",
            "deepseek-ai/deepseek-coder-1.3b-instruct",
            "deepseek-ai/deepseek-coder-6.7b-instruct",
            "deepseek-ai/deepseek-coder-33b-instruct",
            "ise-uiuc/Magicoder-S-DS-6.7B",
            "mistralai/Mistral-7B-v0.1",
            "mistralai/Mixtral-8x7B-v0.1",
            "microsoft/phi-1",
            "microsoft/phi-1_5",
            "microsoft/phi-2",
            "Phind/Phind-CodeLlama-34B-v2",
            "bigcode/starcoderbase-7b",
            "bigcode/starcoderbase",
            "bigcode/starcoder",
            "WizardLM/WizardCoder-Python-13B-V1.0",
            "WizardLM/WizardCoder-Python-34B-V1.0"
        ],
        "output_format": "JSON",
        "output_path": "/om2/user/gua/Documents/verify/inference/model_generations_raw/{$dir}/shard_{$i}.json"
    }
}
{
    "application": "SLURM",
    "details": {
        "scheduler": "SLURM",
        "job_name": "eval_all_pruner",
        "tasks": 1,
        "partition": "a800",
        "gpus": 1,
        "output_file": "/remote-home/zyzeng/collie/logs/eval/eval_all_%A_%a.out",
        "error_file": "/remote-home/zyzeng/collie/logs/eval/eval_all_%A_%a.err",
        "master_port": "12345",
        "python_path": "/remote-home/zyzeng/miniconda3/envs/collie/bin/python",
        "dynamic_incremental": "Incremental_Chunk_Streaming_Dynamic_History",
        "chunk_streaming": "Chunk_Streaming",
        "perceiver_path": "ckpts/llmllama2_7b#fuserllm#memoryIncremental_Chunk_Streaming_Dynamic_History#lr2e-05#chunk512#compress64/epoch_0-batch_1000/",
        "cuda_home": "/remote-home/zyzeng/cuda-11.8",
        "num_gpus": 4,
        "memory_per_cpu": "4G",
        "python_script": "/remote-home/zyzeng/collie/examples/mem_perceiver/finetune_mem_perceive.py",
        "fuser_type": "llm",
        "do_train": true,
        "lr": 2e-05,
        "memory_type": "Incremental_Chunk_Streaming_Dynamic_History",
        "llm_model": "llama2_7b",
        "num_train_samples": 122070,
        "num_eval_samples": 128,
        "chunk_size": 512,
        "use_flash": true,
        "compressed_chunk_size": 64,
        "temperature": 1.0,
        "eval_every": 1000,
        "eval_len": 16384
    }
}
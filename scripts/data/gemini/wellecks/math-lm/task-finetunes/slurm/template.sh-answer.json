{
    "application": "SLURM",
    "details": {
        "job_name": "mathlm",
        "partition": "g40423",
        "memory": "16GB per CPU",
        "nodes": 2,
        "tasks_per_node": 8,
        "cores_per_task": 6,
        "gpus": 8,
        "output_file": "deploy_train_6B.out",
        "error_file": "deploy_train_6B.out",
        "open_mode": "append",
        "exclusive": true,
        "comment": "eleuther",
        "environment": {
            "conda_setup": "/fsx/hailey/conda_setup.sh",
            "NCCL_DEBUG": "WARN",
            "NCCL_TREE_THRESHOLD": 0,
            "NCCL_PROTO": "simple",
            "NCCL_IBEXT_DISABLE": 1,
            "NCCL_SOCKET_IFNAME": "^docker0,lo",
            "FI_EFA_FORK_SAFE": 1,
            "FI_EFA_USE_DEVICE_RDMA": 1,
            "FI_EFA_ENABLE_SHM_TRANSFER": 0,
            "FI_PROVIDER": "efa",
            "FI_EFA_TX_MIN_CREDITS": 64,
            "PYTHONFAULTHANDLER": 1,
            "OMPI_MCA_mtl_base_verbose": 1,
            "OMPI_MCA_btl": "^openib",
            "HOSTNAMES": "from scontrol show hostnames",
            "MASTER_ADDR": "first hostname from scontrol show hostnames",
            "MASTER_PORT": 12802,
            "COUNT_NODE": "number of hostnames from scontrol show hostnames",
            "TORCHELASTIC_ERROR_FILE": "$TRAIN_PATH/tmp/torch-elastic-error.json",
            "TORCH_EXTENSIONS_DIR": "./extensions/",
            "TRAIN_PATH": "/fsx/mathlm0/gpt-neox",
            "DLTS_HOSTFILE": "/fsx/zhangir.azerbayev/hostfiles/hosts_$SLURM_JOBID",
            "WANDB_API_KEY": "xxx"
        },
        "python_script": {
            "path": "$TRAIN_PATH/deepy.py",
            "arguments": [
                "$TRAIN_PATH/train.py",
                "--conf_dir",
                "/fsx/mathlm0/gpt-neox/configs",
                "mathlm0_6B_train.yml"
            ]
        }
    }
}
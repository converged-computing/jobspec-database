{
    "application": "SLURM",
    "details": {
        "scheduler": "SLURM",
        "job_name": "principled-pre-training",
        "nodes": 1,
        "gpus": 2,
        "cpus_per_task": 32,
        "account": "nlp",
        "partition": "nlp",
        "nodes_list": "nlp-ada-2,nlp-a40-1",
        "stdout": "fine_tuning_runs/slurm_%N_%j_out.txt",
        "stderr": "fine_tuning_runs/slurm_%N_%j_err.txt",
        "mail_type": "fail",
        "mail_user": "zachary@campus.technion.ac.il",
        "environment": "virtual environment '.depth'",
        "framework": "deepspeed",
        "model": "DEPTH",
        "precision": "bf16",
        "dataset": "SParxiv",
        "optimizer": "adamw_torch",
        "learning_rate": "1e-5",
        "batch_size": 64,
        "total_steps": "1_500",
        "warmup_steps": 100,
        "gradient_accumulation": 1,
        "lr_scheduler": "linear",
        "checkpoint_path": "checkpoints/pre_train/from_scratch/depth/c4_en/lr_0_0001_linear_bsz_200_shuffle_p_0_5/2024-03-11_18-50/checkpoint-1000000",
        "resume_training": "false",
        "checkpoint_frequency": "4_000 steps",
        "checkpoint_limit": 3,
        "logging_frequency": "10 steps",
        "wandb": "true",
        "evaluation_frequency": "100 steps"
    }
}
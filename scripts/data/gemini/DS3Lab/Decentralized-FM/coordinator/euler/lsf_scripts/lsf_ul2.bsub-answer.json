{
    "application": "Python",
    "software": [
        "gcc/6.3.0",
        "cuda/11.0.3",
        "pipeline (conda environment)",
        "nvidia-smi",
        "dist_inference_runner_w_euler_coordinator.py"
    ],
    "resource_requirements": {
        "cores": 2,
        "runtime": "3 minutes",
        "memory": "8 GB per core",
        "gpus": 1,
        "gpu_memory": "10 GB or more"
    },
    "environment_variables": [
        "NCCL_SOCKET_IFNAME=access",
        "GLOO_SOCKET_IFNAME=access",
        "NCCL_DEBUG=INFO",
        "NCCL_IB_DISABLE=1",
        "NCCL_P2P_DISABLE=1"
    ],
    "other": [
        "Uses a distributed inference runner with a coordinator server",
        "Loads a T5 model from a specified location",
        "Sets a budget for inference",
        "Specifies the number of layers to use for inference"
    ]
}
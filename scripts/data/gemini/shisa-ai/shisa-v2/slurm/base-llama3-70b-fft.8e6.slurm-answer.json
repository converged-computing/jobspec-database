{
    "application": "Axolotl",
    "details": {
        "software": [
            "Axolotl",
            "Accelerate",
            "PyTorch",
            "NCCL",
            "WandB",
            "SLURM",
            "Mamba"
        ],
        "resources": [
            "GPUs",
            "CPU",
            "RAM",
            "Network",
            "File System (Fsx)",
            "Slurm (High Performance Computing System)"
        ],
        "configuration": {
            "nodes": 2,
            "gpus_per_node": 8,
            "partition": "dev",
            "mixed_precision": "bf16",
            "accelerate_log_level": "info",
            "transformers_verbosity": "info",
            "nccl_async_error_handling": 1,
            "wandb_entity": "augmxnt",
            "wandb_project": "shisa-v2",
            "axolotl_cfg": "llama3-70b-fft.8e6.yaml"
        },
        "script_purpose": "Training a large language model (LLM) with Axolotl on a Slurm cluster."
    }
}
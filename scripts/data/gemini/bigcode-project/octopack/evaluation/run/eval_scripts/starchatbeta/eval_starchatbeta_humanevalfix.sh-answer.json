{
    "application": "accelerate",
    "details": {
        "environment": {
            "python_environment": "/pfs/lustrep2/scratch/project_462000241/muennighoff/venv",
            "working_directory": "/pfs/lustrep2/scratch/project_462000185/muennighoff/bigcode-evaluation-harness"
        },
        "resources": {
            "nodes": 1,
            "tasks_per_node": 1,
            "partition": "small-g",
            "walltime": "48:00:00",
            "gpus": "mi250:1",
            "account": "project_462000241",
            "output": "logs/%j.out",
            "error": "logs/%j.err",
            "hints": "nomultithread",
            "exclusive": "user"
        },
        "script_arguments": {
            "config_file": "config_1gpus_bf16.yaml",
            "main_process_port": "20888",
            "model": "starchat-beta",
            "tasks": "humanevalfixtests-python",
            "do_sample": "True",
            "temperature": "0.2",
            "n_samples": "20",
            "batch_size": "5",
            "allow_code_execution": "True",
            "save_generations": "True",
            "trust_remote_code": "True",
            "prompt": "starchat",
            "save_generations_path": "generations_humanevalfixpython_starchatbeta_temp02.json",
            "metric_output_path": "evaluation_humanevalfixpython_starchatbeta_temp02.json",
            "max_length_generation": "2048",
            "generation_only": "True",
            "precision": "bf16"
        }
    }
}
{
    "application": "DeepSpeed",
    "details": {
        "resource_requirements": {
            "cores": 40,
            "nodes": 1,
            "tasks": 1,
            "time": "20:00:00",
            "partition": "cpu_p1",
            "account": "six@cpu",
            "qos": "qos_cpu-t3"
        },
        "software": {
            "script_language": "bash",
            "python_modules": [
                "deepspeed",
                "transformers",
                "datasets"
            ],
            "external_resources": {
                "tokenizer_path": "bigscience/tokenizer",
                "megatron_deepspeed_repo": "$six_ALL_CCFRWORK/code/tr13f-6B3-ml-t0/Megatron-DeepSpeed",
                "data_path": "/gpfswork/rech/six/commun/bigscience-training/jsonls/xp3cappedmixednewcode",
                "output_path": "/gpfswork/rech/six/commun/bigscience-training/xp3cappedmixednewcodelong"
            }
        },
        "script_functionality": "Preprocessing data for a language model training task. The script merges, shuffles, and tokenizes data from multiple languages. It uses the Megatron-DeepSpeed framework for large-scale model training."
    }
}
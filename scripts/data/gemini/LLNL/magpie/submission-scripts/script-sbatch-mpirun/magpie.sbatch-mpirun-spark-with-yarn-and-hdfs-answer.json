{
    "application": "Magpie",
    "details": {
        "description": "A collection of scripts for running Hadoop and Spark on traditional HPC systems.  Magpie is a comprehensive framework that provides a straightforward method for launching jobs on top of traditional HPC systems without requiring modifications to the underlying HPC resource manager (e.g. SLURM, Torque/PBS, etc.).  It simplifies Hadoop and Spark deployment and setup on traditional HPC systems.",
        "software_requirements": [
            "SLURM",
            "Hadoop (3.3.6)",
            "Spark (3.5.0-bin-hadoop3)",
            "Python3",
            "Java 8"
        ],
        "resource_requirements": [
            "Nodes (configurable)",
            "Time (configurable)",
            "Partition (configurable)",
            "Memory (configurable)",
            "Cores (configurable)"
        ],
        "dependency": "NFS for sharing files",
        "usage": "This script is designed for running Hadoop and Spark jobs.  It sets up the necessary environment variables, configurations, and launches the appropriate daemons."
    }
}
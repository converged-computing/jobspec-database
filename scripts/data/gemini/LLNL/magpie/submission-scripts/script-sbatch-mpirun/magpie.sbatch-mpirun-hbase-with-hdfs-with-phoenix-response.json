{
    "candidates": [
        {
            "content": {
                "parts": [
                    {
                        "text": "Magpie\n```json\n{\n  \"application\": \"Magpie\",\n  \"description\": \"Magpie is a set of scripts for running Hadoop on traditional HPC systems, primarily developed at Lawrence Livermore National Laboratory. It streamlines the process of setting up and running Hadoop, HBase, Zookeeper, and Phoenix on clusters with various filesystem configurations. \",\n  \"software_requirements\": [\n    {\n      \"name\": \"Hadoop\",\n      \"version\": \"2.9.1\",\n      \"setup_types\": [\"MR\", \"YARN\", \"HDFS\"],\n      \"filesystem_modes\": [\"hdfs\", \"hdfsoverlustre\", \"hdfsovernetworkfs\", \"rawnetworkfs\"],\n      \"terasort\": {\n        \"size\": \"10000000000 (1 TB)\",\n        \"map_count\": \"computed based on size and block size\",\n        \"reducer_count\": \"compute node count * 2\"\n      },\n      \"decommission_nodes\": {\n        \"enabled\": \"yes\",\n        \"version_constraint\": \"Hadoop versions >= 2.3.0\"\n      },\n      \"localstore\": {\n        \"purpose\": \"store intermediate data on local SSD/NVRAM\",\n        \"multiple_paths\": \"comma separated list of directories\"\n      }\n    },\n    {\n      \"name\": \"HBase\",\n      \"version\": \"1.6.0\",\n      \"setup_type\": \"HBase\",\n      \"jobs\": [\"performanceeval\"],\n      \"performanceeval\": {\n        \"modes\": [\"sequential-thread\", \"sequential-mr\", \"random-thread\", \"random-mr\"],\n        \"row_count\": \"1048576 (~1 million)\",\n        \"client_count\": \"1\"\n      },\n      \"compaction\": {\n        \"type\": \"major compaction\",\n        \"on_shutdown\": \"yes\",\n        \"script\": \"hbase-major-compaction.sh\"\n      }\n    },\n    {\n      \"name\": \"Phoenix\",\n      \"version\": \"4.14.0-HBase-1.4\",\n      \"setup_type\": \"Phoenix\",\n      \"jobs\": [\"performanceeval\", \"script\"],\n      \"performanceeval\": {\n        \"row_count\": \"100000\"\n      },\n      \"script\": {\n        \"execution\": \"sqline.py ${PHOENIX_SCRIPT_OPTS} ${PHOENIX_SCRIPT_PATH}\"\n      },\n      \"queryserver\": {\n        \"per_regionserver\": \"yes\",\n        \"extra_opts\": \"-Djava.io.tmpdir=${PHOENIX_LOCAL_JOB_DIR}/tmp\"\n      }\n    },\n    {\n      \"name\": \"Zookeeper\",\n      \"version\": \"3.4.14\",\n      \"setup_type\": \"Zookeeper\",\n      \"replication_count\": \"odd number (recommended)\",\n      \"node_sharing\": \"yes/no (defaults to no)\",\n      \"jobs\": [\"zookeeperruok\"],\n      \"data_dir\": {\n        \"networkfs\": \"increase timeouts for Zookeeper\",\n        \"local\": \"local drive\"\n      },\n      \"ticktime\": \"2000 milliseconds\",\n      \"initlimit\": \"10 (local), 20 (networkfs)\",\n      \"synclimit\": \"5 (local), 10 (networkfs)\"\n    }\n  ],\n  \"resource_requirements\": {\n    \"nodes\": {\n      \"description\": \"Node count should include master, compute, Zookeeper nodes, etc.\",\n      \"specification\": \"SBATCH --nodes=<my_node_count>\",\n      \"default_count\": \"not specified\"\n    },\n    \"timelimit\": {\n      \"description\": \"Job time limit in minutes (should be larger than startup/shutdown time)\",\n      \"specification\": \"SBATCH --time=<my_time_in_minutes>\",\n      \"default_value\": \"not specified\"\n    },\n    \"job_name\": {\n      \"description\": \"Job name used for directory naming\",\n      \"specification\": \"SBATCH --job-name=<my_job_name>\",\n      \"default_value\": \"not specified\"\n    },\n    \"partition\": {\n      \"description\": \"Partition to launch job in\",\n      \"specification\": \"SBATCH --partition=<my_partition>\",\n      \"default_value\": \"not specified\"\n    },\n    \"tasks_per_node\": {\n      \"description\": \"Number of tasks per node (defaults to 1)\",\n      \"specification\": \"SBATCH --ntasks-per-node=1\"\n    },\n    \"memory\": {\n      \"description\": \"Daemon heap sizes (Hadoop, HBase, etc.)\",\n      \"default_value\": \"1000 megs (Hadoop daemons), 16000 megs or 50% of system memory (HBase regionservers)\"\n    },\n    \"java_home\": {\n      \"description\": \"Path to Java installation\",\n      \"default_value\": \"/usr/lib/jvm/jre-1.7.0/\"\n    }\n  },\n  \"environment_variables\": {\n    \"scripts_home\": {\n      \"description\": \"Directory where scripts are stored (typically NFS mount)\",\n      \"default_value\": \"${HOME}/magpie\"\n    },\n    \"local_dir\": {\n      \"description\": \"Local directory for storing configuration files and logs (typically /tmp)\",\n      \"default_value\": \"/tmp/${USER}/magpie\"\n    },\n    \"job_type\": {\n      \"description\": \"Type of job to run (Hadoop, HBase, Zookeeper, Phoenix, etc.)\",\n      \"options\": [\n        \"hbase\",\n        \"phoenix\",\n        \"zookeeper\",\n        \"testall\",\n        \"script\",\n        \"interactive\",\n        \"setuponly\"\n      ],\n      \"default_value\": \"phoenix\"\n    },\n    \"job_script\": {\n      \"description\": \"Script to execute for 'script' job type\",\n      \"default_value\": \"${HOME}/my-job-script\"\n    },\n    \"startup_time\": {\n      \"description\": \"Time window for job startup activities\",\n      \"default_value\": \"30 minutes\"\n    },\n    \"shutdown_time\": {\n      \"description\": \"Time window for job shutdown activities\",\n      \"default_value\": \"30 minutes\"\n    },\n    \"one_time_run\": {\n      \"description\": \"Indicates a one-time job run (data is not expected to be reused)\",\n      \"default_value\": \"no\"\n    },\n    \"pre_job_run\": {\n      \"description\": \"Script to run before the main job\",\n      \"default_value\": \"${MAGPIE_SCRIPTS_HOME}/scripts/pre-job-run-scripts/my-pre-job-script\"\n    },\n    \"post_job_run\": {\n      \"description\": \"Script to run after the main job\",\n      \"default_value\": \"${MAGPIE_SCRIPTS_HOME}/scripts/post-job-run-scripts/my-post-job-script\"\n    },\n    \"environment_variable_script\": {\n      \"description\": \"File to store environment variables for interactive use\",\n      \"default_value\": \"${HOME}/my-job-env\"\n    },\n    \"hostname_cmd\": {\n      \"description\": \"Command to determine hostname for configuration\",\n      \"default_value\": \"hostname\"\n    },\n    \"remote_cmd\": {\n      \"description\": \"Remote shell command for launching daemons\",\n      \"default_value\": \"ssh\"\n    },\n    \"remote_cmd_opts\": {\n      \"description\": \"Options for remote shell command\",\n      \"default_value\": \"\"\n    },\n    \"hadoop_setup\": {\n      \"description\": \"Indicates whether Hadoop should be setup\",\n      \"default_value\": \"no\"\n    },\n    \"hadoop_setup_type\": {\n      \"description\": \"Type of Hadoop setup (MR, YARN, HDFS)\",\n      \"default_value\": \"HDFS\"\n    },\n    \"hadoop_version\": {\n      \"description\": \"Hadoop version\",\n      \"default_value\": \"2.9.1\"\n    },\n    \"hadoop_home\": {\n      \"description\": \"Path to Hadoop installation\",\n      \"default_value\": \"${HOME}/hadoop-${HADOOP_VERSION}\"\n    },\n    \"hadoop_local_dir\": {\n      \"description\": \"Local directory for Hadoop configuration files and logs\",\n      \"default_value\": \"/tmp/${USER}/hadoop\"\n    },\n    \"hadoop_conf_files\": {\n      \"description\": \"Directory for alternate Hadoop configuration templates\",\n      \"default_value\": \"${HOME}/myconf\"\n    },\n    \"hadoop_daemon_heap_max\": {\n      \"description\": \"Heap maximum for Hadoop daemons (excluding Namenode)\",\n      \"default_value\": \"1000 megs\"\n    },\n    \"hadoop_namenode_daemon_heap_max\": {\n      \"description\": \"Heap maximum for Hadoop Namenode daemon\",\n      \"default_value\": \"HADOOP_DAEMON_HEAP_MAX\"\n    },\n    \"hadoop_environment_extra_path\": {\n      \"description\": \"File containing extra environment settings for Hadoop\",\n      \"default_value\": \"${HOME}/hadoop-my-environment\"\n    },\n    \"hadoop_job\": {\n      \"description\": \"Job to run for Hadoop\",\n      \"options\": [\n        \"terasort\",\n        \"upgradehdfs\",\n        \"decommissionhdfsnodes\"\n      ],\n      \"default_value\": \"terasort\"\n    },\n    \"hadoop_max_tasks_per_node\": {\n      \"description\": \"Maximum tasks per node (computed based on CPU count)\",\n      \"default_value\": \"not specified\"\n    },\n    \"hadoop_default_map_tasks\": {\n      \"description\": \"Default number of map tasks\",\n      \"default_value\": \"HADOOP_MAX_TASKS_PER_NODE * compute nodes\"\n    },\n    \"hadoop_default_reduce_tasks\": {\n      \"description\": \"Default number of reduce tasks\",\n      \"default_value\": \"compute node count\"\n    },\n    \"hadoop_child_heapsize\": {\n      \"description\": \"Heap size for Yarn containers\",\n      \"default_value\": \"computed based on total memory and CPU count\"\n    },\n    \"hadoop_child_map_heapsize\": {\n      \"description\": \"Heap size for map containers\",\n      \"default_value\": \"HADOOP_CHILD_HEAPSIZE\"\n    },\n    \"hadoop_child_reduce_heapsize\": {\n      \"description\": \"Heap size for reduce containers\",\n      \"default_value\": \"2 * HADOOP_CHILD_MAP_HEAPSIZE\"\n    },\n    \"hadoop_child_map_container_buffer\": {\n      \"description\": \"Overhead for map containers\",\n      \"default_value\": \"computed based on total memory\"\n    },\n    \"hadoop_child_reduce_container_buffer\": {\n      \"description\": \"Overhead for reduce containers\",\n      \"default_value\": \"computed based on total memory\"\n    },\n    \"hadoop_mapreduce_slowstart\": {\n      \"description\": \"Percentage of maps to complete before reducers start\",\n      \"default_value\": \"0.05\"\n    },\n    \"yarn_resource_memory\": {\n      \"description\": \"Memory for Yarn containers\",\n      \"default_value\": \"computed based on total memory\"\n    },\n    \"yarn_vmem_check\": {\n      \"description\": \"Enforce virtual memory limits for containers\",\n      \"default_value\": \"true\"\n    },\n    \"yarn_pmem_check\": {\n      \"description\": \"Enforce physical memory limits for containers\",\n      \"default_value\": \"true\"\n    },\n    \"hadoop_compression\": {\n      \"description\": \"Enable compression of outputs and intermediate data\",\n      \"default_value\": \"no\"\n    },\n    \"hadoop_io_sort_factor\": {\n      \"description\": \"Number of streams to sort during reduce\",\n      \"default_value\": \"computed based on system configuration\"\n    },\n    \"hadoop_io_sort_mb\": {\n      \"description\": \"Memory to use for sorting during reduce\",\n      \"default_value\": \"computed based on system configuration\"\n    },\n    \"hadoop_parallel_copies\": {\n      \"description\": \"Number of parallel transfers during reduce shuffle\",\n      \"default_value\": \"computed based on system configuration\"\n    },\n    \"hadoop_terasort_size\": {\n      \"description\": \"Size of terasort data in units of 100\",\n      \"default_value\": \"50000000\"\n    },\n    \"hadoop_teragen_map_count\": {\n      \"description\": \"Number of map tasks for teragen\",\n      \"default_value\": \"computed based on size and block size\"\n    },\n    \"hadoop_terasort_reducer_count\": {\n      \"description\": \"Number of reduce tasks for terasort\",\n      \"default_value\": \"compute node count * 2\"\n    },\n    \"hadoop_terasort_clear_cache\": {\n      \"description\": \"Flush page cache between teragen and terasort\",\n      \"default_value\": \"yes\"\n    },\n    \"hadoop_terasort_output_replication\": {\n      \"description\": \"Replication factor for terasort output\",\n      \"default_value\": \"1\"\n    },\n    \"hadoop_terasort_run_terachecksum\": {\n      \"description\": \"Calculate checksum of teragen output\",\n      \"default_value\": \"no\"\n    },\n    \"hadoop_terasort_run_teravalidate\": {\n      \"description\": \"Validate sorted data after terasort\",\n      \"default_value\": \"no\"\n    },\n    \"hadoop_decommission_hdfs_node_size\": {\n      \"description\": \"Node size to decommission to\",\n      \"default_value\": \"not specified\"\n    },\n    \"hadoop_filesystem_mode\": {\n      \"description\": \"Filesystem mode for Hadoop\",\n      \"options\": [\n        \"hdfs\",\n        \"hdfsoverlustre\",\n        \"hdfsovernetworkfs\",\n        \"rawnetworkfs\"\n      ],\n      \"default_value\": \"hdfsoverlustre\"\n    },\n    \"hadoop_local_filesystem_blocksize\": {\n      \"description\": \"Block size for local filesystem I/O\",\n      \"default_value\": \"33554432 (32 MB)\"\n    },\n    \"hadoop_hdfs_replication\": {\n      \"description\": \"Replication factor for HDFS\",\n      \"default_value\": \"3\"\n    },\n    \"hadoop_hdfs_blocksize\": {\n      \"description\": \"Block size for HDFS\",\n      \"default_value\": \"134217728 (128 MB)\"\n    },\n    \"hadoop_hdfs_path\": {\n      \"description\": \"Path for HDFS when using local disk\",\n      \"default_value\": \"/ssd/${USER}/hdfs\"\n    },\n    \"hadoop_hdfs_path_clear\": {\n      \"description\": \"Clear HDFS data directory after job completion\",\n      \"default_value\": \"yes\"\n    },\n    \"hadoop_hdfsoverlustre_path\": {\n      \"description\": \"Path for HDFS over Lustre\",\n      \"default_value\": \"/lustre/${USER}/hdfsoverlustre/\"\n    },\n    \"hadoop_hdfsoverlustre_remove_locks\": {\n      \"description\": \"Remove in_use.lock files before launching HDFS\",\n      \"default_value\": \"no\"\n    },\n    \"hadoop_hdfsovernetworkfs_path\": {\n      \"description\": \"Path for HDFS over Network FS\",\n      \"default_value\": \"/networkfs/${USER}/hdfsovernetworkfs/\"\n    },\n    \"hadoop_hdfsovernetworkfs_remove_locks\": {\n      \"description\": \"Remove in_use.lock files before launching HDFS\",\n      \"default_value\": \"no\"\n    },\n    \"hadoop_rawnetworkfs_path\": {\n      \"description\": \"Path for raw networked filesystem\",\n      \"default_value\": \"/lustre/${USER}/rawnetworkfs/\"\n    },\n    \"hbase_setup\": {\n      \"description\": \"Indicates whether HBase should be setup\",\n      \"default_value\": \"no\"\n    },\n    \"hbase_version\": {\n      \"description\": \"HBase version\",\n      \"default_value\": \"1.6.0\"\n    },\n    \"hbase_home\": {\n      \"description\": \"Path to HBase installation\",\n      \"default_value\": \"${HOME}/hbase-${HBASE_VERSION}\"\n    },\n    \"hbase_local_dir\": {\n      \"description\": \"Local directory for HBase configuration files and logs\",\n      \"default_value\": \"/tmp/${USER}/hbase\"\n    },\n    \"hbase_conf_files\": {\n      \"description\": \"Directory for alternate HBase configuration templates\",\n      \"default_value\": \"${HOME}/myconf\"\n    },\n    \"hbase_master_daemon_heap_max\": {\n      \"description\": \"Heap maximum for HBase master daemon\",\n      \"default_value\": \"1000 megs\"\n    },\n    \"hbase_regionserver_daemon_heap_max\": {\n      \"description\": \"Heap maximum for HBase regionserver daemon\",\n      \"default_value\": \"16000 megs or 50% of system memory\"\n    },\n    \"hbase_environment_extra_path\": {\n      \"description\": \"File containing extra environment settings for HBase\",\n      \"default_value\": \"${HOME}/hbase-my-environment\"\n    },\n    \"hbase_job\": {\n      \"description\": \"Job to run for HBase\",\n      \"options\": [\n        \"performanceeval\"\n      ],\n      \"default_value\": \"performanceeval\"\n    },\n    \"hbase_start_thrift_server\": {\n      \"description\": \"Start HBase with thrift server\",\n      \"default_value\": \"no\"\n    },\n    \"hbase_major_compaction_on_shutdown\": {\n      \"description\": \"Perform major compaction when HBase shuts down\",\n      \"default_value\": \"yes\"\n    },\n    \"hbase_performanceeval_mode\": {\n      \"description\": \"Mode for HBase performance evaluation\",\n      \"options\": [\n        \"sequential-thread\",\n        \"sequential-mr\",\n        \"random-thread\",\n        \"random-mr\"\n      ],\n      \"default_value\": \"sequential-thread\"\n    },\n    \"hbase_performanceeval_row_count\": {\n      \"description\": \"Number of rows for HBase performance evaluation\",\n      \"default_value\": \"1048576 (~1 million)\"\n    },\n    \"hbase_performanceeval_client_count\": {\n      \"description\": \"Number of clients for HBase performance evaluation\",\n      \"default_value\": \"1\"\n    },\n    \"phoenix_setup\": {\n      \"description\": \"Indicates whether Phoenix should be setup\",\n      \"default_value\": \"no\"\n    },\n    \"phoenix_version\": {\n      \"description\": \"Phoenix version\",\n      \"default_value\": \"4.14.0-HBase-1.4\"\n    },\n    \"phoenix_home\": {\n      \"description\": \"Path to Phoenix installation\",\n      \"default_value\": \"${HOME}/apache-phoenix-${PHOENIX_VERSION}-bin\"\n    },\n    \"phoenix_local_dir\": {\n      \"description\": \"Local directory for Phoenix configuration files and logs\",\n      \"default_value\": \"/tmp/${USER}/phoenix\"\n    },\n    \"phoenix_conf_files\": {\n      \"description\": \"Directory for alternate Phoenix configuration templates\",\n      \"default_value\": \"${HOME}/myconf\"\n    },\n    \"phoenix_start_queryserver\": {\n      \"description\": \"Start Phoenix with Query Server\",\n      \"default_value\": \"no\"\n    },\n    \"phoenix_job\": {\n      \"description\": \"Job to run for Phoenix\",\n      \"options\": [\n        \"performanceeval\",\n        \"script\"\n      ],\n      \"default_value\": \"performanceeval\"\n    },\n    \"phoenix_script_path\": {\n      \"description\": \"Path to Phoenix script for 'script' job type\",\n      \"default_value\": \"${HOME}/my-phoenix-script\"\n    },\n    \"phoenix_script_opts\": {\n      \"description\": \"Options for running Phoenix script\",\n      \"default_value\": \"\"\n    },\n    \"phoenix_queryserver_extra_opts\": {\n      \"description\": \"Extra Java runtime options for Phoenix Query Server\",\n      \"default_value\": \"-Djava.io.tmpdir=${PHOENIX_LOCAL_JOB_DIR}/tmp\"\n    },\n    \"phoenix_performanceeval_row_count\": {\n      \"description\": \"Number of rows for Phoenix performance evaluation\",\n      \"default_value\": \"100000\"\n    },\n    \"zookeeper_setup\": {\n      \"description\": \"Indicates whether Zookeeper should be setup\",\n      \"default_value\": \"no\"\n    },\n    \"zookeeper_replication_count\": {\n      \"description\": \"Replication count for Zookeeper (recommended to be odd)\",\n      \"default_value\": \"3\"\n    },\n    \"zookeeper_share_nodes\": {\n      \"description\": \"Share Zookeeper nodes with Hadoop/HBase\",\n      \"default_value\": \"no\"\n    },\n    \"zookeeper_job\": {\n      \"description\": \"Job to run for Zookeeper\",\n      \"options\": [\n        \"zookeeperruok\"\n      ],\n      \"default_value\": \"zookeeperruok\"\n    },\n    \"zookeeper_version\": {\n      \"description\": \"Zookeeper version\",\n      \"default_value\": \"3.4.14\"\n    },\n    \"zookeeper_home\": {\n      \"description\": \"Path to Zookeeper installation\",\n      \"default_value\": \"${HOME}/zookeeper-${ZOOKEEPER_VERSION}\"\n    },\n    \"zookeeper_conf_files\": {\n      \"description\": \"Directory for alternate Zookeeper configuration templates\",\n      \"default_value\": \"${HOME}/myconf\"\n    },\n    \"zookeeper_data_dir\": {\n      \"description\": \"Path for Zookeeper data\",\n      \"default_value\": \"/lustre/${USER}/zookeeper\"\n    },\n    \"zookeeper_data_dir_clear\": {\n      \"description\": \"Clear Zookeeper data directory after job completion\",\n      \"default_value\": \"yes\"\n    },\n    \"zookeeper_data_dir_type\": {\n      \"description\": \"Type of Zookeeper data directory (networkfs or local)\",\n      \"options\": [\n        \"networkfs\",\n        \"local\"\n      ],\n      \"default_value\": \"networkfs\"\n    },\n    \"zookeeper_local_dir\": {\n      \"description\": \"Local directory for Zookeeper configuration files and logs\",\n      \"default_value\": \"/tmp/${USER}/zookeeper\"\n    },\n    \"zookeeper_ticktime\": {\n      \"description\": \"Tick time for Zookeeper (in milliseconds)\",\n      \"default_value\": \"2000\"\n    },\n    \"zookeeper_initlimit\": {\n      \"description\": \"Init limit for Zookeeper (multiple of ticks)\",\n      \"default_value\": \"10 (local), 20 (networkfs)\"\n    },\n    \"zookeeper_synclimit\": {\n      \"description\": \"Sync limit for Zookeeper (multiple of ticks)\",\n      \"default_value\": \"5 (local), 10 (networkfs)\"\n    }\n  }\n}\n```"
                    }
                ],
                "role": "model"
            },
            "finish_reason": 1,
            "index": 0,
            "safety_ratings": [
                {
                    "category": 9,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 8,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 7,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 10,
                    "probability": 1,
                    "blocked": false
                }
            ],
            "token_count": 0,
            "grounding_attributions": []
        }
    ],
    "usage_metadata": {
        "prompt_token_count": 12140,
        "candidates_token_count": 5487,
        "total_token_count": 17627,
        "cached_content_token_count": 0
    }
}
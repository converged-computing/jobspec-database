{
    "application": "text-generation-inference",
    "details": {
        "framework": "Hugging Face",
        "model": "mistralai/Mistral-7B-Instruct-v0.1",
        "resource_requirements": {
            "gpu": "8",
            "cpu": "12",
            "memory": "11G per cpu",
            "storage": "/scratch"
        },
        "launch_parameters": {
            "max_concurrent_requests": "530",
            "max_total_tokens": "8192",
            "max_input_length": "7168",
            "max_batch_prefill_tokens": "7168"
        },
        "environment_variables": {
            "HUGGING_FACE_HUB_TOKEN": "required",
            "PORT": "dynamically assigned"
        },
        "container_image": "/fsx/loubna/docker_images/huggingface+text-generation-inference.sqsh"
    }
}
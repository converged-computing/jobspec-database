{
    "application": "SLURM",
    "details": {
        "resource_requirements": {
            "partition": "v100_full_node",
            "nodes": 2,
            "tasks": 2,
            "gpus_per_node": 4,
            "error_file": "/scratch/user/OUTPUTS/logs/%A.err",
            "output_file": "/scratch/user/OUTPUTS/logs/%A.out"
        },
        "software": {
            "language": "Python",
            "framework": "PyTorch",
            "library": "DINO",
            "dependency": "nccl"
        },
        "script_functionality": {
            "purpose": "Run a DINO training job on a SLURM cluster",
            "master_node_selection": "Selects a master node using hostname and a random port between 6000-9999",
            "distributed_training": "Uses distributed training with multiprocessing and nccl backend",
            "job_configuration": "Allows specifying a configuration file, a database, and a resume flag",
            "logging": "Logs stdout and stderr to files named after the job id and rank"
        }
    }
}
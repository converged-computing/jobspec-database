{
    "application": "DeepSpeed",
    "details": {
        "framework": "PyTorch",
        "model": "LLAMA 7B MoE (Mixture of Experts)",
        "training_type": "Fine-tuning with LoRA (Low-Rank Adaptation)",
        "optimizer": "AdamW",
        "learning_rate": 0.0002,
        "batch_size": 128,
        "gradient_accumulation_steps": 1,
        "block_size": 2048,
        "max_steps": 48828125,
        "max_train_samples": 48828125,
        "data_source": "/mnt/petrelfs/share_data/quxiaoye/pretrain_LLAMA_all_data_processed",
        "hardware": {
            "nodes": 2,
            "GPUs_per_node": 8,
            "CPU_cores_per_task": 64,
            "compute_resource": "MoE partition"
        },
        "software_dependencies": {
            "anaconda": "smoe environment",
            "deepspeed": "bf16.json config file"
        }
    }
}
{
    "application": "SLURM",
    "details": {
        "job_name": "cpt-llama2_random_scale4_112gpus_dynamic_data",
        "output_file": "/mnt/petrelfs/share_data/quxiaoye/runs/llama2_random_scale4_112gpus_dynamic_data/%x-%j.log",
        "error_file": "/mnt/petrelfs/share_data/quxiaoye/runs/llama2_random_scale4_112gpus_dynamic_data/%x-%j.log",
        "partition": "MoE",
        "tasks_per_node": 1,
        "cpus_per_task": 64,
        "memory": 0,
        "nodes": 14,
        "gpus": 8,
        "quota_type": "reserved",
        "conda_environment": "smoe",
        "model_type": "llama_moe",
        "pretrained_model": "/mnt/petrelfs/share_data/quxiaoye/models/LlamaMoEForCausalLM/Random/llama2_7B-16Select4-up_proj-Scale4.0",
        "tokenizer_path": "/mnt/petrelfs/share_data/quxiaoye/models/llama2_7B",
        "dataset_dir": "/mnt/petrelfs/share_data/quxiaoye/SlimPajama_processed",
        "validation_dir": "/mnt/petrelfs/share_data/quxiaoye/data/llama1_7B_val_set_tokenized",
        "learning_rate": "2e-4",
        "final_lr_portion": "0.1",
        "per_device_train_batch_size": 8,
        "per_device_eval_batch_size": 8,
        "gradient_accumulation_steps": 4,
        "block_size": 4096,
        "num_tokens": "200*10^9",
        "warmup_tokens": "15*10^8",
        "eval_tokens": "2.5*10^9",
        "seed": 1227,
        "deepspeed_config_file": "conf/deepspeed/bf16_zero1_default.json",
        "num_selects": 4,
        "data_cache": "resources/cache",
        "base_dir": "/mnt/petrelfs/share_data/quxiaoye/runs/llama2_random_scale4_112gpus_dynamic_data",
        "output_dir": "$base_dir/outputs/$SLURM_JOB_NAME-$SLURM_JOB_ID",
        "comment": "llama 2 7B, random 4/16",
        "environment_variables": {
            "OMP_NUM_THREADS": "32",
            "LOGLEVEL": "INFO"
        },
        "framework": "PyTorch",
        "library": "deepspeed",
        "training_parameters": {
            "max_steps": "max_steps=$(echo \"${num_tokens} / ($block_size * $per_device_train_batch_size * $gradient_accumulation_steps * $num_nodes * $num_gpu_per_node)\" | bc)",
            "max_train_samples": "max_train_samples=$(echo \"${num_tokens} / ($block_size)\" | bc)",
            "global_batch_size": "global_bs=$(echo \"$per_device_train_batch_size * $gradient_accumulation_steps * $num_nodes * $num_gpu_per_node\" | bc)",
            "tokens_per_batch": "tokens_per_batch=$(echo \"$global_bs * $block_size\" | bc)",
            "warmup_steps": "warmup_steps=$(echo \"$warmup_tokens / ($tokens_per_batch)\" | bc)",
            "eval_steps": "eval_steps=$(echo \"$eval_tokens / ($tokens_per_batch)\" | bc)"
        },
        "distributed_training": {
            "strategy": "DDP",
            "nodes": 14,
            "gpus_per_node": 8,
            "communication_backend": "c10d",
            "rdzv_backend": "c10d"
        },
        "model_architecture": "MoE",
        "gate_type": "TopKBalancedNoisyGate",
        "calculator_type": "UniversalCalculator",
        "data_parallelism": {
            "strategy": "DataParallel"
        }
    }
}
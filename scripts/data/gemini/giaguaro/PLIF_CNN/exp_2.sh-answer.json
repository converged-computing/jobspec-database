{
    "application": "PyTorch",
    "details": {
        "framework": "PyTorch",
        "distributed training": true,
        "cluster management": "SLURM",
        "nodes": 3,
        "GPUs per node": 4,
        "CPUs per task": 6,
        "memory per node": 0,
        "environment": "conda",
        "environment name": "plifs",
        "communication backend": "NCCL",
        "script": "main_distributed.py"
    }
}
{
    "application": "SLURM",
    "details": {
        "resource_requirements": {
            "nodes": 2,
            "time": "04:00:00",
            "account": "m1759",
            "gres": "gpu:8",
            "exclusive": true,
            "cores": 80
        },
        "software": {
            "modules": [
                "pytorch/v1.4.0-gpu"
            ],
            "python": "torch.distributed.launch"
        },
        "environment_variables": {
            "HDF5_USE_FILE_LOCKING": "FALSE"
        },
        "script_description": "This script launches a distributed PyTorch training job across 8 nodes, each with 8 GPUs. It uses SLURM to manage the resources and distribute the training workload."
    }
}
{
    "application": "inference",
    "software": [
        "Python",
        "conda",
        "NVIDIA NCCL",
        "GLOO",
        "dist_inference_runner_w_slurm_coordinator.py"
    ],
    "resource_requirements": {
        "compute": {
            "partition": "sphinx",
            "gpus": 1,
            "cpus": 4,
            "memory": "8G per CPU",
            "time": "3:59:00"
        },
        "network": {
            "interface": "en*"
        },
        "storage": {
            "output": "/afs/cs.stanford.edu/u/biyuan/exe_log/opt_%j.log",
            "model": "/sailhome/biyuan/fm/models/glm-130b-new"
        }
    }
}
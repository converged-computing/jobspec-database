{
    "application": "SLURM",
    "details": {
        "job_name": "pendulum-llama",
        "output_file": "/home/quantinx/finetuning-LLM/logs/llama2-pendulum.out",
        "error_file": "/home/quantinx/finetuning-LLM/logs/llama2-pendulum.err",
        "partition": "general",
        "nodes": 1,
        "tasks_per_node": 1,
        "cpus_per_task": 1,
        "time": "1:00:00",
        "gpu": 1,
        "cuda_version": "12.2",
        "python_packages": "requirements.txt",
        "python_script": "src/run_sft.py",
        "config_file": "recipes/model_configs/sft/config_lora_llama.yaml"
    }
}
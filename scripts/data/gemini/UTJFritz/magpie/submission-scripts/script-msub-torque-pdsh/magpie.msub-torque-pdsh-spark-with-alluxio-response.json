{
    "candidates": [
        {
            "content": {
                "parts": [
                    {
                        "text": "Magpie\n```json\n{\n\"application\": \"Magpie\",\n\"software\": [\n\"Spark\",\n\"Alluxio\",\n\"Hadoop\",\n\"Zookeeper\",\n\"Pig\",\n\"Kafka\",\n\"Zeppelin\",\n\"Phoenix\",\n\"HBase\",\n\"Storm\"\n],\n\"resource_requirements\": {\n\"nodes\": {\n\"description\": \"Node count for the job (including head/master node).  For example, for 8 compute nodes, specify 9 nodes below.\",\n\"variable\": \"PBS -l nodes=<my_node_count>\",\n\"default\": null\n},\n\"partition\": {\n\"description\": \"Partition to launch the job in.\",\n\"variable\": \"PBS -l partition=<my_partition>\",\n\"default\": null\n},\n\"queue\": {\n\"description\": \"Batch queue to launch the job in.\",\n\"variable\": \"PBS -q <my_batch_queue>\",\n\"default\": null\n},\n\"time\": {\n\"description\": \"Time limit for the job (in seconds or HH:MM:SS).  Should be greater than MAGPIE_STARTUP_TIME + MAGPIE_SHUTDOWN_TIME.\",\n\"variable\": \"PBS -l walltime=<my_time_in_seconds_or_HH:MM:SS>\",\n\"default\": null\n},\n\"java\": {\n\"description\": \"Java home directory.\",\n\"variable\": \"JAVA_HOME\",\n\"default\": \"/usr/lib/jvm/jre-1.8.0/\"\n},\n\"python\": {\n\"description\": \"Python path for Spark PySpark and TensorFlow.\",\n\"variable\": \"MAGPIE_PYTHON\",\n\"default\": \"/usr/bin/python\"\n},\n\"scripts_home\": {\n\"description\": \"Directory for launch scripts and files.\",\n\"variable\": \"MAGPIE_SCRIPTS_HOME\",\n\"default\": \"${HOME}/magpie\"\n},\n\"local_dir\": {\n\"description\": \"Path for local configuration files and log files.\",\n\"variable\": \"MAGPIE_LOCAL_DIR\",\n\"default\": \"/tmp/${USER}/magpie\"\n},\n\"job_type\": {\n\"description\": \"Type of job to run.\",\n\"variable\": \"MAGPIE_JOB_TYPE\",\n\"default\": \"spark\"\n},\n\"startup_time\": {\n\"description\": \"Time allowed for startup activities (in minutes).\",\n\"variable\": \"MAGPIE_STARTUP_TIME\",\n\"default\": 30\n},\n\"shutdown_time\": {\n\"description\": \"Time allowed for shutdown activities (in minutes).\",\n\"variable\": \"MAGPIE_SHUTDOWN_TIME\",\n\"default\": 30\n},\n\"one_time_run\": {\n\"description\": \"Flag indicating if job data should be persistent.\",\n\"variable\": \"MAGPIE_ONE_TIME_RUN\",\n\"default\": \"no\"\n},\n\"pre_job_run\": {\n\"description\": \"Script(s) to run before the job.\",\n\"variable\": \"MAGPIE_PRE_JOB_RUN\",\n\"default\": null\n},\n\"post_job_run\": {\n\"description\": \"Script(s) to run after the job.\",\n\"variable\": \"MAGPIE_POST_JOB_RUN\",\n\"default\": null\n},\n\"pre_execute_run\": {\n\"description\": \"Script(s) to run after stack setup, before job execution.\",\n\"variable\": \"MAGPIE_PRE_EXECUTE_RUN\",\n\"default\": null\n},\n\"post_execute_run\": {\n\"description\": \"Script(s) to run after job execution, before stack shutdown.\",\n\"variable\": \"MAGPIE_POST_EXECUTE_RUN\",\n\"default\": null\n},\n\"environment_variable_script\": {\n\"description\": \"Path to script containing environment variables.\",\n\"variable\": \"MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT\",\n\"default\": null\n},\n\"environment_variable_script_shell\": {\n\"description\": \"Shell type for environment variable script output.\",\n\"variable\": \"MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT_SHELL\",\n\"default\": \"/bin/bash\"\n},\n\"remote_cmd\": {\n\"description\": \"Remote shell command to launch daemons.\",\n\"variable\": \"MAGPIE_REMOTE_CMD\",\n\"default\": \"ssh\"\n},\n\"remote_cmd_opts\": {\n\"description\": \"Options for the remote shell command.\",\n\"variable\": \"MAGPIE_REMOTE_CMD_OPTS\",\n\"default\": \"\"\n}\n},\n\"spark_configurations\": {\n\"setup\": {\n\"description\": \"Flag indicating if Spark should be setup.\",\n\"variable\": \"SPARK_SETUP\",\n\"default\": \"yes\"\n},\n\"setup_type\": {\n\"description\": \"Type of Spark setup (STANDALONE or YARN).\",\n\"variable\": \"SPARK_SETUP_TYPE\",\n\"default\": \"STANDALONE\"\n},\n\"version\": {\n\"description\": \"Spark version.\",\n\"variable\": \"SPARK_VERSION\",\n\"default\": \"3.0.3-bin-hadoop3.2\"\n},\n\"home\": {\n\"description\": \"Path to Spark build/binaries.\",\n\"variable\": \"SPARK_HOME\",\n\"default\": \"${HOME}/spark-${SPARK_VERSION}\"\n},\n\"local_dir\": {\n\"description\": \"Path for local Spark configuration files and log files.\",\n\"variable\": \"SPARK_LOCAL_DIR\",\n\"default\": \"/tmp/${USER}/spark\"\n},\n\"conf_files\": {\n\"description\": \"Directory for alternative Spark configuration templates.\",\n\"variable\": \"SPARK_CONF_FILES\",\n\"default\": null\n},\n\"worker_cores_per_node\": {\n\"description\": \"Number of worker cores per node.\",\n\"variable\": \"SPARK_WORKER_CORES_PER_NODE\",\n\"default\": null\n},\n\"worker_memory_per_node\": {\n\"description\": \"Memory per worker node (in MB).\",\n\"variable\": \"SPARK_WORKER_MEMORY_PER_NODE\",\n\"default\": null\n},\n\"worker_directory\": {\n\"description\": \"Directory to run Spark applications.\",\n\"variable\": \"SPARK_WORKER_DIRECTORY\",\n\"default\": \"/tmp/${USER}/spark/work\"\n},\n\"job_memory\": {\n\"description\": \"Memory for Spark jobs (in MB).\",\n\"variable\": \"SPARK_JOB_MEMORY\",\n\"default\": null\n},\n\"driver_memory\": {\n\"description\": \"Memory for Spark driver (in MB).\",\n\"variable\": \"SPARK_DRIVER_MEMORY\",\n\"default\": null\n},\n\"daemon_heap_max\": {\n\"description\": \"Maximum heap size for Spark daemons (in MB).\",\n\"variable\": \"SPARK_DAEMON_HEAP_MAX\",\n\"default\": 1000\n},\n\"environment_extra_path\": {\n\"description\": \"Path to file with additional environment settings for Spark.\",\n\"variable\": \"SPARK_ENVIRONMENT_EXTRA_PATH\",\n\"default\": null\n},\n\"default_parallelism\": {\n\"description\": \"Default number of tasks for distributed shuffle operations.\",\n\"variable\": \"SPARK_DEFAULT_PARALLELISM\",\n\"default\": null\n},\n\"memory_fraction\": {\n\"description\": \"Fraction of heap space for Spark execution and storage.\",\n\"variable\": \"SPARK_MEMORY_FRACTION\",\n\"default\": 0.6\n},\n\"memory_storage_fraction\": {\n\"description\": \"Fraction of storage memory immune to eviction.\",\n\"variable\": \"SPARK_MEMORY_STORAGE_FRACTION\",\n\"default\": 0.5\n},\n\"storage_memory_fraction\": {\n\"description\": \"Fraction of Java heap for Spark's memory cache.\",\n\"variable\": \"SPARK_STORAGE_MEMORY_FRACTION\",\n\"default\": 0.6\n},\n\"shuffle_memory_fraction\": {\n\"description\": \"Fraction of Java heap for aggregation and cogroups during shuffles.\",\n\"variable\": \"SPARK_SHUFFLE_MEMORY_FRACTION\",\n\"default\": 0.2\n},\n\"rdd_compress\": {\n\"description\": \"Flag indicating if RDDs should be compressed by default.\",\n\"variable\": \"SPARK_RDD_COMPRESS\",\n\"default\": \"true\"\n},\n\"io_compression_codec\": {\n\"description\": \"Compression codec for Spark I/O (lz4, snappy, lzf, snappy, or zstd).\",\n\"variable\": \"SPARK_IO_COMPRESSION_CODEC\",\n\"default\": \"lz4\"\n},\n\"deploy_spreadout\": {\n\"description\": \"Flag indicating if Spark applications should be spread out across nodes.\",\n\"variable\": \"SPARK_DEPLOY_SPREADOUT\",\n\"default\": \"true\"\n},\n\"job_classpath\": {\n\"description\": \"Classpath for Spark jobs.\",\n\"variable\": \"SPARK_JOB_CLASSPATH\",\n\"default\": \"\"\n},\n\"job_library_path\": {\n\"description\": \"Library path for Spark jobs.\",\n\"variable\": \"SPARK_JOB_LIBRARY_PATH\",\n\"default\": \"\"\n},\n\"job_java_opts\": {\n\"description\": \"Java options for Spark jobs.\",\n\"variable\": \"SPARK_JOB_JAVA_OPTS\",\n\"default\": \"\"\n},\n\"local_scratch_dir\": {\n\"description\": \"Directory for Spark local scratch data.\",\n\"variable\": \"SPARK_LOCAL_SCRATCH_DIR\",\n\"default\": \"/lustre/${USER}/sparkscratch/\"\n},\n\"local_scratch_dir_clear\": {\n\"description\": \"Flag indicating if local scratch directory should be cleared after job completion.\",\n\"variable\": \"SPARK_LOCAL_SCRATCH_DIR_CLEAR\",\n\"default\": \"yes\"\n},\n\"network_timeout\": {\n\"description\": \"Network timeout configuration for Spark (in seconds).\",\n\"variable\": \"SPARK_NETWORK_TIMEOUT\",\n\"default\": 120\n},\n\"yarn_staging_dir\": {\n\"description\": \"Staging directory for Spark jobs running with Yarn.\",\n\"variable\": \"SPARK_YARN_STAGING_DIR\",\n\"default\": \"file:///lustre/${USER}/sparkStaging/\"\n},\n\"sparkpi_slices\": {\n\"description\": \"Number of slices for SparkPi estimation.\",\n\"variable\": \"SPARK_SPARKPI_SLICES\",\n\"default\": null\n},\n\"sparkwordcount_file\": {\n\"description\": \"File for SparkWordCount job.\",\n\"variable\": \"SPARK_SPARKWORDCOUNT_FILE\",\n\"default\": null\n},\n\"sparkwordcount_copy_in_file\": {\n\"description\": \"File to copy in for SparkWordCount job.\",\n\"variable\": \"SPARK_SPARKWORDCOUNT_COPY_IN_FILE\",\n\"default\": null\n}\n},\n\"alluxio_configurations\": {\n\"setup\": {\n\"description\": \"Flag indicating if Alluxio should be setup.\",\n\"variable\": \"ALLUXIO_SETUP\",\n\"default\": \"yes\"\n},\n\"version\": {\n\"description\": \"Alluxio version.\",\n\"variable\": \"ALLUXIO_VERSION\",\n\"default\": \"2.3.0\"\n},\n\"home\": {\n\"description\": \"Path to Alluxio build/binaries.\",\n\"variable\": \"ALLUXIO_HOME\",\n\"default\": \"${HOME}/alluxio-${ALLUXIO_VERSION}\"\n},\n\"worker_on_master\": {\n\"description\": \"Flag indicating if Alluxio worker should run on master node.\",\n\"variable\": \"ALLUXIO_WORKER_ON_MASTER\",\n\"default\": \"yes\"\n},\n\"local_dir\": {\n\"description\": \"Path for local Alluxio configuration files and log files.\",\n\"variable\": \"ALLUXIO_LOCAL_DIR\",\n\"default\": \"/tmp/${USER}/alluxio\"\n},\n\"under_fs_dir\": {\n\"description\": \"Path to store Alluxio Under File System data.\",\n\"variable\": \"ALLUXIO_UNDER_FS_DIR\",\n\"default\": \"/lustre/${USER}/alluxio\"\n},\n\"data_clear\": {\n\"description\": \"Flag indicating if Alluxio data should be cleared on every run.\",\n\"variable\": \"ALLUXIO_DATA_CLEAR\",\n\"default\": \"yes\"\n},\n\"worker_memory_size\": {\n\"description\": \"Memory capacity of each Alluxio worker node (in GB).\",\n\"variable\": \"ALLUXIO_WORKER_MEMORY_SIZE\",\n\"default\": 16\n},\n\"worker_tier0_path\": {\n\"description\": \"Path for tiered storage level 0 in Alluxio.\",\n\"variable\": \"ALLUXIO_WORKER_TIER0_PATH\",\n\"default\": \"/mnt/ramdisk\"\n},\n\"ramfs_mount_option\": {\n\"description\": \"Mount option for RamFS in Alluxio (Mount, SudoMount, NoMount).\",\n\"variable\": \"ALLUXIO_RAMFS_MOUNT_OPTION\",\n\"default\": \"SudoMount\"\n},\n\"read_type\": {\n\"description\": \"Default read type for Alluxio files (CACHE_PROMOTE, CACHE, NO_CACHE).\",\n\"variable\": \"ALLUXIO_READ_TYPE\",\n\"default\": \"CACHE\"\n},\n\"write_type\": {\n\"description\": \"Default write type for Alluxio files (MUST_CACHE, CACHE_THROUGH, THROUGH, ASYNC_THROUGH).\",\n\"variable\": \"ALLUXIO_WRITE_TYPE\",\n\"default\": \"CACHE_THROUGH\"\n},\n\"testalluxio_job\": {\n\"description\": \"Job for running Alluxio sanity tests.\",\n\"variable\": \"ALLUXIO_JOB\",\n\"default\": \"testalluxio\"\n}\n}\n}\n```"
                    }
                ],
                "role": "model"
            },
            "finish_reason": 1,
            "index": 0,
            "safety_ratings": [
                {
                    "category": 9,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 8,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 7,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 10,
                    "probability": 1,
                    "blocked": false
                }
            ],
            "token_count": 0,
            "grounding_attributions": []
        }
    ],
    "usage_metadata": {
        "prompt_token_count": 8826,
        "candidates_token_count": 3145,
        "total_token_count": 11971,
        "cached_content_token_count": 0
    }
}
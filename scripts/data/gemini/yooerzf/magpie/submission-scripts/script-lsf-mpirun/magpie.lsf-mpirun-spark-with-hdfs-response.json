{
    "candidates": [
        {
            "content": {
                "parts": [
                    {
                        "text": "```json\n{\n\"application\": \"Magpie\",\n\"software\": [\n{\n\"name\": \"Hadoop\",\n\"version\": \"2.7.3\",\n\"type\": \"HDFS\",\n\"setup\": \"yes\",\n\"home\": \"${HOME}/hadoop-${HADOOP_VERSION}\",\n\"local_dir\": \"/tmp/${USER}/hadoop\",\n\"conf_files\": \"${HOME}/myconf\",\n\"daemon_heap_max\": \"2000\",\n\"namenode_daemon_heap_max\": \"2000\",\n\"environment_extra_path\": \"${HOME}/hadoop-my-environment\"\n},\n{\n\"name\": \"Spark\",\n\"version\": \"2.4.3-bin-hadoop2.7\",\n\"type\": \"STANDALONE\",\n\"setup\": \"yes\",\n\"home\": \"${HOME}/spark-${SPARK_VERSION}\",\n\"local_dir\": \"/tmp/${USER}/spark\",\n\"conf_files\": \"${HOME}/myconf\",\n\"worker_cores_per_node\": \"8\",\n\"worker_memory_per_node\": \"16000\",\n\"worker_directory\": \"/tmp/${USER}/spark/work\",\n\"job_memory\": \"2048\",\n\"driver_memory\": \"2048\",\n\"daemon_heap_max\": \"2000\",\n\"environment_extra_path\": \"${HOME}/spark-my-environment\"\n},\n{\n\"name\": \"Python\",\n\"path\": \"/usr/bin/python\"\n},\n{\n\"name\": \"Java\",\n\"home\": \"/usr/lib/jvm/jre-1.8.0/\"\n}\n],\n\"resource\": [\n{\n\"type\": \"node_count\",\n\"description\": \"Node count should include one node for the head/management/master node.  For example, if you want 8 compute nodes to process data, specify 9 nodes below.\",\n\"value\": \"<my_node_count>\"\n},\n{\n\"type\": \"time\",\n\"description\": \"Note defaults of MAGPIE_STARTUP_TIME & MAGPIE_SHUTDOWN_TIME, this timelimit should be a fair amount larger than them combined.\",\n\"value\": \"<my_time_in_hours:minutes>\"\n},\n{\n\"type\": \"job_name\",\n\"description\": \"Job name.  This will be used in naming directories for the job.\",\n\"value\": \"<my_job_name>\"\n},\n{\n\"type\": \"queue\",\n\"description\": \"Queue to launch job in\",\n\"value\": \"<my_queue>\"\n},\n{\n\"type\": \"scripts_home\",\n\"description\": \"Directory your launching scripts/files are stored\",\n\"value\": \"${HOME}/magpie\"\n},\n{\n\"type\": \"local_dir\",\n\"description\": \"Path to store data local to each cluster node, typically something in /tmp.  This will store local conf files and log files for your job.\",\n\"value\": \"/tmp/${USER}/magpie\"\n},\n{\n\"type\": \"startup_time\",\n\"description\": \"Specifies the amount of time to give startup / shutdown activities a chance to succeed before Magpie will give up.\",\n\"value\": \"30\"\n},\n{\n\"type\": \"shutdown_time\",\n\"description\": \"Specifies the amount of time to give startup / shutdown activities a chance to succeed before Magpie will give up.\",\n\"value\": \"30\"\n},\n{\n\"type\": \"hdfs_path\",\n\"description\": \"Path for HDFS when using local disk\",\n\"value\": \"/ssd/${USER}/hdfs\"\n},\n{\n\"type\": \"hdfs_over_lustre_path\",\n\"description\": \"Lustre path to do Hadoop HDFS out of\",\n\"value\": \"/lustre/${USER}/hdfsoverlustre/\"\n},\n{\n\"type\": \"hdfs_over_networkfs_path\",\n\"description\": \"Networkfs path to do Hadoop HDFS out of\",\n\"value\": \"/networkfs/${USER}/hdfsovernetworkfs/\"\n},\n{\n\"type\": \"raw_networkfs_path\",\n\"description\": \"Path for rawnetworkfs\",\n\"value\": \"/lustre/${USER}/rawnetworkfs/\"\n},\n{\n\"type\": \"localstore\",\n\"description\": \"If you have a local SSD or NVRAM, performance may be better to store intermediate data on it rather than Lustre or some other networked filesystem.\",\n\"value\": \"/ssd/${USER}/localstore/\"\n},\n{\n\"type\": \"spark_local_scratch_dir\",\n\"description\": \"By default, if Hadoop is setup with a file system, the Spark local scratch directory, where scratch data is placed, will automatically be calculated and configured.\",\n\"value\": \"/lustre/${USER}/sparkscratch/\"\n},\n{\n\"type\": \"spark_yarn_staging_dir\",\n\"description\": \"By default, Spark w/ Yarn will use your home directory for staging files for a job.\",\n\"value\": \"file:///lustre/${USER}/sparkStaging/\"\n}\n],\n\"requirements\": [\n{\n\"name\": \"passwordless remote shell\",\n\"description\": \"Magpie requires a passwordless remote shell command to launch necessary daemons across your job allocation.\"\n},\n{\n\"name\": \"NFS mount\",\n\"description\": \"MAGPIE_SCRIPTS_HOME should normally be an NFS mount, someplace magpie can be reached on all nodes.\"\n},\n{\n\"name\": \"keys setup\",\n\"description\": \"If using ssh, Magpie requires keys to be setup ahead of time so it can be executed without passwords.\"\n},\n{\n\"name\": \"HADOOP_HDFS_PATH\",\n\"description\": \"User must set HADOOP_HDFS_PATH if HADOOP_FILESYSTEM_MODE is set to \\\"hdfs\\\".\"\n},\n{\n\"name\": \"HADOOP_HDFSOVERLUSTRE_PATH\",\n\"description\": \"User must set HADOOP_HDFSOVERLUSTRE_PATH if HADOOP_FILESYSTEM_MODE is set to \\\"hdfsoverlustre\\\".\"\n},\n{\n\"name\": \"HADOOP_HDFSOVERNETWORKFS_PATH\",\n\"description\": \"User must set HADOOP_HDFSOVERNETWORKFS_PATH if HADOOP_FILESYSTEM_MODE is set to \\\"hdfsovernetworkfs\\\".\"\n},\n{\n\"name\": \"HADOOP_RAWNETWORKFS_PATH\",\n\"description\": \"User must set HADOOP_RAWNETWORKFS_PATH if HADOOP_FILESYSTEM_MODE is set to \\\"rawnetworkfs\\\".\"\n},\n{\n\"name\": \"SPARK_SPARKWORDCOUNT_FILE\",\n\"description\": \"Specify the file to do the word count on.  Specify the scheme, such as hdfs:// or file://, appropriately.\"\n},\n{\n\"name\": \"SPARK_LOCAL_SCRATCH_DIR\",\n\"description\": \"If Hadoop is not setup, SPARK_LOCAL_SCRATCH_DIR must be specified.\"\n}\n]\n}\n```"
                    }
                ],
                "role": "model"
            },
            "finish_reason": 1,
            "index": 0,
            "safety_ratings": [
                {
                    "category": 9,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 8,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 7,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 10,
                    "probability": 1,
                    "blocked": false
                }
            ],
            "citation_metadata": {
                "citation_sources": [
                    {
                        "start_index": 1732,
                        "end_index": 1864,
                        "uri": "https://github.com/LLNL/magpie",
                        "license_": ""
                    },
                    {
                        "start_index": 3050,
                        "end_index": 3211,
                        "uri": "https://github.com/akbog/urban_data",
                        "license_": ""
                    }
                ]
            },
            "token_count": 0,
            "grounding_attributions": []
        }
    ],
    "usage_metadata": {
        "prompt_token_count": 12771,
        "candidates_token_count": 1589,
        "total_token_count": 14360,
        "cached_content_token_count": 0
    }
}
{
    "application": "Machine Learning",
    "details": {
        "software": [
            "Python",
            "PyTorch",
            "SLURM (for job submission)",
            "evaluate_model.py (custom script)"
        ],
        "resources": [
            "GPU (NVIDIA or AMD)",
            "CPU",
            "Memory (30GB)",
            "Storage (for logs and checkpoints)"
        ],
        "hyperparameters": {
            "dataset": "joint",
            "model": "small",
            "img_size": 32,
            "batch_size": 1000,
            "dist_epochs": 100,
            "nr_epochs": 150,
            "num_folds": 5,
            "rho": 0.9,
            "rho_test": [
                0.5,
                0.7,
                0.9
            ],
            "border": 6,
            "label_balance_method": "downsample",
            "weight_model_type": "small",
            "pred_model_type": "small",
            "theta_lr": 0.001,
            "gamma_lr": 0.001,
            "phi_lr": 0.001,
            "nr_lr": 0.001,
            "dist_decay": 0.0,
            "phi_decay": 0.0,
            "nr_decay": 0.01,
            "nr_strategy": "weight",
            "frac_phi_steps": 2,
            "randomrestart": 1,
            "lambda_": 1,
            "max_lambda_": 1
        },
        "execution_modes": [
            "print (output script)",
            "execute (run script)",
            "submit (submit to SLURM)"
        ],
        "other": [
            "This script appears to be conducting a machine learning experiment with different hyperparameter settings and running on a distributed system. It uses the SLURM job scheduler to submit and execute tasks on a cluster. "
        ]
    }
}
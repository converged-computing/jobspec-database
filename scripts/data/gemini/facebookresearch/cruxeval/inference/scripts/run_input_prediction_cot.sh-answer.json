{
    "application": "SLURM",
    "details": {
        "software": [
            "Python",
            "Bash",
            "SLURM",
            "main.py"
        ],
        "resources": {
            "partition": "YOUR_PARTITION_HERE",
            "cpus": 10,
            "gpus": 1,
            "memory": "0GB",
            "time": "03:00:00"
        },
        "model": [
            "codellama/CodeLlama-7b-hf",
            "codellama/CodeLlama-13b-hf",
            "codellama/CodeLlama-34b-hf"
        ],
        "parameters": {
            "temperature": [
                0.2,
                0.8
            ],
            "batch_size": 10,
            "n_samples": 10,
            "max_length_generation": 2048,
            "precision": "bf16",
            "limit": 800,
            "save_generations": true,
            "save_generations_path": "model_generations_raw/${dir}/shard_$(($i)).json",
            "cot": true,
            "shuffle": true
        }
    }
}
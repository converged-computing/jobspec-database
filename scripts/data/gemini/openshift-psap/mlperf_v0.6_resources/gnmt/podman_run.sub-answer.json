{
    "application": "SLURM",
    "details": {
        "partition": "mlperf",
        "nodes": 1,
        "wall_time": "12:00:00",
        "job_name": "rnn_translator",
        "memory": "all available",
        "mail_type": "FAIL",
        "tasks_per_node": 8,
        "threads_per_core": 2,
        "cores_per_socket": 20,
        "benchmark": "rnn_translator",
        "benchmark_name": "GNMT",
        "container": "mlperf-nvidia:rnn_translator",
        "data_dir": "/raid/data/wmt16_de_en",
        "preprocessed_data_dir": "/raid/scratch/gnmt",
        "log_dir": "/raid/results/rnn_translator",
        "number_of_experiments": 10,
        "system_logging": true,
        "preprocess": true,
        "system": "DGX1",
        "docker_volumes": "-v /raid/data/wmt16_de_en:/data -v /raid/results/rnn_translator:/results -v /raid/scratch/gnmt:/preproc_data",
        "docker_exec": "env NV_GPU=${NV_GPU} podman run --rm --net=host --uts=host --ipc=host --ulimit stack=67108864 --ulimit memlock=-1 -e NVIDIA_VISIBLE_DEVICES=ALL --cap-drop=ALL --security-opt label=type:nvidia_container_t --security-opt seccomp=unconfined --name=cont_${SLURM_JOB_ID} $NVIDIA_DEVICES",
        "mlperf_host_os": "/etc/redhat-release",
        "pull": true,
        "preprocess_command": "python3 preprocess_data.py --preproc-data-dir /preproc_data --dataset-dir /data --max-length-train ${MAX_SEQ_LEN}",
        "cache_clear_command": "sync && sudo /sbin/sysctl vm.drop_caches=3",
        "benchmark_command": "./run_and_time.sh",
        "environment_variables": [
            "DGXSYSTEM=$DGXSYSTEM",
            "MULTI_NODE=$MULTI_NODE",
            "LR=$LR",
            "TRAIN_BATCH_SIZE=$TRAIN_BATCH_SIZE",
            "TEST_BATCH_SIZE=$TEST_BATCH_SIZE",
            "WARMUP_STEPS=$WARMUP_STEPS",
            "REMAIN_STEPS=$REMAIN_STEPS",
            "DECAY_INTERVAL=$DECAY_INTERVAL",
            "TARGET=$TARGET",
            "NUMEPOCHS=$NUMEPOCHS",
            "MAX_SEQ_LEN=$MAX_SEQ_LEN",
            "EXTRA_OPTS=$EXTRA_OPTS",
            "SLURM_JOB_ID=$SLURM_JOB_ID",
            "SLURM_NTASKS_PER_NODE=$SLURM_NTASKS_PER_NODE",
            "SLURM_NNODES=$SLURM_NNODES"
        ],
        "cleanup_command": "podman rm -f cont_${SLURM_JOB_ID}"
    }
}
{
    "application": "LSF",
    "details": {
        "queue": "gpuv100",
        "job_name": "LUKE_exp",
        "cores": 4,
        "gpu": "1 (exclusive process)",
        "walltime": "24:00",
        "memory": "32GB",
        "environment": "conda activate luke",
        "python_library": "luke",
        "python_commands": [
            "python luke_hpc/experiment_setup_file.py",
            "python -m examples.cli --model-file=luke_large_500k.tar.gz --output-dir=data/outputs/paper_reconstruction/OpenEntity entity-typing run --data-dir=data/OpenEntity --train-batch-size=2 --gradient-accumulation-steps=2 --learning-rate=1e-5 --num-train-epochs=3 --fp16",
            "python -m examples.cli --model-file=luke_large_500k.tar.gz --output-dir=data/outputs/paper_reconstruction/conll2003 ner run --data-dir=data/CoNLL2003 --fp16 --train-batch-size=4 --gradient-accumulation-steps=2 --learning-rate=1e-5 --num-train-epochs=5",
            "python -m examples.cli --model-file=luke_large_500k.tar.gz --output-dir=data/outputs/paper_reconstruction/tacred relation-classification run --data-dir=data/TACRED --fp16 --train-batch-size=4 --gradient-accumulation-steps=8 --learning-rate=1e-5 --num-train-epochs=5",
            "python -m examples.cli --model-file=luke_large_500k.tar.gz --output-dir=data/outputs/paper_reconstruction/OpenEntity_author_checkpoint_file entity-typing run --data-dir=data/OpenEntity --checkpoint-file=data/check_point_files/OpenEntity_author/pytorch_model.bin --no-train",
            "python -m examples.cli --model-file=luke_large_500k.tar.gz --output-dir=data/outputs/paper_reconstruction/OpenEntity_our_checkpoint_file entity-typing run --data-dir=data/OpenEntity --checkpoint-file=data/check_point_files/OpenEntity_reconstructed/pytorch_model.bin --no-train"
        ]
    }
}
{
    "application": "slurm",
    "details": {
        "scheduler": "slurm",
        "job name": "alpaca",
        "partition": "a800",
        "output log": "log/%j.out",
        "error log": "log/%j.err",
        "total processes": 10,
        "processes per node": 20,
        "cpu cores per process": 3,
        "gpus per node": 8,
        "conda environment": "liguodong-310",
        "cuda version": "11.7.1",
        "pytorch distribution": "torchrun",
        "model": "llama-7b",
        "training data": "alpaca_data_cleaned.json",
        "output directory": "/data/hpc/home/guodong.li/output",
        "training steps": 200,
        "training batch size": 2,
        "evaluation batch size": 1,
        "gradient accumulation steps": 2,
        "evaluation strategy": "no",
        "save strategy": "steps",
        "save steps": 100,
        "save total limit": 1,
        "learning rate": 2e-05,
        "weight decay": 0.0,
        "warmup ratio": 0.03,
        "learning rate scheduler": "cosine",
        "logging steps": 1,
        "logging platform": "tensorboard",
        "gradient checkpointing": "True",
        "mixed precision": "True",
        "deepspeed config": "ds_config_zero2.json"
    }
}
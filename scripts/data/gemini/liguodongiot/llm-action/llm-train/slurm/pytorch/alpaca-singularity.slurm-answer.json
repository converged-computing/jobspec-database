{
    "application": "SLURM",
    "details": {
        "job_name": "alpaca",
        "partition": "a800",
        "output": "log/%j.out",
        "error": "log/%j.err",
        "num_processes": 10,
        "processes_per_node": 20,
        "cpu_cores_per_process": 3,
        "gpus_per_node": 8,
        "singularity_image": "pytorch-alpaca.sif",
        "working_directory": "/workspace/stanford_alpaca/",
        "data_directory": "/data/hpc/home/guodong.li/",
        "model_path": "/workspaces/llama-7b",
        "data_path": "/workspaces/alpaca_data_cleaned.json",
        "output_directory": "/workspaces/output",
        "training_parameters": {
            "bf16": true,
            "max_steps": 100,
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 2,
            "evaluation_strategy": "no",
            "save_strategy": "steps",
            "save_steps": 2000,
            "save_total_limit": 1,
            "learning_rate": 2e-05,
            "weight_decay": 0.0,
            "warmup_ratio": 0.03,
            "lr_scheduler_type": "cosine",
            "logging_steps": 1,
            "report_to": "tensorboard",
            "fsdp": "full_shard auto_wrap",
            "fsdp_transformer_layer_cls_to_wrap": "LlamaDecoderLayer",
            "tf32": true
        }
    }
}
{
    "application": "Slurm",
    "details": {
        "job_name": "megatron-multinode-ib-30b-2",
        "partition": "h800-ib-2",
        "output": "log/%j.out",
        "error": "log/%j.out",
        "nodes": 8,
        "cores_per_node": 80,
        "gpus_per_node": 8,
        "container_runtime": "singularity",
        "container_image": "megatron-deepspeed-v4-gcc.sif",
        "framework": "PyTorch",
        "distributed_backend": "nccl",
        "model_type": "GPT",
        "model_size": "30B",
        "deepspeed_config": "./tmp/deepspeed.json",
        "data_path": [
            "/workspace/data/gpt2-data/my-gpt2_text_document",
            "/workspace/data/gpt2-data/my-gpt2-1_text_document"
        ],
        "tokenizer_model": "/workspace/model/llama-tokenizer/tokenizer.model",
        "data_impl": "mmap",
        "tokenizer_type": "GPTSentencePieceTokenizer",
        "training_iterations": 1500,
        "save_path": "./tmp-llama-65b",
        "load_path": "./tmp-llama-65b",
        "learning_rate": 0.0003,
        "learning_rate_decay_style": "cosine",
        "min_learning_rate": 3e-05,
        "weight_decay": 0.1,
        "gradient_clipping": 1,
        "learning_rate_warmup_iterations": 500,
        "optimizer": "adam",
        "adam_beta1": 0.9,
        "adam_beta2": 0.95,
        "log_interval": 1,
        "save_interval": 2000,
        "evaluation_interval": 1000,
        "evaluation_iterations": 10,
        "fp16": true,
        "query_key_layer_scaling": false,
        "attention_dropout": 0,
        "hidden_dropout": 0,
        "rotary_position_embeddings": true,
        "untie_embeddings_and_output_weights": true,
        "activation_function": "swiglu",
        "normalization": "rmsnorm",
        "bias_linear": false,
        "deepspeed_activation_checkpointing": true,
        "zero_stage": 0
    }
}
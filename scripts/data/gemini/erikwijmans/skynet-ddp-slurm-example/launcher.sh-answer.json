{
    "application": "PyTorch",
    "details": {
        "framework": "PyTorch",
        "distributed training": "DDP (Distributed Data Parallel)",
        "resource manager": "SLURM",
        "nodes": 1,
        "GPUs per task": 1,
        "tasks per node": 2,
        "partition": "short",
        "script": "ddp_example.train_cifar10"
    }
}
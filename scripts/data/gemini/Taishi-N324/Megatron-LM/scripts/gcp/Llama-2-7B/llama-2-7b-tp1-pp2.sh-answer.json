{
    "application": "SLURM",
    "details": {
        "job_name": "JOB_NAME",
        "time": "0:50:00",
        "partition": "a3",
        "nodes": 4,
        "gpus_per_node": 8,
        "ntasks_per_node": 8,
        "output": "outputs/llama-2-7b/%x-%j.out",
        "error": "outputs/llama-2-7b/%x-%j.out",
        "modules": [
            "cuda/12.1",
            "cudnn/8.9.7",
            "hpcx/2.17.1"
        ],
        "open_file_limit": "65536 1048576",
        "virtualenv": ".env/bin/activate",
        "tcp_x_environment_variables": [
            "UDS_PATH",
            "USE_TCPX"
        ],
        "nccl_environment_variables": [
            "NCCL_NET",
            "NCCL_SOCKET_IFNAME",
            "NCCL_GPUDIRECTTCPX_CTRL_DEV",
            "NCCL_GPUDIRECTTCPX_SOCKET_IFNAME",
            "NCCL_CROSS_NIC",
            "NCCL_ALGO",
            "NCCL_PROTO",
            "NCCL_NSOCKS_PERTHREAD",
            "NCCL_SOCKET_NTHREADS",
            "NCCL_MAX_NCHANNELS",
            "NCCL_MIN_NCHANNELS",
            "NCCL_DYNAMIC_CHUNK_SIZE",
            "NCCL_P2P_NET_CHUNKSIZE",
            "NCCL_P2P_PCI_CHUNKSIZE",
            "NCCL_P2P_NVL_CHUNKSIZE",
            "NCCL_BUFFSIZE",
            "CUDA_VISIBLE_DEVICES",
            "NCCL_NET_GDR_LEVEL",
            "NCCL_P2P_PXN_LEVEL",
            "NCCL_GPUDIRECTTCPX_UNIX_CLIENT_PREFIX",
            "NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS",
            "NCCL_GPUDIRECTTCPX_FORCE_ACK",
            "NCCL_GPUDIRECTTCPX_TX_COMPLETION_NANOSLEEP",
            "LD_LIBRARY_PATH"
        ],
        "distributed_settings": [
            "MASTER_ADDR",
            "MASTER_PORT",
            "NUM_GPU_PER_NODE",
            "NODE_TYPE",
            "NUM_NODES",
            "NUM_GPUS",
            "TENSOR_PARALLEL_SIZE",
            "PIPELINE_PARALLEL_SIZE",
            "DATA_PARALLEL_SIZE"
        ],
        "training_config": [
            "MICRO_BATCH_SIZE",
            "GLOBAL_BATCH_SIZE",
            "TRAIN_STEPS",
            "LR",
            "MIN_LR",
            "LR_WARMUP_STEPS",
            "WEIGHT_DECAY",
            "GRAD_CLIP"
        ],
        "model_config": [
            "HIDDEN_SIZE",
            "FFN_HIDDEN_SIZE",
            "NUM_LAYERS",
            "NUM_HEADS",
            "SEQ_LENGTH",
            "TOKENIZER_MODEL",
            "CHECKPOINT_SAVE_DIR"
        ],
        "data_config": [
            "DATASET_DIR",
            "DATA_PATH"
        ],
        "checkpoint_args": "CHECKPOINT_ARGS",
        "python_script": "pretrain_gpt.py",
        "script_arguments": [
            "--tensor-model-parallel-size",
            "--pipeline-model-parallel-size",
            "--sequence-parallel",
            "--use-distributed-optimizer",
            "--num-layers",
            "--hidden-size",
            "--ffn-hidden-size",
            "--num-attention-heads",
            "--seq-length",
            "--max-position-embeddings",
            "--micro-batch-size",
            "--global-batch-size",
            "--train-iters",
            "--tokenizer-type",
            "--tokenizer-model",
            "--save",
            "--data-path",
            "--split",
            "--distributed-backend",
            "--init-method-std",
            "--lr",
            "--min-lr",
            "--lr-decay-style",
            "--weight-decay",
            "--clip-grad",
            "--lr-warmup-iters",
            "--optimizer",
            "--adam-beta1",
            "--adam-beta2",
            "--log-interval",
            "--save-interval",
            "--eval-interval",
            "--eval-iters",
            "--bf16",
            "--untie-embeddings-and-output-weights",
            "--use-rotary-position-embeddings",
            "--disable-bias-linear",
            "--normalization",
            "--norm-epsilon",
            "--no-position-embedding",
            "--no-masked-softmax-fusion",
            "--attention-dropout",
            "--hidden-dropout",
            "--swiglu",
            "--use-flash-attn",
            "--recompute-activations",
            "--recompute-granularity",
            "--use-mpi",
            "--wandb-name",
            "--wandb-project",
            "--wandb-entity"
        ]
    }
}
{
    "application": "PyTorch",
    "details": {
        "framework": "PyTorch",
        "distributed training": "Yes",
        "communication backend": "NCCL",
        "network interface": "EFA (Elastic Fabric Adapter)",
        "scheduler": "SLURM",
        "resource requirements": {
            "nodes": 4,
            "tasks": 16,
            "GPUs": 32,
            "CPUs": 384,
            "memory": "Not specified"
        },
        "other": {
            "NCCL_BUFFSIZE": "2097152",
            "FI_EFA_SET_CUDA_SYNC_MEMOPS": 0,
            "CUDA_LAUNCH_BLOCKING": 0
        },
        "configuration file": "train_configs/llama2_13b.toml"
    }
}
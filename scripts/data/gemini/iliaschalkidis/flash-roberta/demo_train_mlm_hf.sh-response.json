{
    "candidates": [
        {
            "content": {
                "parts": [
                    {
                        "text": "```json\n{\n  \"application\": \"SLURM\",\n  \"details\": {\n    \"job_name\": \"roberta-flash-attention-train\",\n    \"cpus_per_task\": 8,\n    \"memory\": \"8000M\",\n    \"excluded_nodes\": \"hendrixgpu01fl\",\n    \"partition\": \"gpu\",\n    \"gpu_type\": \"a100\",\n    \"gpu_count\": 1,\n    \"output_file\": \"/home/rwg642/flash-roberta/roberta-flash-attention-hf-train.txt\",\n    \"time_limit\": \"6:00:00\",\n    \"modules\": [\n      \"miniconda/4.12.0\"\n    ],\n    \"conda_environment\": \"kiddothe2b\",\n    \"python_script\": \"run_mlm.py\",\n    \"model_class\": \"roberta\",\n    \"model_path\": \"roberta-base\",\n    \"dataset\": \"c4\",\n    \"dataset_config\": \"en\",\n    \"output_dir\": \"data/PLMs/roberta-base-mlm-train\",\n    \"logging_steps\": 1000,\n    \"evaluation_strategy\": \"steps\",\n    \"eval_steps\": 10000,\n    \"max_steps\": 10000,\n    \"per_device_train_batch_size\": 32,\n    \"per_device_eval_batch_size\": 32,\n    \"mlm_probability\": 0.15,\n    \"max_seq_length\": 64,\n    \"max_eval_samples\": 10000,\n    \"save_strategy\": \"steps\",\n    \"save_steps\": 10000,\n    \"save_total_limit\": 5,\n    \"streaming\": true,\n    \"fp16\": true,\n    \"fp16_full_eval\": true\n  }\n}\n```"
                    }
                ],
                "role": "model"
            },
            "finish_reason": 1,
            "index": 0,
            "safety_ratings": [
                {
                    "category": 9,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 8,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 7,
                    "probability": 1,
                    "blocked": false
                },
                {
                    "category": 10,
                    "probability": 1,
                    "blocked": false
                }
            ],
            "citation_metadata": {
                "citation_sources": [
                    {
                        "start_index": 648,
                        "end_index": 1028,
                        "uri": "https://github.com/iliaschalkidis/flash-roberta",
                        "license_": ""
                    }
                ]
            },
            "token_count": 0,
            "grounding_attributions": []
        }
    ],
    "usage_metadata": {
        "prompt_token_count": 581,
        "candidates_token_count": 474,
        "total_token_count": 1055,
        "cached_content_token_count": 0
    }
}
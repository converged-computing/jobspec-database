{
    "application": "SLURM",
    "details": {
        "job_name": "roberta-flash-attention-train",
        "cpus_per_task": 8,
        "memory": "8000M",
        "excluded_nodes": "hendrixgpu01fl",
        "partition": "gpu",
        "gpu_type": "a100",
        "gpu_count": 1,
        "output_file": "/home/rwg642/flash-roberta/roberta-flash-attention-hf-train.txt",
        "time_limit": "6:00:00",
        "modules": [
            "miniconda/4.12.0"
        ],
        "conda_environment": "kiddothe2b",
        "python_script": "run_mlm.py",
        "model_class": "roberta",
        "model_path": "roberta-base",
        "dataset": "c4",
        "dataset_config": "en",
        "output_dir": "data/PLMs/roberta-base-mlm-train",
        "logging_steps": 1000,
        "evaluation_strategy": "steps",
        "eval_steps": 10000,
        "max_steps": 10000,
        "per_device_train_batch_size": 32,
        "per_device_eval_batch_size": 32,
        "mlm_probability": 0.15,
        "max_seq_length": 64,
        "max_eval_samples": 10000,
        "save_strategy": "steps",
        "save_steps": 10000,
        "save_total_limit": 5,
        "streaming": true,
        "fp16": true,
        "fp16_full_eval": true
    }
}
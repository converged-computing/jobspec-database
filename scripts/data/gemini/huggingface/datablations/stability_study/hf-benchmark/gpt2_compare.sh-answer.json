{
    "application": "SLURM",
    "details": {
        "job_name": "gpt2_compare",
        "nodes": 1,
        "tasks_per_node": 1,
        "cpus_per_task": 40,
        "memory": "256G",
        "partition": "pilot",
        "time": "12:00:00",
        "gpus": "mi250:8",
        "exclusive": "user",
        "hint": "nomultithread",
        "account": "project_462000119",
        "output": "logs/%x-%j.out",
        "error": "logs/%x-%j.err",
        "environment": {
            "TORCH_EXTENSIONS_DIR": "/tmp/$USER/torch_extensions/",
            "NCCL_SOCKET_IFNAME": "hsn0,hsn1,hsn2,hsn3",
            "TRANSFORMERS_CACHE": "/scratch/project_462000119/tf_cache",
            "HF_DATASETS_CACHE": "/scratch/project_462000119/tf_cache/datasets",
            "HF_MODULES_CACHE": "/scratch/project_462000119/tf_cache/modules",
            "HF_METRICS_CACHE": "/scratch/project_462000119/tf_cache/metrics"
        },
        "command": "python transformers/scripts/benchmark/trainer-benchmark.py --base-cmd ' transformers/examples/pytorch/language-modeling/run_clm.py --model_type gpt2 --tokenizer_name gpt2 --dataset_name stas/openwebtext-10k --logging_strategy steps --logging_steps 1 --log_level debug --report_to tensorboard --save_strategy no --do_train --max_train_samples 1000000 --per_device_train_batch_size 8 --num_train_epochs 10 --warmup_steps 8 --block_size 512 --fp16 ' --target-metric-key train_samples_per_second --repeat-times 1 --variations '--optim adamw_torch|--optim adamw_apex_fused' --report-metric-keys train_loss --base-variation '--optim adamw_torch' --verbose --output_dir hf_trainer_benchmark/gpt2_compare"
    }
}
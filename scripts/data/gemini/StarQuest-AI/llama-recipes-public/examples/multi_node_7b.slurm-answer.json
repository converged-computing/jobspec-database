{
    "application": "SLURM",
    "details": {
        "resource_requirements": {
            "nodes": 2,
            "tasks": 2,
            "gpus_per_task": 4,
            "partition": "debug"
        },
        "software_requirements": {
            "python_packages": [
                "llama-recipes",
                "torch",
                "torchvision",
                "torchaudio",
                "peft"
            ],
            "environment_variables": [
                "FI_PROVIDER",
                "LOGLEVEL",
                "NCCL_DEBUG",
                "NCCL_DEBUG_SUBSYS",
                "PYTHONFAULTHANDLER",
                "LD_LIBRARY_PATH",
                "CUDA_LAUNCH_BLOCKING",
                "NCCL_SOCKET_IFNAME",
                "FI_EFA_USE_DEVICE_RDMA"
            ],
            "other_requirements": {
                "model_path": "/data0/models/huggingface/meta-llama/Llama-2-7b-hf",
                "output_dir": "/data0/model_output/Llama-2-7b-hf-sft/",
                "checkpoint_root_folder": "/data0/model_output/checkpoint/Llama-2-7b-hf-sft"
            }
        },
        "script_purpose": "Fine-tuning a Llama-2 7B model using PEFT (LoRA)"
    }
}
{
    "application": "Slurm",
    "details": {
        "slurm_account": "example",
        "slurm_partition": "partition",
        "nodes": 1,
        "wall_time": "00:20:00",
        "job_name": "maxtext:test",
        "exclusive_node_access": true,
        "memory": "all available",
        "tasks_per_node": 8,
        "overcommit": true,
        "dependency": "singleton",
        "container_image": "ghcr.io/nvidia/jax:maxtext-2024-04-15",
        "base_workspace_dir": "${BASE_WORKSPACE_DIR}",
        "base_tfds_data_dir": "${BASE_TFDS_DATA_DIR}",
        "base_vocab_path": "${BASE_VOCAB_PATH}",
        "base_checkpoint_dir": "${BASE_CHECKPOINT_DIR}",
        "maxtext_dir": "/opt/maxtext",
        "workspace_dir": "/opt/maxtext/workspace",
        "tfds_data_dir": "/mnt/datasets",
        "gpt_vocab_path": "/mnt/vocab",
        "checkpoint_dir": "/opt/maxtext/workspace/llama-checkpoint",
        "xla_python_client_mem_fraction": 0.9,
        "cuda_device_max_connections": 1,
        "nvte_fused_attn": 1,
        "nccl_ib_sl": 1,
        "xla_flags": "--xla_gpu_enable_latency_hiding_scheduler=true\n                --xla_gpu_enable_async_all_gather=true\n                --xla_gpu_enable_async_reduce_scatter=true\n                --xla_gpu_enable_triton_gemm=false\n                --xla_gpu_simplify_all_fp_conversions\n                --xla_gpu_graph_level=0\n                --xla_gpu_enable_async_all_reduce=true\n                --xla_gpu_enable_highest_priority_async_stream=true\n                --xla_gpu_all_reduce_combine_threshold_bytes=1073741824\n                --xla_gpu_all_gather_combine_threshold_bytes=1073741824\n                --xla_gpu_reduce_scatter_combine_threshold_bytes=134217728\n                --xla_gpu_enable_pipelined_all_gather=true\n                --xla_gpu_enable_pipelined_reduce_scatter=true\n                --xla_gpu_enable_pipelined_all_reduce=true\n                --xla_gpu_enable_while_loop_double_buffering=true\n                --xla_gpu_enable_triton_softmax_fusion=false\n                --xla_gpu_enable_all_gather_combine_by_dim=false\n                --xla_gpu_enable_reduce_scatter_combine_by_dim=false\n                --xla_disable_hlo_passes=rematerialization\n                --xla_gpu_enable_custom_fusions=false\n                --xla_gpu_enable_address_computation_fusion=false",
        "output_dir": "outputs",
        "model": "llama2-7b",
        "run_name": "demo",
        "python_script": "MaxText/train.py",
        "config_file": "MaxText/configs/base.yml",
        "per_device_batch_size": 2,
        "steps": 15,
        "scan_layers": false,
        "remat_policy": "minimal_flash",
        "attention": "cudnn_flash_te",
        "max_target_length": 4096,
        "use_iota_embed": true,
        "logits_dot_in_fp32": false,
        "enable_checkpointing": false,
        "base_output_directory": "local_train",
        "dataset_path": "local",
        "dataset_type": "synthetic",
        "hardware": "gpu_multiprocess"
    }
}
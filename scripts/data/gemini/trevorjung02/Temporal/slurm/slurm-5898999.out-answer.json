{
    "application": "SLURM",
    "details": {
        "job_name": "data-proc",
        "partition": "gpu-a40",
        "account": "ark",
        "nodes": 1,
        "cpus_per_task": 8,
        "memory": "32G",
        "gpu": 1,
        "time": "04:59:00",
        "mail_type": "ALL",
        "mail_user": "tjung2@uw.edu",
        "environment": "conda",
        "conda_environment": "ckl",
        "python_version": "3.8",
        "framework": "pytorch_lightning",
        "model": "T5ForConditionalGeneration",
        "model_size": "81.7 M",
        "trainable_parameters": "4.7 M",
        "non_trainable_parameters": "77.0 M",
        "total_parameters": "81.7 M",
        "estimated_model_size": "326.730 MB",
        "config_file": "configs/templama/training/t5_kadapters_2010_prefixed.json",
        "dataset": "templama",
        "dataset_version": "2010",
        "method": "kadapter",
        "mode": "pretrain",
        "model_name_or_path": "google/t5-small-ssm",
        "tokenizer_name_or_path": "google/t5-small-ssm",
        "output_dir": "outputs/T5_small_templama(2010)_lr.001_adapters_prefixed",
        "accelerator": "ddp",
        "backend": "nccl",
        "num_workers": 4,
        "optimization_level": "O1",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "learning_rate": 0.1,
        "max_grad_norm": 0.5,
        "max_input_length": 50,
        "max_output_length": 25,
        "num_train_epochs": 30,
        "seed": 42,
        "use_lr_scheduling": true,
        "warmup_steps": 0,
        "weight_decay": 0.0
    }
}
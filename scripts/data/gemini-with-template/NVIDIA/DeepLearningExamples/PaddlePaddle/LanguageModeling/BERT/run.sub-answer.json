{
    "application": "bert",
    "software": [
        "paddle",
        "python",
        "preprocess_bert_pretrain",
        "balance_dask_output",
        "bert-large-uncased",
        "run_pretraining.py",
        "lamb"
    ],
    "modules": [],
    "environment_variables": {
        "IMAGE_VERSION": "22.12-py3",
        "DASK_TASKS_PER_NODE": "128",
        "PHASE": "1",
        "SEED": "42",
        "SAMPLE_RATIO": "0.9",
        "GPUS": "8",
        "BIN_SIZE": "none",
        "NUM_SHARDS_PER_WORKER": "none",
        "NUM_WORKERS": "4",
        "RERUN_DASK": "true",
        "MASKING": "static",
        "USE_JEMALLOC": "true",
        "PRECISION": "fp16",
        "INIT_CHECKPOINT": "none",
        "TRAIN_BATCH_SIZE": "256",
        "GRADIENT_ACCUMULATION_STEPS": "32",
        "docker_image": "bert:22.12-py3",
        "host_datadir": "/home/${USER}/datasets",
        "container_datadir": "/datasets",
        "host_wikipedia_source": "${host_datadir}/wikipedia/source",
        "container_wikipedia_source": "${container_datadir}/wikipedia/source",
        "wikipedia_mount": "${host_wikipedia_source}:${container_wikipedia_source}",
        "host_pretrain": "${host_datadir}/pretrain",
        "container_pretrain": "${container_datadir}/pretrain",
        "pretrain_mount": "${host_pretrain}:${container_pretrain}",
        "host_output": "$PWD/results/${SLURM_JOB_ID}",
        "container_output": "/results",
        "output_mount": "${host_output}:${container_output}",
        "mounts": "${PWD}:/workspace/bert,${wikipedia_mount},${pretrain_mount},${output_mount}",
        "container_init_checkpoint": "",
        "host_pretrain_parquet": "${host_output}/parquet",
        "container_pretrain_parquet": "${container_output}/parquet",
        "num_blocks": "4096",
        "target_seq_len_flag": "",
        "bin_size_flag": "",
        "masking_flag": "--masking",
        "use_jemalloc_flag": "--export=ALL,LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so",
        "IP_CMD": "hostname -i",
        "IP_STR": "",
        "PHASES": [
            "    --learning-rate=6e-3 \n    --warmup-proportion=0.2843 \n    --phase1 \n    --max-seq-length=128 \n    --max-predictions-per-seq=20 \n    --max-steps=7038 \n    --num-steps-per-checkpoint=2500 \n    ",
            "    --learning-rate=4e-3 \n    --warmup-proportion=0.128 \n    --phase2 \n    --max-seq-length=512 \n    --max-predictions-per-seq=80 \n    --max-steps=1563 \n    --num-steps-per-checkpoint=1000 \n    --from-pretrained-params=${container_init_checkpoint} \n    "
        ],
        "BERT_CMD": "    python -m paddle.distributed.launch \n    --gpus=0,1,2,3,4,5,6,7 \n    --ips=\"${IP_STR}\" \n    /workspace/bert/run_pretraining.py \n    ${PHASES[$((PHASE - 1))]} \n    --batch-size=${TRAIN_BATCH_SIZE} \n    --input-dir=${container_pretrain_parquet} \n    --output-dir=${container_output} \n    --vocab-file=/workspace/bert/vocab/bert-large-uncased-vocab.txt \n    --bert-model=bert-large-uncased \n    --config-file=/workspace/bert/bert_configs/bert-large-uncased.json \n    --gradient-merge-steps=${GRADIENT_ACCUMULATION_STEPS} \n    --log-freq=1 \n    --seed=12345 \n    --optimizer=Lamb \n    ${fp16_flags} ",
        "fp16_flags": "--amp --use-dynamic-loss-scaling --scale-loss=1048576"
    },
    "resources": {
        "gres": "",
        "cpus_per_task": "",
        "tasks": "",
        "ntasks_per_code": "",
        "gpus": "8",
        "gpus_per_node": "8",
        "cores_per_socket": "",
        "gpus_per_task": "",
        "exclusive": "true",
        "cpus_per_gpu": "",
        "gpu_type": "",
        "time": "",
        "ntasks_per_node": "1",
        "nodes": "${SLURM_JOB_NUM_NODES}",
        "memory": "0",
        "sockets_per_node": "",
        "ntasks_per_socket": "",
        "mem_per_gpu": "",
        "mem_per_cpu": "",
        "gres_flags": ""
    },
    "versions": {}
}
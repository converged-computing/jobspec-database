{
    "application": "pytorch",
    "software": [
        "python",
        "torch",
        "torch.distributed.run",
        "conda",
        "megron-2",
        "srun",
        "bash",
        "slurm"
    ],
    "modules": [
        "megron-2"
    ],
    "environment_variables": {
        "CUDA_HOME": "$CONDA_PREFIX",
        "LIBRARY_PATH": "$CONDA_PREFIX/lib:$LIBRARY_PATH",
        "LD_LIBRARY_PATH": "$CONDA_PREFIX/lib:$LD_LIBRARY_PATH",
        "MASTER_ADDR": "$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)",
        "MASTER_PORT": "6000",
        "NNODES": "$SLURM_NNODES",
        "NODE_RANK": "$SLURM_PROCID",
        "WORLD_SIZE": "$(($GPUS_PER_NODE*$NNODES))",
        "GPUS_PER_NODE": "4",
        "CHECKPOINT_PATH": "/leonardo_scratch/large/userexternal/lbenalla/trainings/ckpts",
        "TOKENIZER_FILE": "/leonardo/home/userexternal/lbenalla/tokenizer-starcoder/tokenizer.json",
        "WEIGHTS_TRAIN": "/leonardo/home/userexternal/lbenalla/code/bigcode-data-mix/data/train_data_paths.txt.tmp",
        "WEIGHTS_VALID": "/leonardo/home/userexternal/lbenalla/code/bigcode-data-mix/data/valid_data_paths.txt.tmp",
        "GPT_ARGS": "\\       --tensor-model-parallel-size 4 \\       --pipeline-model-parallel-size 4 \\       --sequence-parallel \\       --num-layers 40 \\       --hidden-size 6144 \\       --num-attention-heads 48 \\       --attention-head-type multiquery \\       --init-method-std 0.01275 \\       --seq-length 4096 \\       --max-position-embeddings 4096 \\       --attention-dropout 0.1 \\       --hidden-dropout 0.1 \\       --micro-batch-size 1 \\       --global-batch-size 512 \\       --lr 0.0003 \\       --min-lr 0.00003 \\       --train-iters 250000 \\       --lr-decay-iters 250000 \\       --lr-decay-style cosine \\       --lr-warmup-iters 2000 \\       --weight-decay .1 \\       --adam-beta2 .95 \\       --clip-grad 1.0 \\       --fp16 \\       --use-flash-attn \\       --fim-rate 0.5 \\       --log-interval 10 \\       --save-interval 2500 \\       --eval-interval 2500 \\       --eval-iters 2 \\       --use-distributed-optimizer \\       --valid-num-workers 0 \\       ",
        "TENSORBOARD_ARGS": "--tensorboard-dir ${CHECKPOINT_PATH}/tensorboard",
        "CMD": "     /leonardo/home/userexternal/lbenalla/code/Megatron-LM/pretrain_gpt.py     $GPT_ARGS     --tokenizer-type TokenizerFromFile     --tokenizer-file $TOKENIZER_FILE     --save $CHECKPOINT_PATH     --load $CHECKPOINT_PATH     --train-weighted-split-paths-path $WEIGHTS_TRAIN     --valid-weighted-split-paths-path $WEIGHTS_VALID     --structured-logs     --structured-logs-dir $CHECKPOINT_PATH/logs     $TENSORBOARD_ARGS     --wandb-entity-name loubnabnl     --wandb-project-name leonardo-training     ",
        "LAUNCHER": "python -u -m torch.distributed.run     --nproc_per_node $GPUS_PER_NODE     --nnodes $NNODES     --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT     --rdzv_backend c10d     --max_restarts 0     --tee 3     ",
        "SRUN_ARGS": "     --wait=60     --kill-on-bad-exit=1     ",
        "SCRIPT_REPO": "/leonardo/home/userexternal/lbenalla/code/Megatron-LM",
        "LOG_PATH": "$SCRIPT_REPO/main_log.txt"
    },
    "resources": {
        "gres": "gpu:4",
        "cpus_per_task": "32",
        "tasks": null,
        "ntasks_per_code": null,
        "gpus": null,
        "gpus_per_node": "4",
        "cores_per_socket": null,
        "gpus_per_task": null,
        "exclusive": "true",
        "cpus_per_gpu": null,
        "gpu_type": null,
        "time": null,
        "ntasks_per_node": "1",
        "nodes": "32",
        "memory": null,
        "sockets_per_node": null,
        "ntasks_per_socket": null,
        "mem_per_gpu": null,
        "mem_per_cpu": null,
        "gres_flags": ""
    },
    "versions": {}
}
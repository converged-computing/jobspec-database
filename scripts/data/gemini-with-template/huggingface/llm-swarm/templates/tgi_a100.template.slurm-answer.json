{
    "application": "docker",
    "software": [
        "text-generation-inference"
    ],
    "modules": [],
    "environment_variables": {
        "model": "{{model}}",
        "revision": "{{revision}}",
        "volume": "/fsx/.cache",
        "PORT": "result of unused_port function",
        "HUGGING_FACE_HUB_TOKEN": "value from environment or ~/.cache/huggingface/token",
        "HF_TOKEN": "value from environment"
    },
    "resources": {
        "gres": "gpu:8",
        "cpus_per_task": "12",
        "tasks": "1",
        "ntasks_per_code": "1",
        "gpus": "8",
        "gpus_per_node": "8",
        "cores_per_socket": null,
        "gpus_per_task": "8",
        "exclusive": null,
        "cpus_per_gpu": "1.5",
        "gpu_type": null,
        "time": null,
        "ntasks_per_node": "1",
        "nodes": "1",
        "memory": "132G",
        "sockets_per_node": null,
        "ntasks_per_socket": null,
        "mem_per_gpu": "16.5G",
        "mem_per_cpu": "11G",
        "gres_flags": null
    },
    "versions": {}
}
{
    "application": "llama",
    "software": [
        "python3",
        "herd"
    ],
    "modules": [
        "cuda/11.6",
        "python3/3.11.4"
    ],
    "environment_variables": {
        "HF_DATASETS_CACHE": "/work3/s212722/herd/cache",
        "WANDB_API_KEY": "[redacted]",
        "WANDB_PROJECT": "herd-llama",
        "WANDB_LOG_MODEL": "true",
        "WANDB_WATCH": "false"
    },
    "resources": {
        "gres": "gpu:num=1::mode=exclusive_process",
        "cpus_per_task": "4",
        "tasks": "1",
        "ntasks_per_code": "1",
        "gpus": "1",
        "gpus_per_node": "1",
        "cores_per_socket": null,
        "gpus_per_task": "1",
        "exclusive": "true",
        "cpus_per_gpu": "4",
        "gpu_type": "a100",
        "time": "23:00",
        "ntasks_per_node": "1",
        "nodes": "1",
        "memory": "8GB",
        "sockets_per_node": null,
        "ntasks_per_socket": null,
        "mem_per_gpu": null,
        "mem_per_cpu": "8GB",
        "gres_flags": ""
    },
    "versions": {
        "cuda": "11.6",
        "python3": "3.11.4"
    }
}
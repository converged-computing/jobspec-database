{
    "application": "pytorch",
    "software": [
        "huggingface-cli",
        "torchrun",
        "srun"
    ],
    "modules": [],
    "environment_variables": {
        "HUGGINGFACE_TOKEN": "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",
        "MASTER_ADDR": "head_node_ip",
        "MASTER_PORT": "29500",
        "MODEL_NAME": "meta-llama/Llama-2-7b-chat-hf",
        "MODEL_NAME_NEW": "meta-llama/Llama-2-7b-chat-hf_new",
        "MODEL_OUTPUT_DIR": "/bucket/model/meta-llama/Llama-2-7b-chat-hf_new",
        "MODEL_CHECKPOINT_ROOT_DIR": "/bucket/checkpoints",
        "MODEL_CHECKPOINT_FOLDER": "meta-llama/Llama-2-7b-chat-hf_new",
        "PYTHONFAULTHANDLER": "1",
        "CUDA_LAUNCH_BLOCKING": "0",
        "LOGLEVEL": "INFO",
        "NCCL_DEBUG": "INFO",
        "NCCL_DEBUG_SUBSYS": "INIT,P2P",
        "NCCL_ASYNC_ERROR_HANDLING": "1",
        "TORCH_CPP_LOG_LEVEL": "INFO",
        "TORCH_DISTRIBUTED_DEBUG": "DETAIL"
    },
    "resources": {
        "gres": "gpu:8",
        "cpus_per_task": null,
        "tasks": null,
        "ntasks_per_code": null,
        "gpus": "8",
        "gpus_per_node": "8",
        "cores_per_socket": null,
        "gpus_per_task": null,
        "exclusive": null,
        "cpus_per_gpu": null,
        "gpu_type": null,
        "time": "01:00:00",
        "ntasks_per_node": null,
        "nodes": "1",
        "memory": null,
        "sockets_per_node": null,
        "ntasks_per_socket": null,
        "mem_per_gpu": null,
        "mem_per_cpu": null,
        "gres_flags": null
    },
    "versions": {}
}
{
    "application": "deepspeed",
    "software": [
        "deepspeed",
        "pytorch",
        "torch.distributed",
        "python",
        "bash"
    ],
    "modules": [],
    "environment_variables": {
        "HF_DATASETS_OFFLINE": "1",
        "TRANSFORMERS_OFFLINE": "1",
        "DATA_OUTPUT_PATH": "$six_ALL_CCFRSCRATCH/checkpoints/tr10-13B",
        "CHECKPOINT_PATH": "$DATA_OUTPUT_PATH/checkpoints",
        "REPO_PATH": "$DATA_OUTPUT_PATH/tr10-13B-logs",
        "TENSORBOARD_PATH": "$REPO_PATH/tensorboard",
        "LOGS_PATH": "$REPO_PATH/logs",
        "MEGATRON_DEEPSPEED_REPO": "$six_ALL_CCFRWORK/code/tr10-13B/Megatron-DeepSpeed",
        "TOKENIZER_NAME": "teven/test_150k_vocab_tokenizer",
        "DATA_PATH": "$six_ALL_CCFRSCRATCH/datasets-custom/150k_vocab_size_test/c4_10k_samples_150k_vocab_size",
        "MASTER_ADDR": "$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)",
        "MASTER_PORT": "6000",
        "GPUS_PER_NODE": "4",
        "NNODES": "4",
        "TP_SIZE": "2",
        "PP_SIZE": "4",
        "MICRO_BATCH_SIZE": "1",
        "GLOBAL_BATCH_SIZE": "2048",
        "NLAYERS": "40",
        "NHIDDEN": "5120",
        "NHEADS": "32",
        "SEQ_LEN": "2048",
        "VOCAB_SIZE": "150000",
        "SAVE_INTERVAL": "300",
        "OPTIMIZER_ARGS": "    --optimizer adam     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-8     --lr 6e-5     --min-lr 6e-6     --lr-decay-style cosine     --lr-decay-samples 126_953_125     --lr-warmup-samples 216_320     --clip-grad 1.0     --weight-decay 1e-1     ",
        "EXIT_OPTS": "    --exit-duration-in-mins 1190     ",
        "GPT_ARGS": "    --num-layers $NLAYERS     --hidden-size $NHIDDEN     --num-attention-heads $NHEADS     --seq-length $SEQ_LEN     --max-position-embeddings $SEQ_LEN     --micro-batch-size $MICRO_BATCH_SIZE     --rampup-batch-size 16 16 6_000_000     --global-batch-size $GLOBAL_BATCH_SIZE     --train-samples 300_000_000     --tokenizer-type PretrainedFromHF     --tokenizer-name-or-path $TOKENIZER_NAME     --loss-scale 12     --init-method-std 0.00884     --fp16     --checkpoint-activations     --seed 43     $OPTIMIZER_ARGS     $EXIT_OPTS     ",
        "OUTPUT_ARGS": "    --log-interval 1     --save-interval $SAVE_INTERVAL     --eval-interval 1000     --eval-iters 5     --tensorboard-dir $TENSORBOARD_PATH     --tensorboard-queue-size 5     --log-timers-to-tensorboard     --log-batch-size-to-tensorboard     --log-validation-ppl-to-tensorboard     --log-level info     --log-level-replica error     ",
        "ZERO_STAGE": "1",
        "config_json": "./ds_config.$SLURM_JOBID.json",
        "DEEPSPEED_ARGS": "    --deepspeed     --deepspeed_config ${config_json}     --zero-stage ${ZERO_STAGE}     --deepspeed-activation-checkpointing     ",
        "LAUNCHER": "python -u -m torch.distributed.run     --nproc_per_node $GPUS_PER_NODE     --nnodes $NNODES     --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT     --rdzv_backend c10d     --max_restarts 0     ",
        "CMD": "     `pwd`/pretrain_gpt.py     --tensor-model-parallel-size $TP_SIZE     --pipeline-model-parallel-size $PP_SIZE     $GPT_ARGS     $OUTPUT_ARGS     --save $CHECKPOINT_PATH     --load $CHECKPOINT_PATH     --data-path $DATA_PATH     --data-impl mmap     --split 900,100,0     --distributed-backend nccl      $DEEPSPEED_ARGS     ",
        "OMP_NUM_THREADS": "1"
    },
    "resources": {
        "gres": "gpu:4",
        "cpus_per_task": "40",
        "tasks": null,
        "ntasks_per_code": null,
        "gpus": "4",
        "gpus_per_node": "4",
        "cores_per_socket": null,
        "gpus_per_task": null,
        "exclusive": null,
        "cpus_per_gpu": null,
        "gpu_type": "v100",
        "time": "20:00:00",
        "ntasks_per_node": "1",
        "nodes": "4",
        "memory": null,
        "sockets_per_node": null,
        "ntasks_per_socket": null,
        "mem_per_gpu": "32g",
        "mem_per_cpu": null,
        "gres_flags": ""
    },
    "versions": {}
}
{
    "application": "slurm",
    "software": [
        "pytorch",
        "conda",
        "run_pretraining.py"
    ],
    "modules": [
        "conda",
        "pytorch"
    ],
    "environment_variables": {
        "OMP_NUM_THREADS": "8",
        "PHASE": "1",
        "CONFIG": "config/bert_pretraining_phase1_config.json",
        "DATA": "/lus/theta-fs0/projects/SuperBERT/jgpaul/datasets/encoded/wikibooks/static_masked_30K/sequences_lowercase_max_seq_len_128_next_seq_task_true/",
        "OUTPUT_DIR": "results/bert_large_uncased_wikibooks_pretraining"
    },
    "resources": {
        "gres": null,
        "cpus_per_task": null,
        "tasks": "32",
        "ntasks_per_code": null,
        "gpus": null,
        "gpus_per_node": null,
        "cores_per_socket": null,
        "gpus_per_task": null,
        "exclusive": null,
        "cpus_per_gpu": null,
        "gpu_type": "v100",
        "time": "48:00:00",
        "ntasks_per_node": null,
        "nodes": "16",
        "memory": null,
        "sockets_per_node": null,
        "ntasks_per_socket": null,
        "mem_per_gpu": null,
        "mem_per_cpu": null,
        "gres_flags": null
    },
    "versions": {}
}
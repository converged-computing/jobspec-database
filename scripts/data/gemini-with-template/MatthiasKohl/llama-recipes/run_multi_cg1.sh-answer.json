{
    "application": "llama",
    "software": [
        "python",
        "pytorch",
        "enroot",
        "nvidia",
        "fsdp",
        "peft"
    ],
    "modules": [],
    "environment_variables": {
        "MASTER_PORT": "10000 + last 4 digits of SLURM_JOBID",
        "WORLD_SIZE": "SLURM_NTASKS_PER_NODE * SLURM_JOB_NUM_NODES",
        "MASTER_ADDR": "first hostname in scontrol show hostnames \"$SLURM_JOB_NODELIST\"",
        "PMIX_MCA_psec": "native",
        "MELLANOX_VISIBLE_DEVICES": "all",
        "LOCAL_RANK": "$SLURM_LOCALID",
        "RANK": "$SLURM_PROCID",
        "OMP_NUM_THREADS": "4",
        "HF_HOME": "$SCRIPT_DIR/hf_home",
        "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:1024"
    },
    "resources": {
        "gres": null,
        "cpus_per_task": "8",
        "tasks": null,
        "ntasks_per_code": null,
        "gpus": null,
        "gpus_per_node": null,
        "cores_per_socket": null,
        "gpus_per_task": null,
        "exclusive": null,
        "cpus_per_gpu": null,
        "gpu_type": null,
        "time": "02:00:00",
        "ntasks_per_node": "1",
        "nodes": "2",
        "memory": "475G",
        "sockets_per_node": null,
        "ntasks_per_socket": null,
        "mem_per_gpu": null,
        "mem_per_cpu": null,
        "gres_flags": null
    },
    "versions": {
        "pytorch": "24.01"
    }
}
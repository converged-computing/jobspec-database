{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 10\n#SBATCH -J pre_all_scrach\n#SBATCH -o pre_cscd_all_128_from_bert.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\ninput_files=/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_O_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_S_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TF_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TK_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TP_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TV_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_P_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TB_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TG_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TL_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TQ_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_U_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_Q_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TD_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TH_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TM_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TS_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_V_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_R_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TE_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TJ_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TN_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_TU_cscd_128.tfrecord,/public/home/zzx6320/lh/Projects/Data/cscd_all/pre_training_X_cscd_128.tfrecord\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file ${input_files}\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --init_checkpoint /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt \\\n --output_dir output/Pre3_cscd_all_128_64_from_bert\\\n --max_seq_length 128\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 64\\\n --learning_rate 2e-5\\\n --num_train_steps 1000000\\\n --save_checkpoints_steps 1000\\"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J pre_14\n#SBATCH -o Sbatch14.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\ninput_files=/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_O_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_S_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TF_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TK_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TP_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TV_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_P_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TB_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TG_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TL_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TQ_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_U_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_Q_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TD_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TH_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TM_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TS_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_V_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_R_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TE_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TJ_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TN_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TU_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_X_cscd_512_wwm_cmesh.tfrecord\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file ${input_files}\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --output_dir /work1/zzx6320/lh/Projects/bert/outputs/Pre14_cscd_all_512_64_from_scratch\\\n --max_seq_length 512\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 10\\\n --learning_rate 2e-5\\\n --num_train_steps 2000000\\\n --save_checkpoints_steps 1000\\"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J 3\n#SBATCH -o Sbatch3.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\ninput_files=/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_O_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_S_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TF_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TK_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TP_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TV_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_P_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TB_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TG_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TL_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TQ_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_U_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_Q_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TD_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TH_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TM_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TS_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_V_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_R_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TE_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TJ_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TN_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TU_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_X_cscd_128_wwm_cmesh.tfrecord\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file ${input_files}\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --init_checkpoint /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt \\\n --output_dir /work1/zzx6320/lh/Projects/bert/outputs/Pre3_cscd_all_128_64_from_bert\\\n --max_seq_length 128\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 64\\\n --learning_rate 2e-5\\\n --num_train_steps 3000000\\\n --save_checkpoints_steps 1000\\"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J cmedaq_base\n#SBATCH -o cMedQA2_pre_3_old.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_classifier_hvd.py \\\n    --task_name=pair \\\n    --do_lower_case=true \\\n    --do_train=true \\\n    --do_eval=true \\\n    --do_predict=true \\\n    --data_dir=/public/home/zzx6320/lh/Projects/bert/data/data_cMedQA2 \\\n    --vocab_file=/public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/vocab.txt \\\n    --bert_config_file=/public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json \\\n    --init_checkpoint=/work1/zzx6320/lh/Projects/bert/outputs/Pre3_cscd_all_128_64_from_bert \\\n    --max_seq_length=512 \\\n    --train_batch_size=5 \\\n    --learning_rate=2e-5 \\\n    --num_train_epochs=3.0 \\\n    --output_dir=/work1/zzx6320/lh/Projects/bert/outputs/cMedQA2_pre_3_old \\\n    --cla_nums=2"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J pre_3\n#SBATCH --exclusive\n#SBATCH -o Pre3_4_nodes.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\ninput_files=/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_O_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_S_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TF_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TK_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TP_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TV_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_P_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TB_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TG_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TL_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TQ_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_U_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_Q_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TD_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TH_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TM_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TS_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_V_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_R_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TE_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TJ_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TN_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_TU_512_wwm.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_pre_wwm/pre_training_X_512_wwm.tfrecord\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file ${input_files}\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --init_checkpoint /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt \\\n --output_dir /work1/zzx6320/lh/Projects/bert/outputs/Pre3_cscd_all_512_wwm_from_bert_4_nodes\\\n --max_seq_length 512\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 8\\\n --learning_rate 2e-5\\\n --num_train_steps 2000000\\\n --save_checkpoints_steps 1000\\"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J cmedaq_base\n#SBATCH -o cMedQA2_base.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_classifier_hvd.py \\\n    --task_name=pair \\\n    --do_lower_case=true \\\n    --do_train=true \\\n    --do_eval=true \\\n    --do_predict=true \\\n    --data_dir=/public/home/zzx6320/lh/Projects/bert/data/data_cMedQA2 \\\n    --vocab_file=/public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/vocab.txt \\\n    --bert_config_file=/public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json \\\n    --init_checkpoint=/public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt \\\n    --max_seq_length=512 \\\n    --train_batch_size=5 \\\n    --learning_rate=2e-5 \\\n    --num_train_epochs=3.0 \\\n    --output_dir=/work1/zzx6320/lh/Projects/bert/outputs/cMedQA2_base \\\n    --cla_nums=2"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J pre_13\n#SBATCH --exclusive\n#SBATCH -o Sbatch13.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\ninput_files=/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_O_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_S_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TF_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TK_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TP_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TV_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_P_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TB_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TG_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TL_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TQ_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_U_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_Q_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TD_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TH_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TM_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TS_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_V_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_R_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TE_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TJ_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TN_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TU_cscd_512_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_X_cscd_512_wwm_cmesh.tfrecord\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file ${input_files}\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --init_checkpoint /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt \\\n --output_dir /work1/zzx6320/lh/Projects/bert/outputs/Pre13_cscd_all_512_64_from_bert\\\n --max_seq_length 512\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 10\\\n --learning_rate 2e-5\\\n --num_train_steps 2000000\\\n --save_checkpoints_steps 1000\\"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J pre_bert\n#SBATCH -o pre_cscd_r_128_from_bert.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file /public/home/zzx6320/lh/Projects/bert/data/cscibert_pre_training/pre_training_R_cscd_128.tfrecord\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --init_checkpoint /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt \\\n --output_dir output/Pre2_cscd_R_128_64_from_bert\\\n --max_seq_length 128\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 64\\\n --learning_rate 2e-5\\\n --num_train_steps 500000\\\n --save_checkpoints_steps 1000\\"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J 4\n#SBATCH -o Sbatch4.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\ninput_files=/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_O_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_S_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TF_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TK_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TP_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TV_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_P_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TB_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TG_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TL_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TQ_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_U_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_Q_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TD_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TH_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TM_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TS_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_V_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_R_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TE_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TJ_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TN_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TU_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_X_cscd_128_wwm_cmesh.tfrecord\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file ${input_files}\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --output_dir /work1/zzx6320/lh/Projects/bert/outputs/Pre4_cscd_all_128_64_from_scratch\\\n --max_seq_length 128\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 64\\\n --learning_rate 2e-5\\\n --num_train_steps 3000000\\\n --save_checkpoints_steps 1000\\"}
{"text": "#!/bin/bash\n#SBATCH -p normal\n#SBATCH -N 4\n#SBATCH -J pre_all_scrach\n#SBATCH -o Sbatch4.out\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=dcu:4\n\nDIR=`pwd`\nhostfile=${DIR}/tmp\nscontrol show hostnames $SLURM_JOB_NODELIST > ${hostfile}\n\nfor i in `cat ${hostfile}`\ndo\n    echo ${i} slots=4\ndone > ${DIR}/hostfile-tmp\n\nnum_node=`cat ${hostfile} | uniq | wc -l`\n((num_DCU=${num_node}*4))\n\nexport MIOPEN_USER_DB_PATH=/tmp/tensorflow-miopen-${USER}-2.8\nexport MIOPEN_DEBUG_DISABLE_FIND_DB=1\nexport HOROVOD_HIERARCHICAL_ALLREDUCE=1\n\ninput_files=/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_O_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_S_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TF_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TK_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TP_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TV_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_P_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TB_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TG_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TL_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TQ_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_U_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_Q_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TD_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TH_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TM_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TS_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_V_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_R_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TE_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TJ_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TN_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_TU_cscd_128_wwm_cmesh.tfrecord,/work1/zzx6320/lh/Projects/bert/data/cscd_all_wwm/pre_training_X_cscd_128_wwm_cmesh.tfrecord\n\nmpirun -np ${num_DCU} --hostfile ${DIR}/hostfile-tmp -mca pml ucx -x UCX_TLS=sm,rc,rocm_cpy,rocm_gdr,rocm_ipc -x LD_LIBRARY_PATH -mca coll_hcoll_enable 0 --bind-to none \\\n    python run_pretraining_hvd.py\\\n --input_file ${input_files}\\\n --bert_config_file /public/home/zzx6320/lh/Projects/bert/models/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\\\n --output_dir /work1/zzx6320/lh/Projects/bert/outputs/Pre4_cscd_all_128_64_from_scratch\\\n --max_seq_length 128\\\n --do_train True\\\n --do_eval True \\\n --train_batch_size 64\\\n --learning_rate 2e-5\\\n --num_train_steps 1000000\\\n --save_checkpoints_steps 1000\\"}

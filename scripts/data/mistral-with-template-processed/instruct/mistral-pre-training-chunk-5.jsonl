{"text": "#!/bin/bash -l\n\n#SBATCH --job-name=cnn\n#SBATCH --time=00:30:00\n#SBATCH --nodes=10\n#SBATCH --ntasks-per-core=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=12\n#SBATCH --hint=nomultithread\n#SBATCH --constraint=gpu\n#SBATCH --account=c24\n#SBATCH --output=m10.out\n#SBATCH --error=m10.err\nmodule load daint-gpu\nmodule load PyTorch\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport NCCL_DEBUG=INFO\nexport NCCL_IB_HCA=ipogif0\nexport NCCL_IB_CUDA_SUPPORT=1\n\nsrun python main.py\n\n"}
{"text": "#!/bin/bash -l\n\n#SBATCH --job-name=cnn\n#SBATCH --time=00:30:00\n#SBATCH --nodes=10\n#SBATCH --ntasks-per-core=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=12\n#SBATCH --hint=nomultithread\n#SBATCH --constraint=gpu\n#SBATCH --account=c24\n#SBATCH --output=m10.out\n#SBATCH --error=m10.err\n\nmodule load daint-gpu\nmodule load PyTorch\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport NCCL_DEBUG=INFO\nexport NCCL_IB_HCA=ipogif0\nexport NCCL_IB_CUDA_SUPPORT=1\n\nsrun python main.py -bs ${1}"}
{"text": "#!/bin/bash -l\n\n#SBATCH -N 1\n#SBATCH -p azad\n#SBATCH -t 150:30:00\n#SBATCH -J FusedMM4DGL\n#SBATCH -o FusedMM4DGL.o%j\nmodule unload gcc\nmodule load gcc\n\nsrun -p azad -N 1 -n 1 -c 1 bash run_all.sh\n"}
{"text": "#!/bin/bash\n#SBATCH -t 240:00:00  # time requested in hour:minute:second\n#SBATCH --mem=200G\n#SBATCH --gres=gpu:1\n#SBATCH --partition=overflow\n#SBATCH --output=/home/zguo30/ppg_ecg_proj/proposed/slurm_outputs/%j.out\n\nsource /labs/hulab/stark_conda/bin/activate\nconda activate base_pytorch\n\necho \"JOB START\"\n\nnvidia-smi\n\npython main.py"}
{"text": "#!/bin/bash\n#SBATCH -t 4:00:00  # time requested in hour:minute:second\n#SBATCH --mem=650G\n#SBATCH --cpus-per-task=16\n#SBATCH --partition=overflow\n\nsource /labs/hulab/stark_conda/bin/activate\nconda activate base_pytorch\n\necho \"JOB START\"\n\nnvidia-smi\n\npython make_whole_data.py"}
{"text": "#!/bin/bash\n#SBATCH -t 240:00:00  # time requested in hour:minute:second\n#SBATCH --mem=220G\n#SBATCH --gres=gpu:volta:2\n#SBATCH --partition=overflow\n#SBATCH --output=/home/zguo30/ppg_ecg_proj/ecg_only_baseline/slurm_outputs/%j.out\n\nsource /labs/hulab/stark_conda/bin/activate\nconda activate base_pytorch\n\necho \"JOB START\"\n\nnvidia-smi\n\npython main.py"}
{"text": "#!/bin/bash\n#SBATCH -t 240:00:00  # time requested in hour:minute:second\n#SBATCH --mem=230G\n#SBATCH --gres=gpu:1\n#SBATCH --partition=overflow\n#SBATCH --output=/home/zguo30/ppg_ecg_proj/ecg_only_baseline_limited_labeled/slurm_outputs/%j.out\n\nsource /labs/hulab/stark_conda/bin/activate\nconda activate base_pytorch\n\necho \"JOB START\"\n\nnvidia-smi\n\npython main.py"}
{"text": "#!/bin/bash\n#SBATCH -t 240:00:00  # time requested in hour:minute:second\n#SBATCH --mem=240G\n#SBATCH --gres=gpu:2\n#SBATCH --partition=overflow\n#SBATCH --output=/home/zguo30/ppg_ecg_proj/deepmi/slurm_outputs/%j.out\n\nsource /labs/hulab/stark_conda/bin/activate\nconda activate base_pytorch\n\necho \"JOB START\"\n\nnvidia-smi\n\npython main.py"}
{"text": "#!/bin/bash\n#SBATCH -t 240:00:00  # time requested in hour:minute:second\n#SBATCH --mem=230G\n#SBATCH --gres=gpu:2\n#SBATCH --partition=overflow\n#SBATCH --output=/home/zguo30/ppg_ecg_proj/proposed_limited_labeled/slurm_outputs/%j.out\n\nsource /labs/hulab/stark_conda/bin/activate\nconda activate base_pytorch\n\necho \"JOB START\"\n\nnvidia-smi\n\npython main.py"}
{"text": "#!/bin/bash\n##\n## GPU submission script for PBS on CRESCENT\n## -----------------------------------------\n##\n## Follow the 5 steps below to configure. If you edit this from Windows,\n## *before* submitting via \"qsub\" run \"dos2unix\" on this file - or you will\n## get strange errors. You have been warned.\n## \n## STEP 1:\n## The following line contains the job name:\n##\n#PBS -N cudatest\n##\n## STEP 2:\n##\n##\n#PBS -l select=1:ncpus=1:mpiprocs=1:ngpus=1:mem=16g\n\n##\n## STEP 3:\n##\n## Select correct queue:\n##\n## for this class we have a special queue\n##\n#PBS -q gpu_V100\n##PBS -l walltime=1:00:00\n##\n## STEP 4:\n##\n## Put YOUR email address in the next line:\n##\n#PBS -M alexis.balayre@cranfield.ac.uk\n##\n##\n##\n## DO NOT CHANGE the following lines\n##------------------------------------------------\n#PBS -j oe\n##PBS -v \"CUDA_VISIBLE_DEVICES=\"\n#PBS -W sandbox=PRIVATE\n#PBS -V\n#PBS -m abe \n#PBS -k n\n##\n## Change to working directory\nln -s $PWD $PBS_O_WORKDIR/$PBS_JOBID\ncd $PBS_O_WORKDIR\n## Allocated gpu(s)\necho CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\n##\n## Calculate number of CPUs\ncpus=`cat $PBS_NODEFILE | wc -l`\ngpus=`echo $CUDA_VISIBLE_DEVICES|awk -F\",\" '{print NF}'`\n##\n##\n##-------------------------------------------------\n##\n## STEP 5: \n## \n## Put correct parameters in mpirun execution line\n## below:\n##\nmodule use /apps/modules/all\nmodule load GCC/11.3.0 CUDA/11.7.0 CMake/3.24.3-GCCcore-11.3.0\n\n./runCuda.o\n\n/bin/rm -f $PBS_JOBID\n"}

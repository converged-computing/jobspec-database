{"text": "#!/bin/bash\n#SBATCH --job-name=general\n#SBATCH --output=logs/%x-%j.out\n#SBATCH --error=logs/%x-%j.err\n#SBATCH --time=3-00:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30480\n#SBATCH --partition=nodes\n#SBATCH --gres=gpu:a100:1\n#SBATCH --chdir=/cluster/raid/home/zhivar.sourati/ExplagraphGen\n# Verify working directory\necho $(pwd)\n# Print gpu configuration for this job\nnvidia-smi\n# Verify gpu allocation (should be 1 GPU)\necho $CUDA_VISIBLE_DEVICES\n# Initialize the shell to use local conda\neval \"$(conda shell.bash hook)\"\n# Activate (local) env\nconda activate explagraphgen\n\n# bash scripts/train_graph_gen.sh\n\n# bash scripts/train_graph_max_margin.sh\n\nbash scripts/train_graph_contrastive.sh\n\n# bash scripts/train_graph_gen_pos_perturbed.sh\n\n\nconda deactivate"}
{"text": "#!/bin/sh\n# SGI\n#PBS -V\n#PBS -N snaq75p_cffirst\n#PBS -e snaq75p_cffirst.err\n#PBS -o snaq75p_cffirst.out\n#PBS -q workq\n#PBS -l place=scatter\n#PBS -l select=1:ncpus=21\n#PBS -l walltime=400:00:00\n# shm, sock, ssm, rdma, rdssm\nFABRIC=rdma\n#CORES=$[ `cat $PBS_NODEFILE | wc -l` ]\n#NODES=$[ `uniq $PBS_NODEFILE | wc -l` ]\n\n#printf \"Current time is: `date`\\n\";\n#TEND = \n#printf \"Current PBS work directory is: $PBS_O_WORKDIR\\n\";\n#printf \"Current PBS queue is: $PBS_O_QUEUE\\n\"; \n#printf \"Current PBS job ID is: $PBS_JOBID\\n\";\n#printf \"Current PBS job name is: $PBS_JOBNAME\\n\";\n#printf \"PBS stdout log is: $PBS_O_WORKDIR/sgi_mpitest.err\\n\";\n#printf \"PBS stderr log is: $PBS_O_WORKDIR/sgi_mpitest.log\\n\";\n#printf \"Fabric interconnect selected is: $FABRIC\\n\";\n#printf \"This jobs will run on $CORES processors.\\n\";\n##. /etc/profile.d/modules.sh\nmodule load gnu8 openmpi3 && echo \"Successfully load modules\"\n\ncd $PBS_O_WORKDIR\n\n# add zlib to the path so that openmpi can find it\n#export LD_LIBRARY_PATH=/apps/ZLIB128/lib/:$LD_LIBRARY_PATH\n# export path to my mpi\nexport PATH=\"/home/gustavo/programs/julia-1.6.3/bin:$PATH\"\n\n#printf \"mpiexec_mpt run command location is: `which mpiexec_mpt`\\n\";\n#printf \"\\n[STAT] qstat -f $PBS_JOBID\\n\";\n#qstat -f $PBS_JOBID\n#printf \"\\n[END] qstat -f $PBS_JOBID\\n\";\n#TBEGIN=`echo \"print time();\" | perl`\n\n#MPI_HOSTS=$(sort $PBS_NODEFILE | uniq -c | \\\n#awk '{print $2 \" \" $1}' | \\\n#tr \"\\n\" \",\" | \\\n#sed 's/.$//')\n\n###########\n# COMMAND #\n###########\n\njulia network_estimation.jl > snaq75p_cffirst.outerr 2>&1 \n\n##########\n# FINISH #\n##########\n\n#TEND=`echo \"print time();\" | perl`\n#printf \"Job finished: `date`\\n\";\n#printf \"Job walltime: `expr $TEND - $TBEGIN`\\n\";\n"}
{"text": "#!/bin/bash\n\n#SBATCH --partition=gpu_shared\n#SBATCH --gres=gpu:1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=3\n#SBATCH --job-name=bert_classifier_products\n#SBATCH --time=00-12:00:00\n#SBATCH --output=slurm_output_%A.out\n\nnvidia-smi\n\nmodule purge\nmodule load 2021\nmodule load Anaconda3/2021.05\n\nhead /proc/sys/vm/overcommit_memory\n\n# Your job starts in the directory where you call sbatch\n# Activate your environment\nsource activate giant-xrt\nwhich python\n# Run your code\nexport WANDB_DIR=$HOME\nexperiment_name=bert_classifier_products\nruns=5\nmkdir experiments/${experiment_name}\nmkdir models/${experiment_name}\n# Download data\ncd data/proc_data_multi_task\ndataset=ogbn-products\nsubset=\"\"  # Whether to take a subset of the data. If yes: \"_subset\". If no: \"\".\nbash download_data.sh ${dataset}\ncd ../../\n# Process data\n# bash proc_data_multi_task.sh ${dataset} ${subset}\n# Move files to scratch node\necho \"Moving data to scratch...\"\ntmp_dir=\"$TMPDIR\"/tuanh_scratch\nmkdir ${tmp_dir}\n# Copy data folder\ncp -r data ${tmp_dir}\n# Copy model folder if it already exists\nif [ -d \"models/${experiment_name}\" ]\nthen\n  mkdir ${tmp_dir}/models\n  cp -r models/${experiment_name} ${tmp_dir}/models\nfi\n# Copy experiment folder if it already exists\nif [ -d \"experiments/${experiment_name}\" ]\nthen\n  mkdir ${tmp_dir}/experiments\n  cp -r experiments/${experiment_name} ${tmp_dir}/experiments\nfi\n# Copy cache folder if it already exists\nif [ -d \"models/cache\" ]\nthen\n  cp -r models/cache ${tmp_dir}/models\nfi\ndata_dir=${tmp_dir}/data/proc_data_multi_task/${dataset}${subset}\nmodel_dir=${tmp_dir}/models/${experiment_name}\nexperiment_dir=${tmp_dir}/experiments/${experiment_name}\ncache_dir=${tmp_dir}/models/cache\n# No matter what happens, we copy the temp output folders back to our login node\ntrap 'cp -r ${experiment_dir} $HOME/UvA_Thesis_pecosEXT/experiments; cp -r ${model_dir} $HOME/UvA_Thesis_pecosEXT/models; cp -r ${cache_dir} $HOME/UvA_Thesis_pecosEXT/models; cp -r ${data_dir}/HierarchialLabelTree $HOME/UvA_Thesis_pecosEXT/data/proc_data_multi_task/${dataset}${subset}' EXIT\nstart_seed=0\nend_seed=runs-1\nfor (( seed=$start_seed; seed<=$end_seed; seed++ ))\ndo\n  if [ -d \"${experiment_dir}/run${seed}\" ]\n\tthen\n\t  echo \"Results for run${seed} exists, skip this run\"\n\t  continue 1\n\tfi\n  mkdir ${model_dir}/run${seed}\n  mkdir ${experiment_dir}/run${seed}\n  python -u baseline_models/bert_classifier.py \\\n    --model_dir ${model_dir}/run${seed} \\\n    --experiment_dir ${experiment_dir}/run${seed} \\\n    --seed ${seed} \\\n    --raw-text-path ${data_dir}/X.all.txt \\\n    --text_tokenizer_path ${data_dir}/xrt_models/text_encoder/text_tokenizer \\\n    --dataset ${dataset} \\\n    --epochs 5 \\\n    | tee -a ${experiment_dir}/run${seed}/train.log\ndone\n"}
{"text": "#!/bin/bash\n\n#SBATCH --partition=gpu_shared\n#SBATCH --gres=gpu:1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=6\n#SBATCH --job-name=ExampleJob\n#SBATCH --time=00-12:00:00\n#SBATCH --output=slurm_output_%A.out\n\nnvidia-smi\n\nmodule purge\nmodule load 2021\nmodule load Anaconda3/2021.05\n\nhead /proc/sys/vm/overcommit_memory\n\n# Your job starts in the directory where you call sbatch\n# Activate your environment\nsource activate giant-xrt\nwhich python\n# Run your code\nexport WANDB_DIR=$HOME\nexperiment_name=mtask_roberta\n# Download data\ncd data/proc_data_multi_task\ndataset=ogbn-arxiv\nsubset=\"\"  # Whether to take a subset of the data. If yes: \"_subset\". If no: \"\".\nbash download_data.sh ${dataset}\ncd ../../\n# Process data\nbash proc_data_multi_task.sh ${dataset} ${subset}\n# Move files to scratch node\necho \"Moving data to scratch...\"\ntmp_dir=\"$TMPDIR\"/tuanh_scratch\nmkdir ${tmp_dir}\n# Copy data folder\ncp -r data ${tmp_dir}\n# Copy model folder if it already exists\nif [ -d \"models/${experiment_name}\" ]\nthen\n  mkdir ${tmp_dir}/models\n  cp -r models/${experiment_name} ${tmp_dir}/models\nfi\n# Copy experiment folder if it already exists\nif [ -d \"experiments/${experiment_name}\" ]\nthen\n  mkdir ${tmp_dir}/experiments\n  cp -r experiments/${experiment_name} ${tmp_dir}/experiments\nfi\n# Copy cache folder if it already exists\nif [ -d \"models/cache\" ]\nthen\n  cp -r models/cache ${tmp_dir}/models\nfi\ndata_dir=${tmp_dir}/data/proc_data_multi_task/${dataset}${subset}\nmodel_dir=${tmp_dir}/models/${experiment_name}\nexperiment_dir=${tmp_dir}/experiments/${experiment_name}\ncache_dir=${tmp_dir}/models/cache\nruns=1\n# No matter what happens, we copy the temp output folders back to our login node\ntrap 'cp -r ${experiment_dir} $HOME/UvA_Thesis_pecosEXT/experiments; cp -r ${model_dir} $HOME/UvA_Thesis_pecosEXT/models; cp -r ${cache_dir} $HOME/UvA_Thesis_pecosEXT/models; cp -r ${data_dir}/HierarchialLabelTree $HOME/UvA_Thesis_pecosEXT/data/proc_data_multi_task/${dataset}${subset}' EXIT\n# Run train-val-test pipeline\nparams_path=data/proc_data_multi_task/params_mtask_${dataset}${subset}.json\nbash multi_task_pipeline.sh ${data_dir} ${model_dir} ${experiment_dir} ${cache_dir} ${params_path} ${runs}\nbash encode_mtask.sh ${data_dir} ${model_dir} ${experiment_dir} ${cache_dir} ${params_path} ${runs}\n#bash hyperparams_sweep.sh ${data_dir}${subset} ${model_dir} ${experiment_dir} ${cache_dir} ${params_path} sweep_configs/${experiment_name}"}
{"text": "#!/bin/bash\n\n# This would run the TPIL Bundle Segmentation Pipeline with the following resources:\n#     Prebuild Singularity images: https://scil.usherbrooke.ca/pages/containers/\n#     Brainnetome atlas in MNI space: https://atlas.brainnetome.org/download.html\n#     FA template in MNI space: https://brain.labsolver.org/hcp_template.html\n\n\n#SBATCH --nodes=1              # --> Generally depends on your nb of subjects.\n                               # See the comment for the cpus-per-task. One general rule could be\n                               # that if you have more subjects than cores/cpus (ex, if you process 38\n                               # subjects on 32 cpus-per-task), you could ask for one more node.\n#SBATCH --cpus-per-task=32     # --> You can see here the choices. For beluga, you can choose 32, 40 or 64.\n                               # https://docs.computecanada.ca/wiki/B%C3%A9luga/en#Node_Characteristics\n#SBATCH --mem=0                # --> 0 means you take all the memory of the node. If you think you will need\n                               # all the node, you can keep 0.\n#SBATCH --time=6:00:00\n\n#SBATCH --mail-user=paul.bautin@polymtl.ca\n#SBATCH --mail-type=BEGIN\n#SBATCH --mail-type=END\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-type=REQUEUE\n#SBATCH --mail-type=ALL\n\n\nmodule load StdEnv/2020 java/14.0.2 nextflow/22.04.3 singularity/3.8\n\n\nmy_singularity_img='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/containers/scilus_1.4.2.sif' # or .sif\nmy_main_nf='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_bundle_segmentation/main.nf'\nmy_input='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/23-02-13_bundle_segmentation_control/'\nmy_atlas='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/BNA-maxprob-thr0-1mm.nii.gz'\nmy_template='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/FSL_HCP1065_FA_1mm.nii.gz'\n\n\nnextflow run $my_main_nf --input $my_input --atlas $my_atlas \\\n    -with-singularity $my_singularity_img --template $my_template -resume\n\nmodule load nextflow/21.10.3\n\nmy_main_nf_qc='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/dmriqc_flow/main.nf'\nmy_input_qc='/home/pabaua/scratch/tpil_dev/results/clbp/23-02-13_accumbofrontal_segmentation/results_bundle'\n\nNXF_VER=21.10.3 nextflow run $my_main_nf_qc -profile rbx_qc --input $my_input_qc \\\n    -with-singularity $my_singularity_img -resume\n\n"}
{"text": "#!/bin/bash\n\n# This would run the TPIL Bundle Segmentation Pipeline with the following resources:\n#     Prebuild Singularity images: https://scil.usherbrooke.ca/pages/containers/\n#     Brainnetome atlas in MNI space: https://atlas.brainnetome.org/download.html\n#     FA template in MNI space: https://brain.labsolver.org/hcp_template.html\n\n\n#SBATCH --nodes=1              # --> Generally depends on your nb of subjects.\n                               # See the comment for the cpus-per-task. One general rule could be\n                               # that if you have more subjects than cores/cpus (ex, if you process 38\n                               # subjects on 32 cpus-per-task), you could ask for one more node.\n#SBATCH --cpus-per-task=32     # --> You can see here the choices. For beluga, you can choose 32, 40 or 64.\n                               # https://docs.computecanada.ca/wiki/B%C3%A9luga/en#Node_Characteristics\n#SBATCH --mem=0                # --> 0 means you take all the memory of the node. If you think you will need\n                               # all the node, you can keep 0.\n#SBATCH --time=6:00:00\n\n#SBATCH --mail-user=paul.bautin@polymtl.ca\n#SBATCH --mail-type=BEGIN\n#SBATCH --mail-type=END\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-type=REQUEUE\n#SBATCH --mail-type=ALL\n\n\nmodule load StdEnv/2020 java/14.0.2 nextflow/22.10.8 apptainer/1.1.8\n\n\nmy_singularity_img='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/containers/singularity_container.sif' # or .sif\nmy_main_nf='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_connectivity_prep/main_accumbofrontal.nf'\nmy_input_tr_con='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/23-07-05_tractoflow_bundling_con/results'\nmy_input_fs_con='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/23_02_09_control_freesurfer_output'\nmy_input_tr_clbp='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/23-07-05_tractoflow_bundling/results'\nmy_input_fs_clbp='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/22-09-21_t1_clbp_freesurfer_output'\nmy_licence_fs='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_connectivity_prep/freesurfer_data/license.txt'\nmy_template='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/mni_masked.nii.gz'\n\n\n\nnextflow run $my_main_nf  \\\n  --input_tr $my_input_tr_clbp \\\n  --input_fs $my_input_fs_clbp \\\n  --licence_fs $my_licence_fs \\\n  --template $my_template \\\n  -with-singularity $my_singularity_img \\\n  -profile compute_canada \\\n  -resume\n\n\n# module load StdEnv/2020 java/14.0.2 nextflow/21.10.3 apptainer/1.1.8\n\n\n# my_singularity_img='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/containers/scilus_1.5.0.sif' # or .img\n# my_main_nf='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/connectoflow/main.nf'\n# my_input_con_BN='/home/pabaua/scratch/tpil_dev/results/control/23-08-17_connectflow/results'\n# my_input_clbp_BN='/home/pabaua/scratch/tpil_dev/results/clbp/23-08-17_connectflow/results'\n# my_input_con_schaefer='/home/pabaua/scratch/tpil_dev/results/control/23-08-17_connectflow_schaefer/results'\n# my_input_clbp_schaefer='/home/pabaua/scratch/tpil_dev/results/clbp/23-08-17_connectflow_schaefer/results'\n# my_template='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/mni_masked.nii.gz'\n# my_labels_list_BN='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_connectivity_prep/freesurfer_data/atlas_brainnetome_first_label_list.txt'\n# my_labels_list_schaefer='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_connectivity_prep/freesurfer_data/atlas_schaefer_200_first_label_list.txt'\n\n# NXF_DEFAULT_DSL=1 nextflow run $my_main_nf \\\n#   --input $my_input_con_BN \\\n#   --labels_list $my_labels_list_BN \\\n#   --labels_img_prefix 'BN_' \\\n#   --template $my_template \\\n#   --apply_t1_labels_transfo false \\\n#   -with-singularity $my_singularity_img \\\n#   -resume\n\n"}
{"text": "#!/bin/bash\n\n# This would run the TPIL Bundle Segmentation Pipeline with the following resources:\n#     Prebuild Singularity images: https://scil.usherbrooke.ca/pages/containers/\n#     Brainnetome atlas in MNI space: https://atlas.brainnetome.org/download.html\n#     FA template in MNI space: https://brain.labsolver.org/hcp_template.html\n\n\n#SBATCH --nodes=1              # --> Generally depends on your nb of subjects.\n                               # See the comment for the cpus-per-task. One general rule could be\n                               # that if you have more subjects than cores/cpus (ex, if you process 38\n                               # subjects on 32 cpus-per-task), you could ask for one more node.\n#SBATCH --cpus-per-task=32     # --> You can see here the choices. For beluga, you can choose 32, 40 or 64.\n                               # https://docs.computecanada.ca/wiki/B%C3%A9luga/en#Node_Characteristics\n#SBATCH --mem=0                # --> 0 means you take all the memory of the node. If you think you will need\n                               # all the node, you can keep 0.\n#SBATCH --time=6:00:00\n\n#SBATCH --mail-user=paul.bautin@polymtl.ca\n#SBATCH --mail-type=BEGIN\n#SBATCH --mail-type=END\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-type=REQUEUE\n#SBATCH --mail-type=ALL\n\n\nmodule load StdEnv/2020 java/14.0.2 nextflow/22.04.3 singularity/3.8\n\n\nmy_singularity_img='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/containers/scilus_1.3.0.sif' # or .sif\nmy_main_nf='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_dmri/bundle_segmentation/original/main.nf'\nmy_input='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/22-10-26_bundle_segmentation/'\nmy_atlas='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/BNA-maxprob-thr0-1mm.nii.gz'\nmy_template='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/FSL_HCP1065_FA_1mm.nii.gz'\n\n\nnextflow run $my_main_nf --input $my_input --atlas $my_atlas \\\n    -with-singularity $my_singularity_img --template $my_template -resume\n\n"}
{"text": "#!/bin/bash\n\n# This would run the TPIL Bundle Segmentation Pipeline with the following resources:\n#     Prebuild Singularity images: https://scil.usherbrooke.ca/pages/containers/\n#     Brainnetome atlas in MNI space: https://atlas.brainnetome.org/download.html\n#     FA template in MNI space: https://brain.labsolver.org/hcp_template.html\n\n\n#SBATCH --nodes=1              # --> Generally depends on your nb of subjects.\n                               # See the comment for the cpus-per-task. One general rule could be\n                               # that if you have more subjects than cores/cpus (ex, if you process 38\n                               # subjects on 32 cpus-per-task), you could ask for one more node.\n#SBATCH --cpus-per-task=32     # --> You can see here the choices. For beluga, you can choose 32, 40 or 64.\n                               # https://docs.computecanada.ca/wiki/B%C3%A9luga/en#Node_Characteristics\n#SBATCH --mem=0                # --> 0 means you take all the memory of the node. If you think you will need\n                               # all the node, you can keep 0.\n#SBATCH --time=1:00:00\n\n#SBATCH --mail-user=paul.bautin@polymtl.ca\n#SBATCH --mail-type=BEGIN\n#SBATCH --mail-type=END\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-type=REQUEUE\n#SBATCH --mail-type=ALL\n\n\nmodule load StdEnv/2020 java/14.0.2 nextflow/22.04.3 singularity/3.8\n\n\nmy_singularity_img='/home/pabaua/dev_scil/containers/scilus_1_3_0.img' # or .sif\nmy_main_nf='/home/pabaua/dev_scil/tpil_bundle_segmentation/main.nf'\nmy_input='/home/pabaua/dev_tpil/data/data_new_bundle'\nmy_atlas='/home/pabaua/dev_tpil/data/data_new_bundle/BNA-maxprob-thr0-1mm.nii.gz'\nmy_template='/home/pabaua/dev_tpil/data/data_new_bundle/FSL_HCP1065_FA_1mm.nii.gz'\n\n\nnextflow run $my_main_nf --input $my_input --atlas $my_atlas \\\n    -with-singularity $my_singularity_img --template $my_template -resume\n\n"}
{"text": "#!/bin/bash\n\n# This would run the TPIL Bundle Segmentation Pipeline with the following resources:\n#     Prebuild Singularity images: https://scil.usherbrooke.ca/pages/containers/\n#     Brainnetome atlas in MNI space: https://atlas.brainnetome.org/download.html\n#     FA template in MNI space: https://brain.labsolver.org/hcp_template.html\n\n\n#SBATCH --nodes=1              # --> Generally depends on your nb of subjects.\n                               # See the comment for the cpus-per-task. One general rule could be\n                               # that if you have more subjects than cores/cpus (ex, if you process 38\n                               # subjects on 32 cpus-per-task), you could ask for one more node.\n#SBATCH --cpus-per-task=32     # --> You can see here the choices. For beluga, you can choose 32, 40 or 64.\n                               # https://docs.computecanada.ca/wiki/B%C3%A9luga/en#Node_Characteristics\n#SBATCH --mem=0                # --> 0 means you take all the memory of the node. If you think you will need\n                               # all the node, you can keep 0.\n#SBATCH --time=6:00:00\n\n#SBATCH --mail-user=paul.bautin@polymtl.ca\n#SBATCH --mail-type=BEGIN\n#SBATCH --mail-type=END\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-type=REQUEUE\n#SBATCH --mail-type=ALL\n\n\nmodule load StdEnv/2020 java/14.0.2 nextflow/22.04.3 singularity/3.8\n\n\nmy_singularity_img='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/containers/scilus_1.4.2.sif' # or .sif\nmy_main_nf='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_dmri/bundle_segmentation/fsl/main.nf'\nmy_input='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/22-10-26_bundle_segmentation/'\nmy_atlas='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/BNA-maxprob-thr0-1mm.nii.gz'\nmy_template='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/atlas/FSL_HCP1065_FA_1mm.nii.gz'\n\n\nnextflow run $my_main_nf --input $my_input --atlas $my_atlas \\\n    -with-singularity $my_singularity_img --template $my_template -resume\n\n"}
{"text": "#!/bin/bash\n\n# This would run Tractometry with the following parameters:\n\n\n#SBATCH --nodes=1              # --> Generally depends on your nb of subjects.\n                               # See the comment for the cpus-per-task. One general rule could be\n                               # that if you have more subjects than cores/cpus (ex, if you process 38\n                               # subjects on 32 cpus-per-task), you could ask for one more node.\n#SBATCH --cpus-per-task=32     # --> You can see here the choices. For beluga, you can choose 32, 40 or 64.\n                               # https://docs.computecanada.ca/wiki/B%C3%A9luga/en#Node_Characteristics\n#SBATCH --mem=0                # --> 0 means you take all the memory of the node. If you think you will need\n                               # all the node, you can keep 0.\n#SBATCH --time=3:00:00\n\n#SBATCH --mail-user=paul.bautin@polymtl.ca\n#SBATCH --mail-type=BEGIN\n#SBATCH --mail-type=END\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-type=REQUEUE\n#SBATCH --mail-type=ALL\n\n\nmodule load StdEnv/2020 java/14.0.2 nextflow/22.04.3 singularity/3.8\n\n\nmy_singularity_img='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_scil/containers/scilus_1.4.2.sif' # or .sif\nmy_main_nf='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/tpil_dmri/tractometry/main.nf'\nmy_input='/home/pabaua/projects/def-pascalt-ab/pabaua/dev_tpil/data/23-02-13_tractometry_clbp/'\n\n\n\nnextflow run $my_main_nf --input $my_input --nb_points 20 \\\n    -with-singularity $my_singularity_img -resume --use_provided_centroids false\n"}
